"use strict";(self.webpackChunkbuildbuddy_docs_website=self.webpackChunkbuildbuddy_docs_website||[]).push([[8324],{15682:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>h,contentTitle:()=>p,default:()=>b,frontMatter:()=>u,metadata:()=>o,toc:()=>f});const o=JSON.parse('{"id":"config-all-options","title":"All Options","description":"Provided below are working, documented YAML configs for each BuildBuddy binary","source":"@site/../docs/config-all-options.mdx","sourceDirName":".","slug":"/config-all-options","permalink":"/docs/config-all-options","draft":false,"unlisted":false,"editUrl":"https://github.com/buildbuddy-io/buildbuddy/edit/master/docs/../docs/config-all-options.mdx","tags":[],"version":"current","lastUpdatedBy":"Brandon Duffany","lastUpdatedAt":1736444241000,"frontMatter":{"id":"config-all-options","title":"All Options","sidebar_label":"All Options"},"sidebar":"someSidebar","previous":{"title":"Flags","permalink":"/docs/config-flags"}}');var a=t(74848),i=t(28453);function r(e){const n={code:"code",pre:"pre",...(0,i.R)(),...e.components};return(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:'# Unstructured settings\n\n# app_directory (string): the directory containing app binary files to host\napp_directory: ""\n# auto_migrate_db (bool): If true, attempt to automigrate the db when \n# connecting\nauto_migrate_db: true\n# auto_migrate_db_and_exit (bool): If true, attempt to automigrate the db when \n# connecting, then exit the program.\nauto_migrate_db_and_exit: false\n# block_profile_rate (int): The fraction of goroutine blocking events \n# reported. (1/rate, 0 disables)\nblock_profile_rate: 0\n# cache_stats_finalization_delay (time.Duration): The time allowed for all \n# metrics collectors across all apps to flush their local cache stats to the \n# backing storage, before finalizing stats in the DB.\ncache_stats_finalization_delay: 500ms\n# cleanup_interval (time.Duration): How often the janitor cleanup tasks will \n# run\ncleanup_interval: 10m0s\n# cleanup_workers (int): How many cleanup tasks to run\ncleanup_workers: 1\n# disable_ga (bool): If true; ga will be disabled\ndisable_ga: false\n# disable_telemetry (bool): If true; telemetry will be disabled\ndisable_telemetry: false\n# drop_invocation_pk_cols (bool): If true, attempt to drop invocation PK cols\ndrop_invocation_pk_cols: false\n# exit_when_ready (bool): If set, the app will exit as soon as it becomes \n# ready (useful for migrations)\nexit_when_ready: false\n# grpc_client_origin_header (string): Header value to set for \n# x-buildbuddy-origin.\ngrpc_client_origin_header: ""\n# grpc_max_recv_msg_size_bytes (int): Configures the max GRPC receive message \n# size [bytes]\ngrpc_max_recv_msg_size_bytes: 50000000\n# grpc_port (int): The port to listen for gRPC traffic on\ngrpc_port: 1985\n# grpcs_port (int): The port to listen for gRPCS traffic on\ngrpcs_port: 1986\n# internal_grpc_port (int): The port to listen for internal gRPC traffic on\ninternal_grpc_port: 1987\n# internal_grpcs_port (int): The port to listen for internal gRPCS traffic on\ninternal_grpcs_port: 1988\n# internal_http_port (int): The port to listen for internal HTTP traffic\ninternal_http_port: 0\n# js_entry_point_path (string): Absolute URL path of the app JS entry point\njs_entry_point_path: /app/app_bundle/app.js?hash={APP_BUNDLE_HASH}\n# listen (string): The interface to listen on (default: 0.0.0.0)\nlisten: 0.0.0.0\n# log_deletion_errors (bool): If true; log errors when ttl-deleting expired \n# data\nlog_deletion_errors: false\n# log_goroutine_profile_on_shutdown (bool): Whether to log all goroutine stack \n# traces on shutdown.\nlog_goroutine_profile_on_shutdown: false\n# max_shutdown_duration (time.Duration): Time to wait for shutdown\nmax_shutdown_duration: 25s\n# max_threads (int): The maximum number of threads to allow before panicking. \n# If unset, the golang default will be used (currently 10,000).\nmax_threads: 0\n# migrate_disk_cache_to_v2_and_exit (bool): If true, attempt to migrate disk \n# cache to v2 layout.\nmigrate_disk_cache_to_v2_and_exit: false\n# monitoring_port (int): The port to listen for monitoring traffic on\nmonitoring_port: 9090\n# mutex_profile_fraction (int): The fraction of mutex contention events \n# reported. (1/rate, 0 disables)\nmutex_profile_fraction: 0\n# port (int): The port to listen for HTTP traffic on\nport: 8080\n# regions ([]region.Region): A list of regions that executors might be \n# connected to.\nregions: []\n# For example:\n# - name: "" # The user-friendly name of this region. Ex: Europe (type: string)\n#   server: "" # The http endpoint for this server, with the protocol. Ex: https://app.europe.buildbuddy.io (type: string)\n#   subdomains: "" # The format for subdomain urls of with a single * wildcard. Ex: https://*.europe.buildbuddy.io (type: string)\n\n# report_not_ready (bool): If set to true, the app will always report as being \n# unready.\nreport_not_ready: false\n# server_type (string): The server type to match on health checks\nserver_type: buildbuddy-server\n# shutdown_lameduck_duration (time.Duration): If set, the server will be \n# marked unready but not run shutdown functions until this period passes.\nshutdown_lameduck_duration: 0s\n# ssl_port (int): The port to listen for HTTPS traffic on\nssl_port: 8081\n# static_directory (string): the directory containing static files to host\nstatic_directory: ""\n# telemetry_endpoint (string): The telemetry endpoint to use\ntelemetry_endpoint: grpcs://t.buildbuddy.io:443\n# telemetry_interval (time.Duration): How often telemetry data will be \n# reported\ntelemetry_interval: 24h0m0s\n# verbose_telemetry_client (bool): If true; print telemetry client information\nverbose_telemetry_client: false\n\n# Structured settings\n\napi:\n    # api.api_key (string): The default API key to use for on-prem enterprise \n    # deploys with a single organization/group. **DEPRECATED** Manual API key \n    # specification is no longer supported; to retrieve specific API keys \n    # programmatically, please use the API key table. This field will still \n    # specify an API key to redact in case a manual API key was specified when \n    # buildbuddy was first set up.\n    api_key: ""\napp:\n    # app.admin_only_create_group (bool): If true, only admins of an existing \n    # group can create a new groups.\n    admin_only_create_group: false\n    # app.audit_logs_ui_enabled (bool): If set, the audit logs UI will be \n    # accessible from the sidebar.\n    audit_logs_ui_enabled: false\n    # app.bazel_buttons_enabled (bool): If set, show remote bazel buttons in \n    # the UI.\n    bazel_buttons_enabled: false\n    # app.build_buddy_url (URL): The external URL where your BuildBuddy \n    # instance can be found.\n    build_buddy_url: http://localhost:8080\n    # app.cache_api_url (URL): Overrides the default remote cache protocol \n    # gRPC address shown by BuildBuddy on the configuration screen.\n    cache_api_url: ""\n    # app.code_editor_enabled (bool): If set, code editor functionality will \n    # be enabled.\n    code_editor_enabled: false\n    # app.code_editor_v2_enabled (bool): If set, show v2 of code editor that \n    # stores state on server instead of local storage.\n    code_editor_v2_enabled: false\n    # app.code_review_enabled (bool): If set, show the code review UI.\n    code_review_enabled: false\n    # app.codesearch_enabled (bool): If set, show the code search UI.\n    codesearch_enabled: false\n    # app.community_links_enabled (bool): If set, show links to BuildBuddy \n    # community in the UI.\n    community_links_enabled: true\n    # app.customer_managed_encryption_keys_enabled (bool): If set, show \n    # customer-managed encryption configuration UI.\n    customer_managed_encryption_keys_enabled: false\n    # app.default_login_slug (string): If set, the login page will default to \n    # using this slug.\n    default_login_slug: ""\n    # app.default_subdomains ([]string): List of subdomains that should not be \n    # handled as user-owned subdomains.\n    default_subdomains: []\n    # app.default_to_dense_mode (bool): Enables the dense UI mode by default.\n    default_to_dense_mode: false\n    # app.deprecate_anonymous_access (bool): If true, log a warning in the \n    # bazel console when clients are unauthenticated\n    deprecate_anonymous_access: false\n    # app.disable_cert_config (bool): If true, the certificate based auth \n    # option will not be shown in the config widget.\n    disable_cert_config: false\n    # app.enable_canaries (bool): If true, enable slow function canaries\n    enable_canaries: true\n    # app.enable_grpc_metrics_by_group_id (bool): If enabled, grpc metrics by \n    # group ID will be recorded\n    enable_grpc_metrics_by_group_id: false\n    # app.enable_prometheus_histograms (bool): If true, collect prometheus \n    # histograms for all RPCs\n    enable_prometheus_histograms: true\n    # app.enable_read_target_statuses_from_olap_db (bool): If enabled, read \n    # target statuses from OLAP DB\n    enable_read_target_statuses_from_olap_db: false\n    # app.enable_structured_logging (bool): If true, log messages will be \n    # json-formatted.\n    enable_structured_logging: false\n    # app.enable_subdomain_matching (bool): If true, request subdomain will be \n    # taken into account when determining what request restrictions should be \n    # applied.\n    enable_subdomain_matching: false\n    # app.enable_target_tracking (bool): Cloud-Only\n    enable_target_tracking: false\n    # app.enable_write_executions_to_olap_db (bool): If enabled, complete \n    # Executions will be flushed to OLAP DB\n    enable_write_executions_to_olap_db: false\n    # app.enable_write_test_target_statuses_to_olap_db (bool): If enabled, \n    # test target statuses will be flushed to OLAP DB\n    enable_write_test_target_statuses_to_olap_db: false\n    # app.enable_write_to_olap_db (bool): If enabled, complete invocations \n    # will be flushed to OLAP DB\n    enable_write_to_olap_db: true\n    # app.events_api_url (URL): Overrides the default build event protocol \n    # gRPC address shown by BuildBuddy on the configuration screen.\n    events_api_url: ""\n    # app.execution_search_enabled (bool): If set, fetch lists of executions \n    # from the OLAP DB in the trends UI.\n    execution_search_enabled: true\n    # app.expanded_suggestions_enabled (bool): If set, enable more build \n    # suggestions in the UI.\n    expanded_suggestions_enabled: false\n    # app.grpc_max_recv_msg_size_bytes (int): DEPRECATED: use \n    # --grpc_max_recv_msg_size_bytes instead\n    grpc_max_recv_msg_size_bytes: 50000000\n    # app.grpc_over_http_port_enabled (bool): Enables grpc traffic to be \n    # served over the http port.\n    grpc_over_http_port_enabled: true\n    # app.ignore_forced_tracing_header (bool): If set, we will not honor the \n    # forced tracing header.\n    ignore_forced_tracing_header: false\n    # app.invocation_log_streaming_enabled (bool): If set, the UI will stream \n    # invocation logs instead of polling.\n    invocation_log_streaming_enabled: false\n    # app.ip_rules_ui_enabled (bool): If set, show the IP rules tab in \n    # settings page.\n    ip_rules_ui_enabled: false\n    # app.log_enable_gcp_logging_format (bool): If true, the output structured \n    # logs will be compatible with format expected by GCP Logging.\n    log_enable_gcp_logging_format: false\n    # app.log_enable_grpc_request (bool): If true, log grpc request when log \n    # level is default\n    log_enable_grpc_request: true\n    # app.log_error_stack_traces (bool): If true, stack traces will be printed \n    # for errors that have them.\n    log_error_stack_traces: false\n    # app.log_gcp_log_id (string): The log ID to log to in GCP (if any).\n    log_gcp_log_id: ""\n    # app.log_gcp_project_id (string): The project ID to log to in GCP (if \n    # any).\n    log_gcp_project_id: ""\n    # app.log_include_short_file_name (bool): If true, log messages will \n    # include shortened originating file name.\n    log_include_short_file_name: false\n    # app.log_level (string): The desired log level. Logs with a level >= this \n    # level will be emitted. One of {\'fatal\', \'error\', \'warn\', \'info\', \n    # \'debug\'}\n    log_level: info\n    # app.new_trends_ui_enabled (bool): DEPRECATED: If set, show a new trends \n    # UI with a bit more organization.\n    new_trends_ui_enabled: false\n    # app.org_admin_api_key_creation_enabled (bool): If set, SCIM API keys \n    # will be able to be created in the UI.\n    org_admin_api_key_creation_enabled: false\n    # app.paginate_invocations (bool): If true, paginate invocations returned \n    # to the UI.\n    paginate_invocations: true\n    # app.pattern_filter_enabled (bool): If set, allow filtering by pattern in \n    # the client.\n    pattern_filter_enabled: true\n    # app.popup_auth_enabled (bool): Whether popup windows should be used for \n    # authentication.\n    popup_auth_enabled: false\n    # app.proxy_targets ([]grpc_forward.proxyPair)\n    proxy_targets: []\n    # For example:\n    # - prefix: "" # The gRPC method prefix to match. (type: string)\n    #   target: "" # The gRPC target to forward requests to. (type: string)\n\n    # app.reader_writer_roles_enabled (bool): If set, Reader/Writer roles will \n    # be enabled in the user management UI.\n    reader_writer_roles_enabled: true\n    # app.remote_execution_api_url (URL): Overrides the default remote \n    # execution protocol gRPC address shown by BuildBuddy on the configuration \n    # screen.\n    remote_execution_api_url: ""\n    # app.restrict_bytestream_dialing (bool): If true, only allow dialing \n    # localhost or the configured cache backend for bytestream requests.\n    restrict_bytestream_dialing: false\n    # app.streaming_http_enabled (bool): Whether to support server-streaming \n    # http requests between server and web UI.\n    streaming_http_enabled: false\n    # app.strict_csp_enabled (bool): If set, set a strict CSP header. \n    # Violations are logged at warning level.\n    strict_csp_enabled: false\n    # app.tags_enabled (bool): Enable setting tags on invocations via \n    # build_metadata\n    tags_enabled: false\n    # app.tags_ui_enabled (bool): If set, expose tags data and let users \n    # filter by tag.\n    tags_ui_enabled: false\n    # app.target_flakes_ui_enabled (bool): If set, show some fancy new \n    # features for analyzing flakes.\n    target_flakes_ui_enabled: false\n    # app.test_grid_v2_enabled (bool): Whether to enable test grid V2\n    test_grid_v2_enabled: true\n    # app.test_output_manifests_enabled (bool): If set, the target page will \n    # render the contents of test output zips.\n    test_output_manifests_enabled: true\n    # app.timeseries_charts_in_timing_profile_enabled (bool): If set, charts \n    # with sampled time series data (such as CPU and memory usage) will be \n    # shown\n    timeseries_charts_in_timing_profile_enabled: true\n    # app.trace_fraction (float64): Fraction of requests to sample for \n    # tracing.\n    trace_fraction: 0\n    # app.trace_fraction_overrides ([]string): Tracing fraction override based \n    # on name in format name=fraction.\n    trace_fraction_overrides: []\n    # app.trace_jaeger_collector (string): Address of the Jager collector \n    # endpoint where traces will be sent.\n    trace_jaeger_collector: ""\n    # app.trace_project_id (string): Optional GCP project ID to export traces \n    # to. If not specified, determined from default credentials or metadata \n    # server if running on GCP.\n    trace_project_id: ""\n    # app.trace_service_name (string): Name of the service to associate with \n    # traces.\n    trace_service_name: ""\n    # app.trace_viewer_enabled (bool): Whether the new trace viewer is \n    # enabled.\n    trace_viewer_enabled: false\n    # app.trends_heatmap_enabled (bool): If set, enable a fancy heatmap UI for \n    # exploring build trends.\n    trends_heatmap_enabled: true\n    # app.trends_range_selection (bool): If set, let users drag to select time \n    # ranges in the trends UI.\n    trends_range_selection: true\n    # app.trends_summary_enabled (bool): If set, show the new \'summary\' \n    # section at the top of the trends UI.\n    trends_summary_enabled: false\n    # app.usage_enabled (bool): If set, the usage page will be enabled in the \n    # UI.\n    usage_enabled: false\n    # app.user_management_enabled (bool): If set, the user management page \n    # will be enabled in the UI. **DEPRECATED** This flag has no effect and \n    # will be removed in the future.\n    user_management_enabled: true\nauth:\n    # auth.domain_wide_cookies (bool): If true, cookies will have domain set \n    # so that they are accessible on domain and all subdomains.\n    domain_wide_cookies: false\n    # auth.https_only_cookies (bool): If true, cookies will only be set over \n    # https connections.\n    https_only_cookies: false\n    # auth.jwt_claims_cache_ttl (time.Duration): TTL for JWT string to parsed \n    # claims caching. Set to \'0\' to disable cache.\n    jwt_claims_cache_ttl: 15s\n    # auth.jwt_duration (time.Duration): Maximum lifetime of the generated \n    # JWT.\n    jwt_duration: 6h0m0s\n    # auth.jwt_key (string): The key to use when signing JWT tokens.\n    jwt_key: set_the_jwt_in_config\n    # auth.new_jwt_key (string): If set, JWT verifications will try both this \n    # and the old JWT key.\n    new_jwt_key: ""\n    # auth.sign_using_new_jwt_key (bool): If true, new JWTs will be signed \n    # using the new JWT key.\n    sign_using_new_jwt_key: false\n    # auth.trust_xforwardedfor_header (bool): If true, client IP information \n    # will be retrieved from the X-Forwarded-For header. Should only be \n    # enabled if the BuildBuddy server is only accessible behind a trusted \n    # proxy.\n    trust_xforwardedfor_header: false\nbuild_event_proxy:\n    # build_event_proxy.buffer_size (int): The number of build events to \n    # buffer locally when proxying build events.\n    buffer_size: 100\n    # build_event_proxy.hosts ([]string): The list of hosts to pass build \n    # events onto.\n    hosts: []\ncache:\n    client:\n        # cache.client.ac_rpc_timeout (time.Duration): Maximum time a single \n        # Action Cache RPC can take.\n        ac_rpc_timeout: 15s\n        # cache.client.cas_rpc_timeout (time.Duration): Maximum time a single \n        # batch RPC or a single ByteStream chunk read can take.\n        cas_rpc_timeout: 1m0s\n        # cache.client.enable_upload_compression (bool): If true, enable \n        # compression of uploads to remote caches\n        enable_upload_compression: true\n    # cache.count_ttl (time.Duration): How long to go without receiving any \n    # cache requests for an invocation before deleting the invocation\'s counts \n    # from the metrics collector.\n    count_ttl: 24h0m0s\n    # cache.detailed_stats_enabled (bool): Whether to enable detailed stats \n    # recording for all cache requests.\n    detailed_stats_enabled: false\n    # cache.detailed_stats_ttl (time.Duration): How long to go without \n    # receiving any cache requests for an invocation before deleting the \n    # invocation\'s detailed results from the metrics collector. Has no effect \n    # if cache.detailed_stats_enabled is not set.\n    detailed_stats_ttl: 3h0m0s\n    # cache.directory_sizes_enabled (bool): If true, enable an RPC that \n    # computes the cumulative size of directories stored in the cache.\n    directory_sizes_enabled: false\n    disk:\n        # cache.disk.partition_mappings ([]disk.PartitionMapping)\n        partition_mappings: []\n        # For example:\n        # - group_id: "" # The Group ID to which this mapping applies. (type: string)\n        #   prefix: "" # The remote instance name prefix used to select this partition. (type: string)\n        #   partition_id: "" # The partition to use if the Group ID and prefix match. (type: string)\n\n        # cache.disk.partitions ([]disk.Partition)\n        partitions: []\n        # For example:\n        # - id: "" # The ID of the partition. (type: string)\n        #   max_size_bytes: 0 # Maximum size of the partition. (type: int64)\n        #   encryption_supported: false # Whether encrypted data can be stored on this partition. (type: bool)\n\n        # cache.disk.root_directory (string): The root directory to store all \n        # blobs in, if using disk based storage.\n        root_directory: ""\n        # cache.disk.use_v2_layout (bool): If enabled, files will be stored \n        # using the v2 layout. See disk_cache.MigrateToV2Layout for a \n        # description.\n        use_v2_layout: false\n    # cache.enable_tree_caching (bool): If true, cache GetTree responses (full \n    # and partial)\n    enable_tree_caching: true\n    # cache.in_memory (bool): Whether or not to use the in_memory cache.\n    in_memory: false\n    # cache.max_direct_write_size_bytes (int64): For bytestream requests \n    # smaller than this size, write straight to the cache without checking if \n    # the entry already exists.\n    max_direct_write_size_bytes: 16384\n    # cache.max_size_bytes (int64): How big to allow the cache to be (in \n    # bytes).\n    max_size_bytes: 10000000000\n    # cache.max_tree_cache_set_duration (time.Duration): The max amount of \n    # time to wait for unfinished tree cache entries to be set.\n    max_tree_cache_set_duration: 1s\n    # cache.tree_cache_min_descendents (int): The min number of descendents a \n    # node must parent in order to be cached\n    tree_cache_min_descendents: 3\n    # cache.tree_cache_min_level (int): The min level at which the tree may be \n    # cached. 0 is the root\n    tree_cache_min_level: 2\n    # cache.tree_cache_seed (string): If set, hash this with digests before \n    # caching / reading from tree cache\n    tree_cache_seed: treecache-09032024\n    # cache.tree_cache_splitting (bool): If true, try to split up TreeCache \n    # entries to save space.\n    tree_cache_splitting: false\n    # cache.tree_cache_splitting_min_size (int): Minimum number of files in a \n    # subtree before we\'ll split it in the treecache.\n    tree_cache_splitting_min_size: 10000\n    # cache.tree_cache_write_probability (float64): Write to the tree cache \n    # with this probability\n    tree_cache_write_probability: 0.1\n    # cache.zstd_transcoding_enabled (bool): Whether to accept requests to \n    # read/write zstd-compressed blobs, compressing/decompressing \n    # outgoing/incoming blobs on the fly.\n    zstd_transcoding_enabled: true\ndatabase:\n    # database.advanced_data_source (db.AdvancedConfig): Alternative to the \n    # database.data_source flag that allows finer control over database \n    # settings as well as allowing use of AWS IAM credentials. For most users, \n    # database.data_source is a simpler configuration method.\n    advanced_data_source:\n        driver: "" # The driver to use: one of sqlite3, mysql, or postgresql. (type: string)\n        endpoint: "" # Typically the host:port combination of the database server. (type: string)\n        username: "" # Username to use when connecting. (type: string)\n        password: "" # Password to use when connecting. Not used if AWS IAM is enabled. (type: string)\n        db_name: "" # The name of the database to use for BuildBuddy data. (type: string)\n        region: "" # Region of the database instance. Required if AWS IAM is enabled. (type: string)\n        use_aws_iam: false # If enabled, AWS IAM authentication is used instead of fixed credentials. Make sure the endpoint includes the port, otherwise IAM-based auth will fail. (type: bool)\n        params: "" # Optional parameters to pass to the database driver (in format key1=val1&key2=val2) (type: string)\n    # database.advanced_read_replica (db.AdvancedConfig): Advanced alternative \n    # to database.read_replica. Refer to database.advanced for more \n    # information.\n    advanced_read_replica:\n        driver: "" # The driver to use: one of sqlite3, mysql, or postgresql. (type: string)\n        endpoint: "" # Typically the host:port combination of the database server. (type: string)\n        username: "" # Username to use when connecting. (type: string)\n        password: "" # Password to use when connecting. Not used if AWS IAM is enabled. (type: string)\n        db_name: "" # The name of the database to use for BuildBuddy data. (type: string)\n        region: "" # Region of the database instance. Required if AWS IAM is enabled. (type: string)\n        use_aws_iam: false # If enabled, AWS IAM authentication is used instead of fixed credentials. Make sure the endpoint includes the port, otherwise IAM-based auth will fail. (type: bool)\n        params: "" # Optional parameters to pass to the database driver (in format key1=val1&key2=val2) (type: string)\n    # database.conn_max_lifetime_seconds (int): The maximum lifetime of a \n    # connection to the db\n    conn_max_lifetime_seconds: 0\n    # database.data_source (string): The SQL database to connect to, specified \n    # as a connection string.\n    data_source: sqlite3:///tmp/buildbuddy.db\n    # database.log_queries (bool): If true, log all queries\n    log_queries: false\n    # database.max_idle_conns (int): The maximum number of idle connections to \n    # maintain to the db\n    max_idle_conns: 0\n    # database.max_open_conns (int): The maximum number of open connections to \n    # maintain to the db\n    max_open_conns: 0\n    # database.print_schema_changes_and_exit (bool): If set, print schema \n    # changes from auto-migration, then exit the program.\n    print_schema_changes_and_exit: false\n    # database.read_replica (string): A secondary, read-only SQL database to \n    # connect to, specified as a connection string.\n    read_replica: ""\n    # database.slow_query_threshold (time.Duration): Queries longer than this \n    # duration will be logged with a \'Slow SQL\' warning.\n    slow_query_threshold: 500ms\n    # database.stats_poll_interval (time.Duration): How often to poll the DB \n    # client for connection stats (default: \'5s\').\n    stats_poll_interval: 5s\nexecutor:\n    # executor.host_id (string): Optional: Allows for manual specification of \n    # an executor\'s host id. If not set, a random UUID will be used.\n    host_id: ""\ngithub:\n    # github.access_token (string): The GitHub access token used to post \n    # GitHub commit statuses. ** Enterprise only **\n    access_token: ""\n    # github.client_id (string): The client ID of your GitHub Oauth App. ** \n    # Enterprise only **\n    client_id: ""\n    # github.client_secret (string): The client secret of your GitHub Oauth \n    # App. ** Enterprise only **\n    client_secret: ""\n    # github.enterprise_host (string): The Github enterprise hostname to use \n    # if using GitHub enterprise server, not including https:// and no \n    # trailing slash.\n    enterprise_host: ""\n    # github.jwt_key (string): The key to use when signing JWT tokens for \n    # github auth.\n    jwt_key: ""\n    # github.status_name_suffix (string): Suffix to be appended to all \n    # reported GitHub status names. Useful for differentiating BuildBuddy \n    # deployments. For example: \'(dev)\' ** Enterprise only **\n    status_name_suffix: ""\n    # github.status_per_test_target (bool): If true, report status per test \n    # target. ** Enterprise only **\n    status_per_test_target: false\ngossip:\n    # gossip.join ([]string): The nodes to join/gossip with. Ex. \n    # \'1.2.3.4:1991,2.3.4.5:1991...\'\n    join: []\n    # gossip.listen_addr (string): The address to listen for gossip traffic \n    # on. Ex. \'localhost:1991\'\n    listen_addr: ""\n    # gossip.node_name (string): The gossip node\'s name. If empty will default \n    # to host_id.\'\n    node_name: ""\n    # gossip.secret_key (string): The value should be either 16, 24, or 32 \n    # bytes.\n    secret_key: ""\ngrpc_client:\n    # grpc_client.enable_pool_cache (bool): Whether or not to enable the \n    # connection pool cache.\n    enable_pool_cache: false\n    # grpc_client.pool_size (int): Number of connections to create to each \n    # target.\n    pool_size: 15\nintegrations:\n    invocation_upload:\n        # integrations.invocation_upload.aws_credentials (string): Credentials \n        # CSV file for Amazon s3 invocation upload webhook. ** Enterprise only \n        # **\n        aws_credentials: ""\n        # integrations.invocation_upload.enabled (bool): Whether to upload \n        # webhook data to the webhook URL configured per-Group. ** Enterprise \n        # only **\n        enabled: false\n        # integrations.invocation_upload.gcs_credentials (string): Credentials \n        # JSON for the Google service account used to authenticate when GCS is \n        # used as the invocation upload target. ** Enterprise only **\n        gcs_credentials: ""\n    slack:\n        # integrations.slack.webhook_url (string): A Slack webhook url to post \n        # build update messages to.\n        webhook_url: ""\nmonitoring:\n    basic_auth:\n        # monitoring.basic_auth.password (string): Optional password for basic \n        # auth on the monitoring port.\n        password: ""\n        # monitoring.basic_auth.username (string): Optional username for basic \n        # auth on the monitoring port.\n        username: ""\nolap_database:\n    # olap_database.cluster_name (string): The cluster name of the database\n    cluster_name: \'{cluster}\'\n    # olap_database.enable_data_replication (bool): If true, data replication \n    # is enabled.\n    enable_data_replication: false\n    # olap_database.replica_name (string): The replica name of the table in \n    # zookeeper\n    replica_name: \'{replica}\'\n    # olap_database.zoo_path (string): The path to the table name in \n    # zookeeper, used to set up data replication\n    zoo_path: /clickhouse/{installation}/{cluster}/tables/{shard}/{database}/{table}\nremote_execution:\n    # remote_execution.enable_executor_key_creation (bool): If enabled, UI \n    # will allow executor keys to be created.\n    enable_executor_key_creation: false\n    # remote_execution.enable_remote_exec (bool): If true, enable remote-exec. \n    # ** Enterprise only **\n    enable_remote_exec: true\n    # remote_execution.enable_user_owned_executors (bool): If enabled, users \n    # can register their own executors with the scheduler.\n    enable_user_owned_executors: false\n    # remote_execution.enable_workflows (bool): Whether to enable BuildBuddy \n    # workflows.\n    enable_workflows: false\n    # remote_execution.force_user_owned_darwin_executors (bool): If enabled, \n    # darwin actions will always run on user-owned executors.\n    force_user_owned_darwin_executors: false\nssl:\n    # ssl.cert_file (string): Path to a PEM encoded certificate file to use \n    # for TLS if not using ACME.\n    cert_file: ""\n    # ssl.client_ca_cert (string): PEM encoded certificate authority used to \n    # issue client certificates for mTLS auth.\n    client_ca_cert: ""\n    # ssl.client_ca_cert_file (string): Path to a PEM encoded certificate \n    # authority file used to issue client certificates for mTLS auth.\n    client_ca_cert_file: ""\n    # ssl.client_ca_key (string): PEM encoded certificate authority key used \n    # to issue client certificates for mTLS auth.\n    client_ca_key: ""\n    # ssl.client_ca_key_file (string): Path to a PEM encoded certificate \n    # authority key file used to issue client certificates for mTLS auth.\n    client_ca_key_file: ""\n    # ssl.client_cert_lifespan (time.Duration): The duration client \n    # certificates are valid for. Ex: \'730h\' for one month. If not set, \n    # defaults to 100 years.\n    client_cert_lifespan: 876000h0m0s\n    # ssl.default_host (string): Host name to use for ACME generated cert if \n    # TLS request does not contain SNI.\n    default_host: ""\n    # ssl.enable_ssl (bool): Whether or not to enable SSL/TLS on gRPC \n    # connections (gRPCS).\n    enable_ssl: false\n    # ssl.host_whitelist ([]string): Cloud-Only\n    host_whitelist: []\n    # ssl.key_file (string): Path to a PEM encoded key file to use for TLS if \n    # not using ACME.\n    key_file: ""\n    # ssl.self_signed (bool): If true, a self-signed cert will be generated \n    # for TLS termination.\n    self_signed: false\n    # ssl.upgrade_insecure (bool): True if http requests should be redirected \n    # to https. Assumes http traffic is served on port 80 and https traffic is \n    # served on port 443 (typically via an ingress / load balancer).\n    upgrade_insecure: false\n    # ssl.use_acme (bool): Whether or not to automatically configure SSL certs \n    # using ACME. If ACME is enabled, cert_file and key_file should not be \n    # set.\n    use_acme: false\nstorage:\n    aws_s3:\n        # storage.aws_s3.bucket (string): The AWS S3 bucket to store files in.\n        bucket: ""\n        # storage.aws_s3.credentials_profile (string): A custom credentials \n        # profile to use.\n        credentials_profile: ""\n        # storage.aws_s3.disable_ssl (bool): Disables the use of SSL, useful \n        # for configuring the use of MinIO. **DEPRECATED** Specify a non-HTTPS \n        # endpoint instead.\n        disable_ssl: false\n        # storage.aws_s3.endpoint (string): The AWS endpoint to use, useful \n        # for configuring the use of MinIO.\n        endpoint: ""\n        # storage.aws_s3.region (string): The AWS region.\n        region: ""\n        # storage.aws_s3.role_arn (string): The role ARN to use for web \n        # identity auth.\n        role_arn: ""\n        # storage.aws_s3.role_session_name (string): The role session name to \n        # use for web identity auth.\n        role_session_name: ""\n        # storage.aws_s3.s3_force_path_style (bool): Force path style urls for \n        # objects, useful for configuring the use of MinIO.\n        s3_force_path_style: false\n        # storage.aws_s3.static_credentials_id (string): Static credentials ID \n        # to use, useful for configuring the use of MinIO.\n        static_credentials_id: ""\n        # storage.aws_s3.static_credentials_secret (string): Static \n        # credentials secret to use, useful for configuring the use of MinIO.\n        static_credentials_secret: ""\n        # storage.aws_s3.static_credentials_token (string): Static credentials \n        # token to use, useful for configuring the use of MinIO.\n        static_credentials_token: ""\n        # storage.aws_s3.web_identity_token_file (string): The file path to \n        # the web identity token file.\n        web_identity_token_file: ""\n    azure:\n        # storage.azure.account_key (string): The key for the Azure storage \n        # account\n        account_key: ""\n        # storage.azure.account_name (string): The name of the Azure storage \n        # account\n        account_name: ""\n        # storage.azure.container_name (string): The name of the Azure storage \n        # container\n        container_name: ""\n    # storage.chunk_file_size_bytes (int): How many bytes to buffer in memory \n    # before flushing a chunk of build protocol data to disk.\n    chunk_file_size_bytes: 3000000\n    # storage.cleanup_batch_size (int): How many invocations to delete in each \n    # janitor cleanup task\n    cleanup_batch_size: 10\n    # storage.disable_persist_cache_artifacts (bool): If disabled, buildbuddy \n    # will not persist cache artifacts in the blobstore. This may make older \n    # invocations not display properly.\n    disable_persist_cache_artifacts: false\n    disk:\n        # storage.disk.root_directory (string): The root directory to store \n        # all blobs in, if using disk based storage.\n        root_directory: /tmp/buildbuddy\n        # storage.disk.use_v2_layout (bool): If enabled, files will be stored \n        # using the v2 layout. See disk_cache.MigrateToV2Layout for a \n        # description.\n        use_v2_layout: false\n    # storage.enable_chunked_event_logs (bool): If true, Event logs will be \n    # stored separately from the invocation proto in chunks.\n    enable_chunked_event_logs: true\n    execution:\n        # storage.execution.cleanup_batch_size (int): How many invocations to \n        # delete in each janitor cleanup task\n        cleanup_batch_size: 200\n        # storage.execution.cleanup_interval (time.Duration): How often the \n        # janitor cleanup tasks will run\n        cleanup_interval: 5m0s\n        # storage.execution.cleanup_workers (int): How many cleanup tasks to \n        # run\n        cleanup_workers: 1\n        # storage.execution.ttl (time.Duration): The time, in seconds, to keep \n        # invocations before deletion. 0 disables invocation deletion.\n        ttl: 0s\n    gcs:\n        # storage.gcs.bucket (string): The name of the GCS bucket to store \n        # build artifact files in.\n        bucket: ""\n        # storage.gcs.credentials (string): Credentials in JSON format that \n        # will be used to authenticate to GCS.\n        credentials: ""\n        # storage.gcs.credentials_file (string): A path to a JSON credentials \n        # file that will be used to authenticate to GCS.\n        credentials_file: ""\n        # storage.gcs.project_id (string): The Google Cloud project ID of the \n        # project owning the above credentials and GCS bucket.\n        project_id: ""\n    # storage.path_prefix (string): The prefix directory to store all blobs in\n    path_prefix: ""\n    # storage.tempdir (string): Root directory for temporary files. Defaults \n    # to the OS-specific temp dir.\n    tempdir: /tmp\n    # storage.ttl_seconds (int): The time, in seconds, to keep invocations \n    # before deletion. 0 disables invocation deletion.\n    ttl_seconds: 0\n'})})}function s(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(r,{...e})}):r(e)}function l(e){const n={code:"code",pre:"pre",...(0,i.R)(),...e.components};return(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:'# Unstructured settings\n\n# app_directory (string): the directory containing app binary files to host\napp_directory: ""\n# auto_migrate_db (bool): If true, attempt to automigrate the db when \n# connecting\nauto_migrate_db: true\n# auto_migrate_db_and_exit (bool): If true, attempt to automigrate the db when \n# connecting, then exit the program.\nauto_migrate_db_and_exit: false\n# block_profile_rate (int): The fraction of goroutine blocking events \n# reported. (1/rate, 0 disables)\nblock_profile_rate: 0\n# cache_stats_finalization_delay (time.Duration): The time allowed for all \n# metrics collectors across all apps to flush their local cache stats to the \n# backing storage, before finalizing stats in the DB.\ncache_stats_finalization_delay: 500ms\n# cleanup_interval (time.Duration): How often the janitor cleanup tasks will \n# run\ncleanup_interval: 10m0s\n# cleanup_workers (int): How many cleanup tasks to run\ncleanup_workers: 1\n# debug_enable_anonymous_runner_recycling (bool): Whether to enable runner \n# recycling for unauthenticated requests. For debugging purposes only - do not \n# use in production.\ndebug_enable_anonymous_runner_recycling: false\n# debug_use_local_images_only (bool): Do not pull OCI images and only used \n# locally cached images. This can be set to test local image builds during \n# development without needing to push to a container registry. Not intended \n# for production use.\ndebug_use_local_images_only: false\n# disable_ga (bool): If true; ga will be disabled\ndisable_ga: false\n# disable_telemetry (bool): If true; telemetry will be disabled\ndisable_telemetry: false\n# drop_invocation_pk_cols (bool): If true, attempt to drop invocation PK cols\ndrop_invocation_pk_cols: false\n# enable_cache_delete_api (bool): If true, enable access to cache delete API.\nenable_cache_delete_api: false\n# exit_when_ready (bool): If set, the app will exit as soon as it becomes \n# ready (useful for migrations)\nexit_when_ready: false\n# grpc_client_origin_header (string): Header value to set for \n# x-buildbuddy-origin.\ngrpc_client_origin_header: ""\n# grpc_max_recv_msg_size_bytes (int): Configures the max GRPC receive message \n# size [bytes]\ngrpc_max_recv_msg_size_bytes: 50000000\n# grpc_port (int): The port to listen for gRPC traffic on\ngrpc_port: 1985\n# grpcs_port (int): The port to listen for gRPCS traffic on\ngrpcs_port: 1986\n# internal_grpc_port (int): The port to listen for internal gRPC traffic on\ninternal_grpc_port: 1987\n# internal_grpcs_port (int): The port to listen for internal gRPCS traffic on\ninternal_grpcs_port: 1988\n# internal_http_port (int): The port to listen for internal HTTP traffic\ninternal_http_port: 0\n# js_entry_point_path (string): Absolute URL path of the app JS entry point\njs_entry_point_path: /app/app_bundle/app.js?hash={APP_BUNDLE_HASH}\n# listen (string): The interface to listen on (default: 0.0.0.0)\nlisten: 0.0.0.0\n# log_deletion_errors (bool): If true; log errors when ttl-deleting expired \n# data\nlog_deletion_errors: false\n# log_goroutine_profile_on_shutdown (bool): Whether to log all goroutine stack \n# traces on shutdown.\nlog_goroutine_profile_on_shutdown: false\n# max_shutdown_duration (time.Duration): Time to wait for shutdown\nmax_shutdown_duration: 25s\n# max_threads (int): The maximum number of threads to allow before panicking. \n# If unset, the golang default will be used (currently 10,000).\nmax_threads: 0\n# migrate_disk_cache_to_v2_and_exit (bool): If true, attempt to migrate disk \n# cache to v2 layout.\nmigrate_disk_cache_to_v2_and_exit: false\n# monitoring_port (int): The port to listen for monitoring traffic on\nmonitoring_port: 9090\n# mutex_profile_fraction (int): The fraction of mutex contention events \n# reported. (1/rate, 0 disables)\nmutex_profile_fraction: 0\n# port (int): The port to listen for HTTP traffic on\nport: 8080\n# redis_command_buffer_flush_period (time.Duration): How long to wait between \n# flushing buffered redis commands. Setting this to 0 will disable buffering \n# at the cost of higher redis QPS.\nredis_command_buffer_flush_period: 250ms\n# regions ([]region.Region): A list of regions that executors might be \n# connected to.\nregions: []\n# For example:\n# - name: "" # The user-friendly name of this region. Ex: Europe (type: string)\n#   server: "" # The http endpoint for this server, with the protocol. Ex: https://app.europe.buildbuddy.io (type: string)\n#   subdomains: "" # The format for subdomain urls of with a single * wildcard. Ex: https://*.europe.buildbuddy.io (type: string)\n\n# report_not_ready (bool): If set to true, the app will always report as being \n# unready.\nreport_not_ready: false\n# server_type (string): The server type to match on health checks\nserver_type: buildbuddy-server\n# shutdown_lameduck_duration (time.Duration): If set, the server will be \n# marked unready but not run shutdown functions until this period passes.\nshutdown_lameduck_duration: 0s\n# ssl_port (int): The port to listen for HTTPS traffic on\nssl_port: 8081\n# static_directory (string): the directory containing static files to host\nstatic_directory: ""\n# telemetry_endpoint (string): The telemetry endpoint to use\ntelemetry_endpoint: grpcs://t.buildbuddy.io:443\n# telemetry_interval (time.Duration): How often telemetry data will be \n# reported\ntelemetry_interval: 24h0m0s\n# telemetry_port (int): The port on which to listen for telemetry events\ntelemetry_port: 9099\n# verbose_telemetry_client (bool): If true; print telemetry client information\nverbose_telemetry_client: false\n# verbose_telemetry_server (bool): If true; print telemetry server information\nverbose_telemetry_server: false\n# zone_override (string): A value that will override the auto-detected zone. \n# Ignored if empty\nzone_override: ""\n\n# Structured settings\n\napi:\n    # api.api_key (string): The default API key to use for on-prem enterprise \n    # deploys with a single organization/group. **DEPRECATED** Manual API key \n    # specification is no longer supported; to retrieve specific API keys \n    # programmatically, please use the API key table. This field will still \n    # specify an API key to redact in case a manual API key was specified when \n    # buildbuddy was first set up.\n    api_key: ""\n    # api.enable_api (bool): Whether or not to enable the BuildBuddy API.\n    enable_api: true\n    # api.enable_cache (bool): Whether or not to enable the API cache.\n    enable_cache: false\n    # api.enable_metrics_api (bool): If true, enable access to metrics API.\n    enable_metrics_api: false\napp:\n    # app.add_user_to_domain_group (bool): Cloud-Only\n    add_user_to_domain_group: false\n    # app.admin_only_create_group (bool): If true, only admins of an existing \n    # group can create a new groups.\n    admin_only_create_group: false\n    # app.audit_logs_enabled (bool): Whether to log administrative events to \n    # an audit log. Requires OLAP database to be configured.\n    audit_logs_enabled: false\n    # app.audit_logs_ui_enabled (bool): If set, the audit logs UI will be \n    # accessible from the sidebar.\n    audit_logs_ui_enabled: false\n    # app.bazel_buttons_enabled (bool): If set, show remote bazel buttons in \n    # the UI.\n    bazel_buttons_enabled: false\n    # app.blended_invocation_search_enabled (bool): If true, \n    # InvocationSearchService will query clickhouse for all searches, filling \n    # in in-progress invocations from the regular DB.\n    blended_invocation_search_enabled: false\n    # app.build_buddy_url (URL): The external URL where your BuildBuddy \n    # instance can be found.\n    build_buddy_url: http://localhost:8080\n    # app.cache_api_url (URL): Overrides the default remote cache protocol \n    # gRPC address shown by BuildBuddy on the configuration screen.\n    cache_api_url: ""\n    client_identity:\n        # app.client_identity.client (string): The client identifier to place \n        # in the identity header.\n        client: ""\n        # app.client_identity.key (string): The key used to sign and verify \n        # identity JWTs.\n        key: ""\n        # app.client_identity.origin (string): The origin identifier to place \n        # in the identity header.\n        origin: ""\n    # app.code_editor_enabled (bool): If set, code editor functionality will \n    # be enabled.\n    code_editor_enabled: false\n    # app.code_editor_v2_enabled (bool): If set, show v2 of code editor that \n    # stores state on server instead of local storage.\n    code_editor_v2_enabled: false\n    # app.code_review_enabled (bool): If set, show the code review UI.\n    code_review_enabled: false\n    # app.codesearch_backend (string): Address and port to connect to\n    codesearch_backend: ""\n    # app.codesearch_enabled (bool): If set, show the code search UI.\n    codesearch_enabled: false\n    # app.community_links_enabled (bool): If set, show links to BuildBuddy \n    # community in the UI.\n    community_links_enabled: true\n    # app.create_group_per_user (bool): Cloud-Only\n    create_group_per_user: false\n    # app.customer_managed_encryption_keys_enabled (bool): If set, show \n    # customer-managed encryption configuration UI.\n    customer_managed_encryption_keys_enabled: false\n    # app.default_login_slug (string): If set, the login page will default to \n    # using this slug.\n    default_login_slug: ""\n    # app.default_redis_target (string): A Redis target for storing remote \n    # shared state. To ease migration, the redis target from the remote \n    # execution config will be used if this value is not specified.\n    default_redis_target: ""\n    default_sharded_redis:\n        # app.default_sharded_redis.password (string): Redis password\n        password: ""\n        # app.default_sharded_redis.shards ([]string): Ordered list of Redis \n        # shard addresses.\n        shards: []\n        # app.default_sharded_redis.username (string): Redis username\n        username: ""\n    # app.default_subdomains ([]string): List of subdomains that should not be \n    # handled as user-owned subdomains.\n    default_subdomains: []\n    # app.default_to_dense_mode (bool): Enables the dense UI mode by default.\n    default_to_dense_mode: false\n    # app.deprecate_anonymous_access (bool): If true, log a warning in the \n    # bazel console when clients are unauthenticated\n    deprecate_anonymous_access: false\n    # app.disable_cert_config (bool): If true, the certificate based auth \n    # option will not be shown in the config widget.\n    disable_cert_config: false\n    # app.enable_canaries (bool): If true, enable slow function canaries\n    enable_canaries: true\n    # app.enable_execution_trends (bool): If enabled, fill execution trend \n    # stats in GetTrendResponse\n    enable_execution_trends: true\n    # app.enable_grpc_metrics_by_group_id (bool): If enabled, grpc metrics by \n    # group ID will be recorded\n    enable_grpc_metrics_by_group_id: false\n    # app.enable_invocation_stat_percentiles (bool): If enabled, provide \n    # percentile breakdowns for invocation stats in GetTrendResponse\n    enable_invocation_stat_percentiles: true\n    # app.enable_prometheus_histograms (bool): If true, collect prometheus \n    # histograms for all RPCs\n    enable_prometheus_histograms: true\n    # app.enable_quota_management (bool): If set, quota management will be \n    # enabled\n    enable_quota_management: false\n    # app.enable_read_from_olap_db (bool): If enabled, read from OLAP DB\n    enable_read_from_olap_db: true\n    # app.enable_read_target_statuses_from_olap_db (bool): If enabled, read \n    # target statuses from OLAP DB\n    enable_read_target_statuses_from_olap_db: false\n    # app.enable_secret_service (bool): If set, secret service will be enabled\n    enable_secret_service: false\n    # app.enable_structured_logging (bool): If true, log messages will be \n    # json-formatted.\n    enable_structured_logging: false\n    # app.enable_subdomain_matching (bool): If true, request subdomain will be \n    # taken into account when determining what request restrictions should be \n    # applied.\n    enable_subdomain_matching: false\n    # app.enable_target_tracking (bool): Cloud-Only\n    enable_target_tracking: false\n    # app.enable_write_executions_to_olap_db (bool): If enabled, complete \n    # Executions will be flushed to OLAP DB\n    enable_write_executions_to_olap_db: false\n    # app.enable_write_test_target_statuses_to_olap_db (bool): If enabled, \n    # test target statuses will be flushed to OLAP DB\n    enable_write_test_target_statuses_to_olap_db: false\n    # app.enable_write_to_olap_db (bool): If enabled, complete invocations \n    # will be flushed to OLAP DB\n    enable_write_to_olap_db: true\n    # app.events_api_url (URL): Overrides the default build event protocol \n    # gRPC address shown by BuildBuddy on the configuration screen.\n    events_api_url: ""\n    # app.execution_search_enabled (bool): If set, fetch lists of executions \n    # from the OLAP DB in the trends UI.\n    execution_search_enabled: true\n    # app.expanded_suggestions_enabled (bool): If set, enable more build \n    # suggestions in the UI.\n    expanded_suggestions_enabled: false\n    # app.fetch_tags_drilldown_data (bool): If enabled, \n    # DrilldownType_TAG_DRILLDOWN_TYPE can be returned in \n    # GetStatDrilldownRequests\n    fetch_tags_drilldown_data: true\n    # app.finer_time_buckets (bool): If enabled, split trends and drilldowns \n    # into smaller time buckets when the user has a smaller date range \n    # selected.\n    finer_time_buckets: false\n    # app.grpc_max_recv_msg_size_bytes (int): DEPRECATED: use \n    # --grpc_max_recv_msg_size_bytes instead\n    grpc_max_recv_msg_size_bytes: 50000000\n    # app.grpc_over_http_port_enabled (bool): Enables grpc traffic to be \n    # served over the http port.\n    grpc_over_http_port_enabled: true\n    # app.ignore_forced_tracing_header (bool): If set, we will not honor the \n    # forced tracing header.\n    ignore_forced_tracing_header: false\n    # app.invocation_log_streaming_enabled (bool): If set, the UI will stream \n    # invocation logs instead of polling.\n    invocation_log_streaming_enabled: false\n    # app.invocation_summary_available_usec (int64): The timstamp when the \n    # invocation summary is available in the DB\n    invocation_summary_available_usec: 0\n    # app.ip_rules_ui_enabled (bool): If set, show the IP rules tab in \n    # settings page.\n    ip_rules_ui_enabled: false\n    # app.log_enable_gcp_logging_format (bool): If true, the output structured \n    # logs will be compatible with format expected by GCP Logging.\n    log_enable_gcp_logging_format: false\n    # app.log_enable_grpc_request (bool): If true, log grpc request when log \n    # level is default\n    log_enable_grpc_request: true\n    # app.log_error_stack_traces (bool): If true, stack traces will be printed \n    # for errors that have them.\n    log_error_stack_traces: false\n    # app.log_gcp_log_id (string): The log ID to log to in GCP (if any).\n    log_gcp_log_id: ""\n    # app.log_gcp_project_id (string): The project ID to log to in GCP (if \n    # any).\n    log_gcp_project_id: ""\n    # app.log_include_short_file_name (bool): If true, log messages will \n    # include shortened originating file name.\n    log_include_short_file_name: false\n    # app.log_level (string): The desired log level. Logs with a level >= this \n    # level will be emitted. One of {\'fatal\', \'error\', \'warn\', \'info\', \n    # \'debug\'}\n    log_level: info\n    # app.new_trends_ui_enabled (bool): DEPRECATED: If set, show a new trends \n    # UI with a bit more organization.\n    new_trends_ui_enabled: false\n    # app.no_default_user_group (bool): Cloud-Only\n    no_default_user_group: false\n    # app.olap_invocation_search_enabled (bool): If true, \n    # InvocationSearchService will query clickhouse for a few impossibly slow \n    # queries (i.e., tags), but mostly use the regular DB.\n    olap_invocation_search_enabled: true\n    # app.org_admin_api_key_creation_enabled (bool): If set, SCIM API keys \n    # will be able to be created in the UI.\n    org_admin_api_key_creation_enabled: false\n    # app.paginate_invocations (bool): If true, paginate invocations returned \n    # to the UI.\n    paginate_invocations: true\n    # app.pattern_filter_enabled (bool): If set, allow filtering by pattern in \n    # the client.\n    pattern_filter_enabled: true\n    # app.popup_auth_enabled (bool): Whether popup windows should be used for \n    # authentication.\n    popup_auth_enabled: false\n    # app.proxy_targets ([]grpc_forward.proxyPair)\n    proxy_targets: []\n    # For example:\n    # - prefix: "" # The gRPC method prefix to match. (type: string)\n    #   target: "" # The gRPC target to forward requests to. (type: string)\n\n    # app.reader_writer_roles_enabled (bool): If set, Reader/Writer roles will \n    # be enabled in the user management UI.\n    reader_writer_roles_enabled: true\n    # app.region (string): The region in which the app is running.\n    region: ""\n    # app.remote_execution_api_url (URL): Overrides the default remote \n    # execution protocol gRPC address shown by BuildBuddy on the configuration \n    # screen.\n    remote_execution_api_url: ""\n    # app.restrict_bytestream_dialing (bool): If true, only allow dialing \n    # localhost or the configured cache backend for bytestream requests.\n    restrict_bytestream_dialing: false\n    # app.streaming_http_enabled (bool): Whether to support server-streaming \n    # http requests between server and web UI.\n    streaming_http_enabled: false\n    # app.strict_csp_enabled (bool): If set, set a strict CSP header. \n    # Violations are logged at warning level.\n    strict_csp_enabled: false\n    # app.tags_enabled (bool): Enable setting tags on invocations via \n    # build_metadata\n    tags_enabled: false\n    # app.tags_ui_enabled (bool): If set, expose tags data and let users \n    # filter by tag.\n    tags_ui_enabled: false\n    # app.target_flakes_ui_enabled (bool): If set, show some fancy new \n    # features for analyzing flakes.\n    target_flakes_ui_enabled: false\n    # app.test_grid_v2_enabled (bool): Whether to enable test grid V2\n    test_grid_v2_enabled: true\n    # app.test_output_manifests_enabled (bool): If set, the target page will \n    # render the contents of test output zips.\n    test_output_manifests_enabled: true\n    # app.timeseries_charts_in_timing_profile_enabled (bool): If set, charts \n    # with sampled time series data (such as CPU and memory usage) will be \n    # shown\n    timeseries_charts_in_timing_profile_enabled: true\n    # app.trace_fraction (float64): Fraction of requests to sample for \n    # tracing.\n    trace_fraction: 0\n    # app.trace_fraction_overrides ([]string): Tracing fraction override based \n    # on name in format name=fraction.\n    trace_fraction_overrides: []\n    # app.trace_jaeger_collector (string): Address of the Jager collector \n    # endpoint where traces will be sent.\n    trace_jaeger_collector: ""\n    # app.trace_project_id (string): Optional GCP project ID to export traces \n    # to. If not specified, determined from default credentials or metadata \n    # server if running on GCP.\n    trace_project_id: ""\n    # app.trace_service_name (string): Name of the service to associate with \n    # traces.\n    trace_service_name: ""\n    # app.trace_viewer_enabled (bool): Whether the new trace viewer is \n    # enabled.\n    trace_viewer_enabled: false\n    # app.trends_heatmap_enabled (bool): If set, enable a fancy heatmap UI for \n    # exploring build trends.\n    trends_heatmap_enabled: true\n    # app.trends_range_selection (bool): If set, let users drag to select time \n    # ranges in the trends UI.\n    trends_range_selection: true\n    # app.trends_summary_enabled (bool): If set, show the new \'summary\' \n    # section at the top of the trends UI.\n    trends_summary_enabled: false\n    # app.usage_enabled (bool): If set, the usage page will be enabled in the \n    # UI.\n    usage_enabled: false\n    # app.usage_start_date (string): If set, usage data will only be viewable \n    # on or after this timestamp. Specified in RFC3339 format, like \n    # 2021-10-01T00:00:00Z\n    usage_start_date: ""\n    # app.usage_tracking_enabled (bool): If set, enable usage data collection.\n    usage_tracking_enabled: false\n    # app.use_timezone_in_heatmap_queries (bool): If enabled, use timezone \n    # instead of \'timezone offset\' to compute day boundaries in heatmap \n    # queries.\n    use_timezone_in_heatmap_queries: true\n    # app.user_management_enabled (bool): If set, the user management page \n    # will be enabled in the UI. **DEPRECATED** This flag has no effect and \n    # will be removed in the future.\n    user_management_enabled: true\n    # app.user_owned_keys_enabled (bool): If true, enable user-owned API keys.\n    user_owned_keys_enabled: false\nauth:\n    # auth.admin_group_id (string): ID of a group whose members can perform \n    # actions only accessible to server admins.\n    admin_group_id: ""\n    api_key_encryption:\n        # auth.api_key_encryption.encrypt_new_keys (bool): If enabled, all new \n        # API keys will be written in an encrypted format.\n        encrypt_new_keys: false\n        # auth.api_key_encryption.encrypt_old_keys (bool): If enabled, all \n        # existing unencrypted keys will be encrypted on startup. The \n        # unencrypted keys will remain in the database and will need to be \n        # cleared manually after verifying the success of the migration.\n        encrypt_old_keys: false\n        # auth.api_key_encryption.key (string): Base64-encoded 256-bit \n        # encryption key for API keys.\n        key: ""\n    # auth.api_key_group_cache_ttl (time.Duration): TTL for API Key to Group \n    # caching. Set to \'0\' to disable cache.\n    api_key_group_cache_ttl: 5m0s\n    # auth.disable_refresh_token (bool): If true, the offline_access scope \n    # which requests refresh tokens will not be requested.\n    disable_refresh_token: false\n    # auth.domain_wide_cookies (bool): If true, cookies will have domain set \n    # so that they are accessible on domain and all subdomains.\n    domain_wide_cookies: false\n    # auth.enable_anonymous_usage (bool): If true, unauthenticated build \n    # uploads will still be allowed but won\'t be associated with your \n    # organization.\n    enable_anonymous_usage: false\n    # auth.enable_scim (bool): Whether or not to enable SCIM.\n    enable_scim: false\n    # auth.enable_self_auth (bool): If true, enables a single user login via \n    # an oauth provider on the buildbuddy server. Recommend use only when \n    # server is behind a firewall; this option may allow anyone with access to \n    # the webpage admin rights to your buildbuddy installation. ** Enterprise \n    # only **\n    enable_self_auth: false\n    # auth.force_approval (bool): If true, when a user doesn\'t have a session \n    # (first time logging in, or manually logged out) force the auth provider \n    # to show the consent screen allowing the user to select an account if \n    # they have multiple. This isn\'t supported by all auth providers.\n    force_approval: false\n    # auth.https_only_cookies (bool): If true, cookies will only be set over \n    # https connections.\n    https_only_cookies: false\n    ip_rules:\n        # auth.ip_rules.allow_ipv6 (bool): If true, IPv6 rules will be \n        # allowed.\n        allow_ipv6: false\n        # auth.ip_rules.cache_ttl (time.Duration): Duration of time IP rules \n        # will be cached in memory.\n        cache_ttl: 5m0s\n        # auth.ip_rules.enable (bool): If true, IP rules will be checked \n        # during auth.\n        enable: false\n    # auth.jwt_claims_cache_ttl (time.Duration): TTL for JWT string to parsed \n    # claims caching. Set to \'0\' to disable cache.\n    jwt_claims_cache_ttl: 15s\n    # auth.jwt_duration (time.Duration): Maximum lifetime of the generated \n    # JWT.\n    jwt_duration: 6h0m0s\n    # auth.jwt_key (string): The key to use when signing JWT tokens.\n    jwt_key: set_the_jwt_in_config\n    # auth.new_jwt_key (string): If set, JWT verifications will try both this \n    # and the old JWT key.\n    new_jwt_key: ""\n    # auth.oauth_providers ([]oidc.OauthProvider): The list of oauth providers \n    # to use to authenticate.\n    oauth_providers: []\n    # For example:\n    # - issuer_url: "" # The issuer URL of this OIDC Provider. (type: string)\n    #   client_id: "" # The oauth client ID. (type: string)\n    #   client_secret: "" # The oauth client secret. (type: string)\n    #   slug: "" # The slug of this OIDC Provider. (type: string)\n\n    saml:\n        # auth.saml.cert (string): PEM encoded certificate used for SAML auth.\n        cert: ""\n        # auth.saml.cert_file (string): Path to a PEM encoded certificate file \n        # used for SAML auth.\n        cert_file: ""\n        # auth.saml.key (string): PEM encoded certificate key used for SAML \n        # auth.\n        key: ""\n        # auth.saml.key_file (string): Path to a PEM encoded certificate key \n        # file used for SAML auth.\n        key_file: ""\n        # auth.saml.trusted_idp_cert_files ([]string): List of PEM-encoded \n        # trusted IDP certificates. Intended for testing and development only.\n        trusted_idp_cert_files: []\n    # auth.sign_using_new_jwt_key (bool): If true, new JWTs will be signed \n    # using the new JWT key.\n    sign_using_new_jwt_key: false\n    # auth.trust_xforwardedfor_header (bool): If true, client IP information \n    # will be retrieved from the X-Forwarded-For header. Should only be \n    # enabled if the BuildBuddy server is only accessible behind a trusted \n    # proxy.\n    trust_xforwardedfor_header: false\nbuild_event_proxy:\n    # build_event_proxy.buffer_size (int): The number of build events to \n    # buffer locally when proxying build events.\n    buffer_size: 100\n    # build_event_proxy.hosts ([]string): The list of hosts to pass build \n    # events onto.\n    hosts: []\ncache:\n    client:\n        # cache.client.ac_rpc_timeout (time.Duration): Maximum time a single \n        # Action Cache RPC can take.\n        ac_rpc_timeout: 15s\n        # cache.client.cas_rpc_timeout (time.Duration): Maximum time a single \n        # batch RPC or a single ByteStream chunk read can take.\n        cas_rpc_timeout: 1m0s\n        # cache.client.enable_upload_compression (bool): If true, enable \n        # compression of uploads to remote caches\n        enable_upload_compression: true\n    # cache.count_ttl (time.Duration): How long to go without receiving any \n    # cache requests for an invocation before deleting the invocation\'s counts \n    # from the metrics collector.\n    count_ttl: 24h0m0s\n    # cache.detailed_stats_enabled (bool): Whether to enable detailed stats \n    # recording for all cache requests.\n    detailed_stats_enabled: false\n    # cache.detailed_stats_ttl (time.Duration): How long to go without \n    # receiving any cache requests for an invocation before deleting the \n    # invocation\'s detailed results from the metrics collector. Has no effect \n    # if cache.detailed_stats_enabled is not set.\n    detailed_stats_ttl: 3h0m0s\n    # cache.directory_sizes_enabled (bool): If true, enable an RPC that \n    # computes the cumulative size of directories stored in the cache.\n    directory_sizes_enabled: false\n    disk:\n        # cache.disk.partition_mappings ([]disk.PartitionMapping)\n        partition_mappings: []\n        # For example:\n        # - group_id: "" # The Group ID to which this mapping applies. (type: string)\n        #   prefix: "" # The remote instance name prefix used to select this partition. (type: string)\n        #   partition_id: "" # The partition to use if the Group ID and prefix match. (type: string)\n\n        # cache.disk.partitions ([]disk.Partition)\n        partitions: []\n        # For example:\n        # - id: "" # The ID of the partition. (type: string)\n        #   max_size_bytes: 0 # Maximum size of the partition. (type: int64)\n        #   encryption_supported: false # Whether encrypted data can be stored on this partition. (type: bool)\n\n        # cache.disk.root_directory (string): The root directory to store all \n        # blobs in, if using disk based storage.\n        root_directory: ""\n        # cache.disk.use_v2_layout (bool): If enabled, files will be stored \n        # using the v2 layout. See disk_cache.MigrateToV2Layout for a \n        # description.\n        use_v2_layout: false\n    distributed_cache:\n        # cache.distributed_cache.cluster_size (int): The total number of \n        # nodes in this cluster. Required for health checking. ** Enterprise \n        # only **\n        cluster_size: 0\n        # cache.distributed_cache.consistent_hash_function (string): A \n        # consistent hash function to use when hashing data. CRC32 or SHA256\n        consistent_hash_function: CRC32\n        # cache.distributed_cache.consistent_hash_vnodes (int): The number of \n        # copies (virtual nodes) of each peer on the consistent hash ring\n        consistent_hash_vnodes: 100\n        # cache.distributed_cache.enable_backfill (bool): If enabled, digests \n        # written to avoid unavailable nodes will be backfilled when those \n        # nodes return\n        enable_backfill: true\n        # cache.distributed_cache.enable_local_compression_lookup (bool): If \n        # enabled, checks the local cache for compression support. If not set, \n        # distributed compression defaults to off.\n        enable_local_compression_lookup: true\n        # cache.distributed_cache.enable_local_writes (bool): If enabled, \n        # shortcuts distributed writes that belong to the local shard to local \n        # cache instead of making an RPC.\n        enable_local_writes: false\n        # cache.distributed_cache.group_name (string): A unique name for this \n        # distributed cache group. ** Enterprise only **\n        group_name: ""\n        # cache.distributed_cache.listen_addr (string): The address to listen \n        # for local BuildBuddy distributed cache traffic on.\n        listen_addr: ""\n        # cache.distributed_cache.lookaside_cache_size_bytes (int64): If > 0 ; \n        # lookaside cache will be enabled\n        lookaside_cache_size_bytes: 0\n        # cache.distributed_cache.lookaside_cache_ttl (time.Duration): How \n        # long to hold stuff in the lookaside cache. Should be << \n        # atime_update_threshold\n        lookaside_cache_ttl: 1m0s\n        # cache.distributed_cache.max_hinted_handoffs_per_peer (int64): The \n        # maximum number of hinted handoffs to keep in memory. Each hinted \n        # handoff is a digest (~64 bytes), prefix, and peer (40 bytes). So \n        # keeping around 100000 of these means an extra 10MB per peer.\n        max_hinted_handoffs_per_peer: 100000\n        # cache.distributed_cache.max_lookaside_entry_bytes (int64): The \n        # biggest allowed entry size in the lookaside cache.\n        max_lookaside_entry_bytes: 10000\n        # cache.distributed_cache.new_consistent_hash_function (string): A \n        # consistent hash function to use when hashing data. CRC32 or SHA256\n        new_consistent_hash_function: CRC32\n        # cache.distributed_cache.new_consistent_hash_vnodes (int): The number \n        # of copies of each peer on the new consistent hash ring\n        new_consistent_hash_vnodes: 100\n        # cache.distributed_cache.new_nodes ([]string): The new nodeset to add \n        # data too. Useful for migrations. ** Enterprise only **\n        new_nodes: []\n        # cache.distributed_cache.new_nodes_read_only (bool): If true, only \n        # attempt to read from the newNodes set; do not write to them yet\n        new_nodes_read_only: false\n        # cache.distributed_cache.nodes ([]string): The hardcoded list of peer \n        # distributed cache nodes. If this is set, redis_target will be \n        # ignored. ** Enterprise only **\n        nodes: []\n        # cache.distributed_cache.redis_target (string): Redis target for used \n        # for discovering distributed cache replicas. Target can be provided \n        # as either a redis connection URI or a host:port pair. URI schemas \n        # supported: redis[s]://[[USER][:PASSWORD]@][HOST][:PORT][/DATABASE] \n        # or unix://[[USER][:PASSWORD]@]SOCKET_PATH[?db=DATABASE] ** \n        # Enterprise only **\n        redis_target: ""\n        # cache.distributed_cache.replication_factor (int): How many total \n        # servers the data should be replicated to. Must be >= 1. ** \n        # Enterprise only **\n        replication_factor: 1\n    # cache.enable_tree_caching (bool): If true, cache GetTree responses (full \n    # and partial)\n    enable_tree_caching: true\n    gcs:\n        # cache.gcs.bucket (string): The name of the GCS bucket to store cache \n        # files in.\n        bucket: ""\n        # cache.gcs.credentials_file (string): A path to a JSON credentials \n        # file that will be used to authenticate to GCS.\n        credentials_file: ""\n        # cache.gcs.project_id (string): The Google Cloud project ID of the \n        # project owning the above credentials and GCS bucket.\n        project_id: ""\n        # cache.gcs.ttl_days (int64): The period after which cache files \n        # should be TTLd. Disabled if 0.\n        ttl_days: 0\n    # cache.in_memory (bool): Whether or not to use the in_memory cache.\n    in_memory: false\n    # cache.max_direct_write_size_bytes (int64): For bytestream requests \n    # smaller than this size, write straight to the cache without checking if \n    # the entry already exists.\n    max_direct_write_size_bytes: 16384\n    # cache.max_size_bytes (int64): How big to allow the cache to be (in \n    # bytes).\n    max_size_bytes: 10000000000\n    # cache.max_tree_cache_set_duration (time.Duration): The max amount of \n    # time to wait for unfinished tree cache entries to be set.\n    max_tree_cache_set_duration: 1s\n    # cache.memcache_targets ([]string): Deprecated. Use Redis Target instead.\n    memcache_targets: []\n    # cache.migration (migration_cache.MigrationConfig): Config to specify the \n    # details of a cache migration\n    migration:\n        src: null # (type: migration_cache.CacheConfig)\n        # For example:\n        #     disk: null # (type: migration_cache.DiskCacheConfig)\n        #     # For example:\n        #     #     root_directory: "" # (type: string)\n        #     #     partitions: [] # (type: []disk.Partition)\n        #     #     # For example:\n        #     #     # - id: "" # The ID of the partition. (type: string)\n        #     #     #   max_size_bytes: 0 # Maximum size of the partition. (type: int64)\n        #     #     #   encryption_supported: false # Whether encrypted data can be stored on this partition. (type: bool)\n        #     #     \n        #     #     partition_mappings: [] # (type: []disk.PartitionMapping)\n        #     #     # For example:\n        #     #     # - group_id: "" # The Group ID to which this mapping applies. (type: string)\n        #     #     #   prefix: "" # The remote instance name prefix used to select this partition. (type: string)\n        #     #     #   partition_id: "" # The partition to use if the Group ID and prefix match. (type: string)\n        #     #     \n        #     #     use_v2_layout: false # (type: bool)\n        #     #     \n        #     \n        #     pebble: null # (type: migration_cache.PebbleCacheConfig)\n        #     # For example:\n        #     #     name: "" # (type: string)\n        #     #     root_directory: "" # (type: string)\n        #     #     partitions: [] # (type: []disk.Partition)\n        #     #     # For example:\n        #     #     # - id: "" # The ID of the partition. (type: string)\n        #     #     #   max_size_bytes: 0 # Maximum size of the partition. (type: int64)\n        #     #     #   encryption_supported: false # Whether encrypted data can be stored on this partition. (type: bool)\n        #     #     \n        #     #     partition_mappings: [] # (type: []disk.PartitionMapping)\n        #     #     # For example:\n        #     #     # - group_id: "" # The Group ID to which this mapping applies. (type: string)\n        #     #     #   prefix: "" # The remote instance name prefix used to select this partition. (type: string)\n        #     #     #   partition_id: "" # The partition to use if the Group ID and prefix match. (type: string)\n        #     #     \n        #     #     max_size_bytes: 0 # (type: int64)\n        #     #     block_cache_size_bytes: 0 # (type: int64)\n        #     #     max_inline_file_size_bytes: 0 # (type: int64)\n        #     #     atime_update_threshold: null # (type: time.Duration)\n        #     #     atime_buffer_size: null # (type: int)\n        #     #     min_eviction_age: null # (type: time.Duration)\n        #     #     min_bytes_auto_zstd_compression: 0 # (type: int64)\n        #     #     average_chunk_size_bytes: 0 # (type: int)\n        #     #     clear_cache_on_startup: false # (type: bool)\n        #     #     active_key_version: null # (type: int64)\n        #     #     \n        #     \n\n        dest: null # (type: migration_cache.CacheConfig)\n        # For example:\n        #     disk: null # (type: migration_cache.DiskCacheConfig)\n        #     # For example:\n        #     #     root_directory: "" # (type: string)\n        #     #     partitions: [] # (type: []disk.Partition)\n        #     #     # For example:\n        #     #     # - id: "" # The ID of the partition. (type: string)\n        #     #     #   max_size_bytes: 0 # Maximum size of the partition. (type: int64)\n        #     #     #   encryption_supported: false # Whether encrypted data can be stored on this partition. (type: bool)\n        #     #     \n        #     #     partition_mappings: [] # (type: []disk.PartitionMapping)\n        #     #     # For example:\n        #     #     # - group_id: "" # The Group ID to which this mapping applies. (type: string)\n        #     #     #   prefix: "" # The remote instance name prefix used to select this partition. (type: string)\n        #     #     #   partition_id: "" # The partition to use if the Group ID and prefix match. (type: string)\n        #     #     \n        #     #     use_v2_layout: false # (type: bool)\n        #     #     \n        #     \n        #     pebble: null # (type: migration_cache.PebbleCacheConfig)\n        #     # For example:\n        #     #     name: "" # (type: string)\n        #     #     root_directory: "" # (type: string)\n        #     #     partitions: [] # (type: []disk.Partition)\n        #     #     # For example:\n        #     #     # - id: "" # The ID of the partition. (type: string)\n        #     #     #   max_size_bytes: 0 # Maximum size of the partition. (type: int64)\n        #     #     #   encryption_supported: false # Whether encrypted data can be stored on this partition. (type: bool)\n        #     #     \n        #     #     partition_mappings: [] # (type: []disk.PartitionMapping)\n        #     #     # For example:\n        #     #     # - group_id: "" # The Group ID to which this mapping applies. (type: string)\n        #     #     #   prefix: "" # The remote instance name prefix used to select this partition. (type: string)\n        #     #     #   partition_id: "" # The partition to use if the Group ID and prefix match. (type: string)\n        #     #     \n        #     #     max_size_bytes: 0 # (type: int64)\n        #     #     block_cache_size_bytes: 0 # (type: int64)\n        #     #     max_inline_file_size_bytes: 0 # (type: int64)\n        #     #     atime_update_threshold: null # (type: time.Duration)\n        #     #     atime_buffer_size: null # (type: int)\n        #     #     min_eviction_age: null # (type: time.Duration)\n        #     #     min_bytes_auto_zstd_compression: 0 # (type: int64)\n        #     #     average_chunk_size_bytes: 0 # (type: int)\n        #     #     clear_cache_on_startup: false # (type: bool)\n        #     #     active_key_version: null # (type: int64)\n        #     #     \n        #     \n\n        double_read_percentage: 0 # (type: float64)\n        decompress_percentage: 0 # (type: float64)\n        log_not_found_errors: false # (type: bool)\n        copy_chan_buffer_size: 0 # (type: int)\n        copy_chan_full_warning_interval_min: 0 # (type: int64)\n        max_copies_per_sec: 0 # (type: int)\n        num_copy_workers: 0 # (type: int)\n        async_dest_writes: false # (type: bool)\n    pebble:\n        # cache.pebble.active_key_version (int64): The key version new data \n        # will be written with. If negative, will write to the highest \n        # existing version in the database, or the highest known version if a \n        # new database is created.\n        active_key_version: -1\n        # cache.pebble.atime_buffer_size (int): Buffer up to this many atime \n        # updates in a channel before dropping atime updates\n        atime_buffer_size: 100000\n        # cache.pebble.atime_update_threshold (time.Duration): Don\'t update \n        # atime if it was updated more recently than this\n        atime_update_threshold: 10m0s\n        # cache.pebble.average_chunk_size_bytes (int): Average size of chunks \n        # that\'s stored in the cache. Disabled if 0.\n        average_chunk_size_bytes: 0\n        # cache.pebble.background_repair_frequency (time.Duration): How \n        # frequently to run period background repair tasks.\n        background_repair_frequency: 24h0m0s\n        # cache.pebble.background_repair_qps_limit (int): QPS limit for \n        # background repair modifications.\n        background_repair_qps_limit: 100\n        # cache.pebble.block_cache_size_bytes (int64): How much ram to give \n        # the block cache\n        block_cache_size_bytes: 1000000000\n        # cache.pebble.copy_partition_data (string): If set, all data will be \n        # copied from the source partition to the destination partition on \n        # startup. The cache will not serve data while the copy is in \n        # progress. Specified in format \n        # source_partition_id:destination_partition_id,\n        copy_partition_data: ""\n        # cache.pebble.delete_buffer_size (int): Buffer up to this many \n        # samples for eviction eviction\n        delete_buffer_size: 20\n        # cache.pebble.deletes_per_eviction (int): Maximum number keys to \n        # delete in one eviction attempt before resampling.\n        deletes_per_eviction: 5\n        # cache.pebble.dir_deletion_delay (time.Duration): How old directories \n        # must be before being eligible for deletion when empty\n        dir_deletion_delay: 1h0m0s\n        # cache.pebble.enable_table_bloom_filter (bool): If true, write bloom \n        # filter data with pebble SSTables.\n        enable_table_bloom_filter: false\n        # cache.pebble.eviction_rate_limit (int): Maximum number of entries to \n        # evict per second (per partition).\n        eviction_rate_limit: 300\n        # cache.pebble.force_calculate_metadata (bool): If set, partition size \n        # and counts will be calculated even if cached information is \n        # available.\n        force_calculate_metadata: false\n        # cache.pebble.force_compaction (bool): If set, compact the DB when \n        # it\'s created\n        force_compaction: false\n        # cache.pebble.include_metadata_size (bool): If true, include metadata \n        # size\n        include_metadata_size: false\n        # cache.pebble.max_inline_file_size_bytes (int64): Files smaller than \n        # this may be inlined directly into pebble\n        max_inline_file_size_bytes: 1024\n        # cache.pebble.migration_qps_limit (int): QPS limit for data version \n        # migration\n        migration_qps_limit: 50\n        # cache.pebble.min_bytes_auto_zstd_compression (int64): Blobs larger \n        # than this will be zstd compressed before written to disk.\n        min_bytes_auto_zstd_compression: 100\n        # cache.pebble.min_eviction_age (time.Duration): Don\'t evict anything \n        # unless it\'s been idle for at least this long\n        min_eviction_age: 6h0m0s\n        # cache.pebble.name (string): The name used in reporting cache metrics \n        # and status.\n        name: pebble_cache\n        # cache.pebble.num_delete_workers (int): Number of deletes in parallel\n        num_delete_workers: 2\n        # cache.pebble.orphan_delete_dry_run (bool): If set, log orphaned \n        # files instead of deleting them\n        orphan_delete_dry_run: true\n        # cache.pebble.partition_mappings ([]disk.PartitionMapping)\n        partition_mappings: []\n        # For example:\n        # - group_id: "" # The Group ID to which this mapping applies. (type: string)\n        #   prefix: "" # The remote instance name prefix used to select this partition. (type: string)\n        #   partition_id: "" # The partition to use if the Group ID and prefix match. (type: string)\n\n        # cache.pebble.partitions ([]disk.Partition)\n        partitions: []\n        # For example:\n        # - id: "" # The ID of the partition. (type: string)\n        #   max_size_bytes: 0 # Maximum size of the partition. (type: int64)\n        #   encryption_supported: false # Whether encrypted data can be stored on this partition. (type: bool)\n\n        # cache.pebble.root_directory (string): The root directory to store \n        # the database in.\n        root_directory: ""\n        # cache.pebble.sample_buffer_size (int): Buffer up to this many \n        # samples for eviction sampling\n        sample_buffer_size: 8000\n        # cache.pebble.sample_pool_size (int): How many deletion candidates to \n        # maintain between evictions\n        sample_pool_size: 500\n        # cache.pebble.sampler_iter_refresh_peroid (time.Duration): How often \n        # we refresh iterator in sampler\n        sampler_iter_refresh_peroid: 5m0s\n        # cache.pebble.samples_per_batch (int): How many keys we read forward \n        # every time we get a random key.\n        samples_per_batch: 10000\n        # cache.pebble.samples_per_eviction (int): How many records to sample \n        # on each eviction\n        samples_per_eviction: 20\n        # cache.pebble.scan_for_missing_files (bool): If set, scan all keys \n        # and check if external files are missing on disk. Deletes keys with \n        # missing files.\n        scan_for_missing_files: false\n        # cache.pebble.scan_for_orphaned_files (bool): If true, scan for \n        # orphaned files\n        scan_for_orphaned_files: false\n        # cache.pebble.warn_about_leaks (bool): If set, warn about leaked DB \n        # handles\n        warn_about_leaks: true\n    raft:\n        # cache.raft.atime_buffer_size (int): Buffer up to this many atime \n        # updates in a channel before dropping atime updates\n        atime_buffer_size: 100000\n        # cache.raft.atime_update_threshold (time.Duration): Don\'t update \n        # atime if it was updated more recently than this\n        atime_update_threshold: 3h0m0s\n        # cache.raft.atime_write_batch_size (int): Buffer this many writes \n        # before writing atime data\n        atime_write_batch_size: 100\n        # cache.raft.clear_cache_on_startup (bool): If set, remove all raft + \n        # cache data on start\n        clear_cache_on_startup: false\n        # cache.raft.clear_prev_cache_on_startup (bool): If set, remove all \n        # raft + cache data from previous run on start\n        clear_prev_cache_on_startup: false\n        # cache.raft.client_session_lifetime (time.Duration): The duration of \n        # a client session before it\'s reset\n        client_session_lifetime: 1h0m0s\n        # cache.raft.client_session_ttl (time.Duration): The duration we keep \n        # the sessions stored.\n        client_session_ttl: 24h0m0s\n        # cache.raft.dead_store_timeout (time.Duration): The amount of time \n        # after which we didn\'t receive alive status for a node, consider a \n        # store dead\n        dead_store_timeout: 5m0s\n        # cache.raft.delete_buffer_size (int): Buffer up to this many samples \n        # for eviction eviction\n        delete_buffer_size: 20\n        # cache.raft.deletes_per_eviction (int): Maximum number keys to delete \n        # in one eviction attempt before resampling.\n        deletes_per_eviction: 5\n        # cache.raft.enable_driver (bool): If true, enable placement driver\n        enable_driver: true\n        # cache.raft.enable_txn_cleanup (bool): If true, clean up stuck \n        # transactions periodically\n        enable_txn_cleanup: true\n        # cache.raft.entries_between_usage_checks (int): Re-check usage after \n        # this many updates\n        entries_between_usage_checks: 1000\n        # cache.raft.eviction_batch_size (int): Buffer this many writes before \n        # delete\n        eviction_batch_size: 100\n        # cache.raft.eviction_rate_limit (int): Maximum number of entries to \n        # evict per second (per partition).\n        eviction_rate_limit: 300\n        # cache.raft.grpc_addr (string): The address to listen for internal \n        # API traffic on. Ex. \'1993\'\n        grpc_addr: ""\n        # cache.raft.http_addr (string): The address to listen for HTTP raft \n        # traffic. Ex. \'1992\'\n        http_addr: ""\n        # cache.raft.leader_updated_chan_size (int64): The length of the \n        # leader updated channel; Should be greather than the max number of \n        # ranges on a node.\n        leader_updated_chan_size: 10000\n        # cache.raft.local_size_update_period (time.Duration): How often we \n        # update local size updates.\n        local_size_update_period: 10s\n        # cache.raft.max_range_size_bytes (int64): If set to a value greater \n        # than 0, ranges will be split until smaller than this size\n        max_range_size_bytes: 100000000\n        # cache.raft.min_eviction_age (time.Duration): Don\'t evict anything \n        # unless it\'s been idle for at least this long\n        min_eviction_age: 6h0m0s\n        # cache.raft.min_meta_range_replicas (int): The minimum number of \n        # replicas each range for meta range\n        min_meta_range_replicas: 5\n        # cache.raft.min_replicas_per_range (int): The minimum number of \n        # replicas each range should have\n        min_replicas_per_range: 3\n        # cache.raft.new_replica_grace_period (time.Duration): The amount of \n        # time we allow for a new replica to catch up to the leader\'s before \n        # we start to consider it to be behind.\n        new_replica_grace_period: 5m0s\n        # cache.raft.node_ready_chan_size (int64): The length of the node \n        # ready channel\n        node_ready_chan_size: 10000\n        # cache.raft.partition_mappings ([]disk.PartitionMapping)\n        partition_mappings: []\n        # For example:\n        # - group_id: "" # The Group ID to which this mapping applies. (type: string)\n        #   prefix: "" # The remote instance name prefix used to select this partition. (type: string)\n        #   partition_id: "" # The partition to use if the Group ID and prefix match. (type: string)\n\n        # cache.raft.partition_usage_delta_bytes_threshold (int): Gossip \n        # partition usage information if it has changed by more than this \n        # amount since the last gossip.\n        partition_usage_delta_bytes_threshold: 100000000\n        # cache.raft.partitions ([]disk.Partition)\n        partitions: []\n        # For example:\n        # - id: "" # The ID of the partition. (type: string)\n        #   max_size_bytes: 0 # Maximum size of the partition. (type: int64)\n        #   encryption_supported: false # Whether encrypted data can be stored on this partition. (type: bool)\n\n        # cache.raft.replica_scan_interval (time.Duration): The interval we \n        # wait to check if the replicas need to be queued for replication\n        replica_scan_interval: 1m0s\n        # cache.raft.root_directory (string): The root directory to use for \n        # storing cached data.\n        root_directory: ""\n        # cache.raft.sample_buffer_size (int): Buffer up to this many samples \n        # for eviction sampling\n        sample_buffer_size: 100\n        # cache.raft.sample_pool_size (int): How many deletion candidates to \n        # maintain between evictions\n        sample_pool_size: 500\n        # cache.raft.sampler_iter_refresh_peroid (time.Duration): How often we \n        # refresh iterator in sampler\n        sampler_iter_refresh_peroid: 5m0s\n        # cache.raft.samples_per_batch (int): How many keys we read forward \n        # every time we get a random key.\n        samples_per_batch: 10000\n        # cache.raft.samples_per_eviction (int): How many records to sample on \n        # each eviction\n        samples_per_eviction: 20\n        # cache.raft.suspect_store_duration (time.Duration): The amount of \n        # time we consider a node suspect after it becomes unavailable\n        suspect_store_duration: 30s\n        # cache.raft.zombie_min_duration (time.Duration): The minimum duration \n        # a replica must remain in a zombie state to be considered a zombie.\n        zombie_min_duration: 1m0s\n        # cache.raft.zombie_node_scan_interval (time.Duration): Check if one \n        # replica is a zombie every this often. 0 to disable.\n        zombie_node_scan_interval: 10s\n    redis:\n        # cache.redis.max_value_size_bytes (int64): The maximum value size to \n        # cache in redis (in bytes).\n        max_value_size_bytes: 10000000\n        # cache.redis.redis_target (string): A redis target for improved \n        # Caching/RBE performance. Target can be provided as either a redis \n        # connection URI or a host:port pair. URI schemas supported: \n        # redis[s]://[[USER][:PASSWORD]@][HOST][:PORT][/DATABASE] or \n        # unix://[[USER][:PASSWORD]@]SOCKET_PATH[?db=DATABASE] ** Enterprise \n        # only **\n        redis_target: ""\n        sharded:\n            # cache.redis.sharded.password (string): Redis password\n            password: ""\n            # cache.redis.sharded.shards ([]string): Ordered list of Redis \n            # shard addresses.\n            shards: []\n            # cache.redis.sharded.username (string): Redis username\n            username: ""\n    # cache.redis_target (string): A redis target for improved Caching/RBE \n    # performance. Target can be provided as either a redis connection URI or \n    # a host:port pair. URI schemas supported: \n    # redis[s]://[[USER][:PASSWORD]@][HOST][:PORT][/DATABASE] or \n    # unix://[[USER][:PASSWORD]@]SOCKET_PATH[?db=DATABASE] ** Enterprise only \n    # **\n    redis_target: ""\n    s3:\n        # cache.s3.bucket (string): The AWS S3 bucket to store files in.\n        bucket: ""\n        # cache.s3.credentials_profile (string): A custom credentials profile \n        # to use.\n        credentials_profile: ""\n        # cache.s3.disable_ssl (bool): Disables the use of SSL, useful for \n        # configuring the use of MinIO. **DEPRECATED** Specify a non-HTTPS \n        # endpoint instead.\n        disable_ssl: false\n        # cache.s3.endpoint (string): The AWS endpoint to use, useful for \n        # configuring the use of MinIO.\n        endpoint: ""\n        # cache.s3.path_prefix (string): Prefix inside the AWS S3 bucket to \n        # store files\n        path_prefix: ""\n        # cache.s3.region (string): The AWS region.\n        region: ""\n        # cache.s3.role_arn (string): The role ARN to use for web identity \n        # auth.\n        role_arn: ""\n        # cache.s3.role_session_name (string): The role session name to use \n        # for web identity auth.\n        role_session_name: ""\n        # cache.s3.s3_force_path_style (bool): Force path style urls for \n        # objects, useful for configuring the use of MinIO.\n        s3_force_path_style: false\n        # cache.s3.static_credentials_id (string): Static credentials ID to \n        # use, useful for configuring the use of MinIO.\n        static_credentials_id: ""\n        # cache.s3.static_credentials_secret (string): Static credentials \n        # secret to use, useful for configuring the use of MinIO.\n        static_credentials_secret: ""\n        # cache.s3.static_credentials_token (string): Static credentials token \n        # to use, useful for configuring the use of MinIO.\n        static_credentials_token: ""\n        # cache.s3.ttl_days (int): The period after which cache files should \n        # be TTLd. Disabled if 0.\n        ttl_days: 0\n        # cache.s3.web_identity_token_file (string): The file path to the web \n        # identity token file.\n        web_identity_token_file: ""\n    # cache.tree_cache_min_descendents (int): The min number of descendents a \n    # node must parent in order to be cached\n    tree_cache_min_descendents: 3\n    # cache.tree_cache_min_level (int): The min level at which the tree may be \n    # cached. 0 is the root\n    tree_cache_min_level: 2\n    # cache.tree_cache_seed (string): If set, hash this with digests before \n    # caching / reading from tree cache\n    tree_cache_seed: treecache-09032024\n    # cache.tree_cache_splitting (bool): If true, try to split up TreeCache \n    # entries to save space.\n    tree_cache_splitting: false\n    # cache.tree_cache_splitting_min_size (int): Minimum number of files in a \n    # subtree before we\'ll split it in the treecache.\n    tree_cache_splitting_min_size: 10000\n    # cache.tree_cache_write_probability (float64): Write to the tree cache \n    # with this probability\n    tree_cache_write_probability: 0.1\n    # cache.zstd_transcoding_enabled (bool): Whether to accept requests to \n    # read/write zstd-compressed blobs, compressing/decompressing \n    # outgoing/incoming blobs on the fly.\n    zstd_transcoding_enabled: true\ncrypter:\n    # crypter.key_reencrypt_interval (time.Duration): How frequently keys will \n    # be re-encrypted (to support key rotation).\n    key_reencrypt_interval: 6h0m0s\n    # crypter.key_ttl (time.Duration): The maximum amount of time a key can be \n    # cached without being re-verified before it is considered invalid.\n    key_ttl: 10m0s\ndatabase:\n    # database.advanced_data_source (db.AdvancedConfig): Alternative to the \n    # database.data_source flag that allows finer control over database \n    # settings as well as allowing use of AWS IAM credentials. For most users, \n    # database.data_source is a simpler configuration method.\n    advanced_data_source:\n        driver: "" # The driver to use: one of sqlite3, mysql, or postgresql. (type: string)\n        endpoint: "" # Typically the host:port combination of the database server. (type: string)\n        username: "" # Username to use when connecting. (type: string)\n        password: "" # Password to use when connecting. Not used if AWS IAM is enabled. (type: string)\n        db_name: "" # The name of the database to use for BuildBuddy data. (type: string)\n        region: "" # Region of the database instance. Required if AWS IAM is enabled. (type: string)\n        use_aws_iam: false # If enabled, AWS IAM authentication is used instead of fixed credentials. Make sure the endpoint includes the port, otherwise IAM-based auth will fail. (type: bool)\n        params: "" # Optional parameters to pass to the database driver (in format key1=val1&key2=val2) (type: string)\n    # database.advanced_read_replica (db.AdvancedConfig): Advanced alternative \n    # to database.read_replica. Refer to database.advanced for more \n    # information.\n    advanced_read_replica:\n        driver: "" # The driver to use: one of sqlite3, mysql, or postgresql. (type: string)\n        endpoint: "" # Typically the host:port combination of the database server. (type: string)\n        username: "" # Username to use when connecting. (type: string)\n        password: "" # Password to use when connecting. Not used if AWS IAM is enabled. (type: string)\n        db_name: "" # The name of the database to use for BuildBuddy data. (type: string)\n        region: "" # Region of the database instance. Required if AWS IAM is enabled. (type: string)\n        use_aws_iam: false # If enabled, AWS IAM authentication is used instead of fixed credentials. Make sure the endpoint includes the port, otherwise IAM-based auth will fail. (type: bool)\n        params: "" # Optional parameters to pass to the database driver (in format key1=val1&key2=val2) (type: string)\n    # database.conn_max_lifetime_seconds (int): The maximum lifetime of a \n    # connection to the db\n    conn_max_lifetime_seconds: 0\n    # database.data_source (string): The SQL database to connect to, specified \n    # as a connection string.\n    data_source: sqlite3:///tmp/buildbuddy.db\n    # database.log_queries (bool): If true, log all queries\n    log_queries: false\n    # database.max_idle_conns (int): The maximum number of idle connections to \n    # maintain to the db\n    max_idle_conns: 0\n    # database.max_open_conns (int): The maximum number of open connections to \n    # maintain to the db\n    max_open_conns: 0\n    # database.print_schema_changes_and_exit (bool): If set, print schema \n    # changes from auto-migration, then exit the program.\n    print_schema_changes_and_exit: false\n    # database.read_replica (string): A secondary, read-only SQL database to \n    # connect to, specified as a connection string.\n    read_replica: ""\n    # database.slow_query_threshold (time.Duration): Queries longer than this \n    # duration will be logged with a \'Slow SQL\' warning.\n    slow_query_threshold: 500ms\n    # database.stats_poll_interval (time.Duration): How often to poll the DB \n    # client for connection stats (default: \'5s\').\n    stats_poll_interval: 5s\nexecutor:\n    # executor.affinity_routing_enabled (bool): Enables affinity routing, \n    # which attempts to route actions to the executor that most recently ran \n    # that action.\n    affinity_routing_enabled: true\n    # executor.container_registries ([]oci.Registry)\n    container_registries: []\n    # For example:\n    # - hostnames: [] # (type: []string)\n    #   username: "" # (type: string)\n    #   password: "" # (type: string)\n\n    # executor.container_registry_mirrors ([]oci.MirrorConfig)\n    container_registry_mirrors: []\n    # For example:\n    # - original_url: "" # (type: string)\n    #   mirror_url: "" # (type: string)\n\n    # executor.container_registry_region (string): All occurrences of \n    # \'{{region}}\' in container image names will be replaced with this string, \n    # if specified.\n    container_registry_region: ""\n    # executor.custom_resources ([]resources.CustomResource): Optional \n    # allocatable custom resources. This works similarly to bazel\'s \n    # local_extra_resources flag. Request these resources in exec_properties \n    # using the \'resources:<name>\': \'<value>\' syntax.\n    custom_resources: []\n    # For example:\n    # - name: "" # (type: string)\n    #   value: 0 # (type: float64)\n\n    # executor.default_image (string): The default docker image to use to warm \n    # up executors or if no platform property is set. Ex: \n    # gcr.io/flame-public/executor-docker-default:enterprise-v1.5.4\n    default_image: gcr.io/flame-public/executor-docker-default:enterprise-v1.6.0\n    # executor.default_isolation_type (string): The default workload isolation \n    # type when no type is specified in an action. If not set, we use the \n    # first of the following that is set: docker, podman, firecracker, or none \n    # (bare).\n    default_isolation_type: ""\n    # executor.default_xcode_version (string): Sets the default Xcode version \n    # number to use if an action doesn\'t specify one. If not set, \n    # /Applications/Xcode.app/ is used.\n    default_xcode_version: ""\n    # executor.docker_socket (string): If set, run execution commands in \n    # docker using the provided socket.\n    docker_socket: ""\n    # executor.enable_bare_runner (bool): Enables running execution commands \n    # directly on the host without isolation.\n    enable_bare_runner: false\n    # executor.enable_firecracker (bool): Enables running execution commands \n    # inside of firecracker VMs\n    enable_firecracker: false\n    # executor.enable_local_snapshot_sharing (bool): Enables local snapshot \n    # sharing for firecracker VMs. Also requires that \n    # executor.firecracker_enable_nbd is true.\n    enable_local_snapshot_sharing: false\n    # executor.enable_oci (bool): Enables running execution commands using an \n    # OCI runtime directly.\n    enable_oci: false\n    # executor.enable_podman (bool): Enables running execution commands inside \n    # podman containers.\n    enable_podman: false\n    # executor.enable_remote_snapshot_sharing (bool): Enables remote snapshot \n    # sharing for firecracker VMs. Also requires that \n    # executor.firecracker_enable_nbd and executor.firecracker_enable_uffd are \n    # true.\n    enable_remote_snapshot_sharing: false\n    # executor.enable_sandbox (bool): Enables running execution commands \n    # inside of sandbox-exec.\n    enable_sandbox: false\n    # executor.enable_vfs (bool): Whether FUSE based filesystem is enabled.\n    enable_vfs: false\n    # executor.extra_env_vars ([]string): Additional environment variables to \n    # pass to remotely executed actions. i.e. MY_ENV_VAR=foo\n    extra_env_vars: []\n    # executor.forced_network_isolation_type (string): If set, run all \n    # commands that require networking with this isolation\n    forced_network_isolation_type: ""\n    # executor.host_id (string): Optional: Allows for manual specification of \n    # an executor\'s host id. If not set, a random UUID will be used.\n    host_id: ""\n    # executor.image_pull_timeout (time.Duration): How long to wait for the \n    # container image to be pulled before returning an Unavailable (retryable) \n    # error for an action execution attempt. Applies to all isolation types \n    # (docker, firecracker, etc.)\n    image_pull_timeout: 5m0s\n    # executor.memory_bytes (int64): Optional maximum memory to allocate to \n    # execution tasks (approximate). Cannot set both this option and the \n    # SYS_MEMORY_BYTES env var.\n    memory_bytes: 0\n    # executor.millicpu (int64): Optional maximum CPU milliseconds to allocate \n    # to execution tasks (approximate). Cannot set both this option and the \n    # SYS_CPU env var.\n    millicpu: 0\n    # executor.mmap_memory_bytes (int64): Maximum memory to be allocated \n    # towards mmapped files for Firecracker copy-on-write functionality. This \n    # is subtraced from the configured memory_bytes. Has no effect if \n    # firecracker is disabled or snapshot sharing is disabled.\n    mmap_memory_bytes: 10000000000\n    # executor.record_usage_timelines (bool): Capture resource usage \n    # timeseries data in UsageStats for each task.\n    record_usage_timelines: false\n    # executor.remote_snapshot_readonly (bool): Disables remote snapshot \n    # writes.\n    remote_snapshot_readonly: false\n    # executor.snaploader_eager_fetch_concurrency (int): Max number of \n    # goroutines allowed to run concurrently when eagerly fetching chunks.\n    snaploader_eager_fetch_concurrency: 32\n    # executor.snaploader_max_eager_fetches_per_sec (int): Max number of \n    # chunks snaploader can eagerly fetch in the background per second.\n    snaploader_max_eager_fetches_per_sec: 1000\n    # executor.verbose_snapshot_logs (bool): Enables extra-verbose snapshot \n    # logs (even at debug log level)\n    verbose_snapshot_logs: false\ngcp:\n    # gcp.client_id (string): The client id to use for GCP linking.\n    client_id: ""\n    # gcp.client_secret (string): The client secret to use for GCP linking.\n    client_secret: ""\ngithub:\n    # github.access_token (string): The GitHub access token used to post \n    # GitHub commit statuses. ** Enterprise only **\n    access_token: ""\n    app:\n        actions:\n            # github.app.actions.runner_enabled (bool): Whether to enable the \n            # buildbuddy-hosted runner for GitHub actions.\n            runner_enabled: false\n            # github.app.actions.runner_label (string): Label to apply to the \n            # actions runner. This is what \'runs-on\' needs to be set to in \n            # GitHub workflow YAML in order to run on this BuildBuddy \n            # instance.\n            runner_label: buildbuddy\n            # github.app.actions.runner_pool_name (string): Executor pool name \n            # to use for GitHub actions runner.\n            runner_pool_name: ""\n        # github.app.client_id (string): GitHub app OAuth client ID.\n        client_id: ""\n        # github.app.client_secret (string): GitHub app OAuth client secret.\n        client_secret: ""\n        # github.app.enabled (bool): Whether to enable the BuildBuddy GitHub \n        # app server.\n        enabled: false\n        # github.app.id (string): GitHub app ID.\n        id: ""\n        # github.app.private_key (string): GitHub app private key.\n        private_key: ""\n        # github.app.public_link (string): GitHub app installation URL.\n        public_link: ""\n        # github.app.review_mutates_enabled (bool): Perform mutations of PRs \n        # via the GitHub API.\n        review_mutates_enabled: false\n        # github.app.webhook_secret (string): GitHub app webhook secret used \n        # to verify that webhook payload contents were sent by GitHub.\n        webhook_secret: ""\n    # github.client_id (string): The client ID of your GitHub Oauth App. ** \n    # Enterprise only **\n    client_id: ""\n    # github.client_secret (string): The client secret of your GitHub Oauth \n    # App. ** Enterprise only **\n    client_secret: ""\n    # github.enterprise_host (string): The Github enterprise hostname to use \n    # if using GitHub enterprise server, not including https:// and no \n    # trailing slash.\n    enterprise_host: ""\n    # github.jwt_key (string): The key to use when signing JWT tokens for \n    # github auth.\n    jwt_key: ""\n    # github.status_name_suffix (string): Suffix to be appended to all \n    # reported GitHub status names. Useful for differentiating BuildBuddy \n    # deployments. For example: \'(dev)\' ** Enterprise only **\n    status_name_suffix: ""\n    # github.status_per_test_target (bool): If true, report status per test \n    # target. ** Enterprise only **\n    status_per_test_target: false\ngossip:\n    # gossip.join ([]string): The nodes to join/gossip with. Ex. \n    # \'1.2.3.4:1991,2.3.4.5:1991...\'\n    join: []\n    # gossip.listen_addr (string): The address to listen for gossip traffic \n    # on. Ex. \'localhost:1991\'\n    listen_addr: ""\n    # gossip.node_name (string): The gossip node\'s name. If empty will default \n    # to host_id.\'\n    node_name: ""\n    # gossip.secret_key (string): The value should be either 16, 24, or 32 \n    # bytes.\n    secret_key: ""\ngrpc_client:\n    # grpc_client.enable_pool_cache (bool): Whether or not to enable the \n    # connection pool cache.\n    enable_pool_cache: false\n    # grpc_client.pool_size (int): Number of connections to create to each \n    # target.\n    pool_size: 15\nintegrations:\n    invocation_upload:\n        # integrations.invocation_upload.aws_credentials (string): Credentials \n        # CSV file for Amazon s3 invocation upload webhook. ** Enterprise only \n        # **\n        aws_credentials: ""\n        # integrations.invocation_upload.enabled (bool): Whether to upload \n        # webhook data to the webhook URL configured per-Group. ** Enterprise \n        # only **\n        enabled: false\n        # integrations.invocation_upload.gcs_credentials (string): Credentials \n        # JSON for the Google service account used to authenticate when GCS is \n        # used as the invocation upload target. ** Enterprise only **\n        gcs_credentials: ""\n    slack:\n        # integrations.slack.webhook_url (string): A Slack webhook url to post \n        # build update messages to.\n        webhook_url: ""\nkeystore:\n    aws:\n        # keystore.aws.credentials (string): AWS CSV credentials that will be \n        # used to authenticate. If not specified, credentials will be \n        # retrieved as described by \n        # https://docs.aws.amazon.com/sdkref/latest/guide/standardized-credentials.html\n        credentials: ""\n        # keystore.aws.credentials_file (string): A path to a AWS CSV \n        # credentials file that will be used to authenticate. If not \n        # specified, credentials will be retrieved as described by \n        # https://docs.aws.amazon.com/sdkref/latest/guide/standardized-credentials.html\n        credentials_file: ""\n        # keystore.aws.enabled (bool): Whether AWS KMS support should be \n        # enabled. Implicitly enabled if the master key URI references an AWS \n        # KMS URI.\n        enabled: false\n    gcp:\n        # keystore.gcp.credentials (string): GCP JSON credentials that will be \n        # used to authenticate.\n        credentials: ""\n        # keystore.gcp.credentials_file (string): A path to a gcp JSON \n        # credentials file that will be used to authenticate.\n        credentials_file: ""\n        # keystore.gcp.enabled (bool): Whether GCP KMS support should be \n        # enabled. Implicitly enabled if the master key URI references a GCP \n        # KMS URI.\n        enabled: false\n    # keystore.local_insecure_kms_directory (string): For development only. If \n    # set, keys in format local-insecure-kms://[id] are read from this \n    # directory.\n    local_insecure_kms_directory: ""\n    # keystore.master_key_uri (string): The master key URI (see tink docs for \n    # example)\n    master_key_uri: ""\nmonitoring:\n    basic_auth:\n        # monitoring.basic_auth.password (string): Optional password for basic \n        # auth on the monitoring port.\n        password: ""\n        # monitoring.basic_auth.username (string): Optional username for basic \n        # auth on the monitoring port.\n        username: ""\nolap_database:\n    # olap_database.auto_migrate_db (bool): If true, attempt to automigrate \n    # the db when connecting\n    auto_migrate_db: true\n    # olap_database.cluster_name (string): The cluster name of the database\n    cluster_name: \'{cluster}\'\n    # olap_database.conn_max_lifetime (time.Duration): The maximum lifetime of \n    # a connection to clickhouse\n    conn_max_lifetime: 0s\n    # olap_database.data_source (string): The clickhouse database to connect \n    # to, specified a a connection string\n    data_source: ""\n    # olap_database.enable_data_replication (bool): If true, data replication \n    # is enabled.\n    enable_data_replication: false\n    # olap_database.max_idle_conns (int): The maximum number of idle \n    # connections to maintain to the db\n    max_idle_conns: 0\n    # olap_database.max_open_conns (int): The maximum number of open \n    # connections to maintain to the db\n    max_open_conns: 0\n    # olap_database.print_schema_changes_and_exit (bool): If set, print schema \n    # changes from auto-migration, then exit the program.\n    print_schema_changes_and_exit: false\n    # olap_database.replica_name (string): The replica name of the table in \n    # zookeeper\n    replica_name: \'{replica}\'\n    # olap_database.zoo_path (string): The path to the table name in \n    # zookeeper, used to set up data replication\n    zoo_path: /clickhouse/{installation}/{cluster}/tables/{shard}/{database}/{table}\nopenai:\n    # openai.api_key (string): OpenAI API key\n    api_key: ""\n    # openai.model (string): OpenAI model name to use. Find them here: \n    # https://platform.openai.com/docs/models\n    model: gpt-3.5-turbo\norg:\n    # org.domain (string): Your organization\'s email domain. If this is set, \n    # only users with email addresses in this domain will be able to register \n    # for a BuildBuddy account.\n    domain: ""\n    # org.name (string): The name of your organization, which is displayed on \n    # your organization\'s build history.\n    name: Organization\nprometheus:\n    # prometheus.address (string): the address of the promethus HTTP API\n    address: ""\nregistry:\n    # registry.backend (string): The registry backend to forward requests to\n    backend: https://bcr.bazel.build/\n    # registry.enabled (bool): Whether the registry service should be enabled.\n    enabled: false\nremote_execution:\n    # remote_execution.action_merging_hedge_count (int): When action merging \n    # is enabled, this flag controls how many additional, \'hedged\' attempts an \n    # action is run in the background. Note that even hedged actions are run \n    # at most once per execution request.\n    action_merging_hedge_count: 0\n    # remote_execution.action_merging_hedge_delay (time.Duration): When action \n    # merging hedging is enabled, up to \n    # --remote_execution.action_merging_hedge_count hedged actions are run \n    # with this delay of linear backoff.\n    action_merging_hedge_delay: 0s\n    # remote_execution.cgroup_settings_enabled (bool): Apply cgroup2 settings \n    # to Linux executions.\n    cgroup_settings_enabled: true\n    # remote_execution.ci_runner_default_timeout (time.Duration): Default \n    # timeout applied to all ci runners.\n    ci_runner_default_timeout: 8h0m0s\n    # remote_execution.ci_runner_recycling_max_wait (time.Duration): Max \n    # duration that a ci_runner task should wait for a warm runner before \n    # running on a potentially cold runner.\n    ci_runner_recycling_max_wait: 3s\n    # remote_execution.cpu_quota_limit (time.Duration): Maximum CPU time \n    # allowed for each quota period.\n    cpu_quota_limit: 3s\n    # remote_execution.cpu_quota_period (time.Duration): How often the CPU \n    # quota is refreshed.\n    cpu_quota_period: 100ms\n    # remote_execution.default_pool_name (string): The default executor pool \n    # to use if one is not specified.\n    default_pool_name: ""\n    # remote_execution.enable_action_merging (bool): If enabled, identical \n    # actions being executed concurrently are merged into a single execution.\n    enable_action_merging: true\n    # remote_execution.enable_executor_key_creation (bool): If enabled, UI \n    # will allow executor keys to be created.\n    enable_executor_key_creation: false\n    # remote_execution.enable_kythe_indexing (bool): If set, and codesearch is \n    # enabled, automatically run a kythe indexing action.\n    enable_kythe_indexing: false\n    # remote_execution.enable_redis_availability_monitoring (bool): If \n    # enabled, the execution server will detect if Redis has lost state and \n    # will ask Bazel to retry executions.\n    enable_redis_availability_monitoring: false\n    # remote_execution.enable_remote_exec (bool): If true, enable remote-exec. \n    # ** Enterprise only **\n    enable_remote_exec: true\n    # remote_execution.enable_user_owned_executors (bool): If enabled, users \n    # can register their own executors with the scheduler.\n    enable_user_owned_executors: false\n    # remote_execution.enable_workflows (bool): Whether to enable BuildBuddy \n    # workflows.\n    enable_workflows: false\n    # remote_execution.force_user_owned_darwin_executors (bool): If enabled, \n    # darwin actions will always run on user-owned executors.\n    force_user_owned_darwin_executors: false\n    # remote_execution.lease_duration (time.Duration): How long before a task \n    # lease must be renewed by the executor client.\n    lease_duration: 10s\n    # remote_execution.lease_grace_period (time.Duration): How long to wait \n    # for the executor to renew the lease after the TTL duration has elapsed.\n    lease_grace_period: 10s\n    # remote_execution.lease_reconnect_grace_period (time.Duration): How long \n    # to delay re-enqueued tasks in order to allow the previous lease holder \n    # to renew its lease (following a server shutdown).\n    lease_reconnect_grace_period: 1s\n    # remote_execution.max_scheduling_delay (time.Duration): Max duration that \n    # actions can sit in a non-preferred executor\'s queue before they are \n    # executed.\n    max_scheduling_delay: 5s\n    # remote_execution.pids_limit (int64): Maximum number of processes allowed \n    # per task at any time.\n    pids_limit: 2048\n    # remote_execution.redis_pubsub_pool_size (int): Maximum number of \n    # connections used for waiting for execution updates.\n    redis_pubsub_pool_size: 10000\n    # remote_execution.redis_target (string): A Redis target for storing \n    # remote execution state. Falls back to app.default_redis_target if \n    # unspecified. Required for remote execution. To ease migration, the redis \n    # target from the cache config will be used if neither this value nor \n    # app.default_redis_target are specified.\n    redis_target: ""\n    # remote_execution.require_executor_authorization (bool): If true, \n    # executors connecting to this server must provide a valid executor API \n    # key.\n    require_executor_authorization: false\n    sharded_redis:\n        # remote_execution.sharded_redis.password (string): Redis password\n        password: ""\n        # remote_execution.sharded_redis.shards ([]string): Ordered list of \n        # Redis shard addresses.\n        shards: []\n        # remote_execution.sharded_redis.username (string): Redis username\n        username: ""\n    # remote_execution.shared_executor_pool_group_id (string): Group ID that \n    # owns the shared executor pool.\n    shared_executor_pool_group_id: ""\n    # remote_execution.stored_task_size_millicpu_limit (int64): Limit placed \n    # on milliCPU calculated from task execution statistics.\n    stored_task_size_millicpu_limit: 7500\n    task_size_model:\n        # remote_execution.task_size_model.enabled (bool): Whether to enable \n        # model-based task size prediction.\n        enabled: false\n        # remote_execution.task_size_model.features_config_path (string): Path \n        # pointing to features.json config file.\n        features_config_path: ""\n        # remote_execution.task_size_model.serving_address (string): gRPC \n        # address pointing to TensorFlow Serving prediction service with task \n        # size models (cpu, mem).\n        serving_address: ""\n    # remote_execution.task_size_psi_correction (float64): What percentage of \n    # full-stall time should be subtracted from the execution duration.\n    task_size_psi_correction: 1\n    # remote_execution.use_measured_task_sizes (bool): Whether to use measured \n    # usage stats to determine task sizes.\n    use_measured_task_sizes: false\n    # remote_execution.workflow_default_branch_routing_enabled (bool): Enables \n    # default branch routing for workflows. When routing a workflow action, if \n    # there are no executors that ran that action for the same git branch, try \n    # to route it to an executor that ran the action for the same default \n    # branch.\n    workflow_default_branch_routing_enabled: false\n    # remote_execution.workflows_ci_runner_bazel_command (string): Bazel \n    # command to be used by the CI runner.\n    workflows_ci_runner_bazel_command: ""\n    # remote_execution.workflows_ci_runner_debug (bool): Whether to run the CI \n    # runner in debug mode.\n    workflows_ci_runner_debug: false\n    # remote_execution.workflows_default_image (string): The default \n    # container-image property to use for workflows. Must include docker:// \n    # prefix if applicable.\n    workflows_default_image: docker://gcr.io/flame-public/buildbuddy-ci-runner@sha256:8cf614fc4695789bea8321446402e7d6f84f6be09b8d39ec93caa508fa3e3cfc\n    # remote_execution.workflows_enable_firecracker (bool): Whether to enable \n    # firecracker for Linux workflow actions.\n    workflows_enable_firecracker: false\n    # remote_execution.workflows_linux_compute_units (int): Number of \n    # BuildBuddy compute units (BCU) to reserve for Linux workflow actions.\n    workflows_linux_compute_units: 3\n    # remote_execution.workflows_mac_compute_units (int): Number of BuildBuddy \n    # compute units (BCU) to reserve for Mac workflow actions.\n    workflows_mac_compute_units: 3\n    # remote_execution.workflows_pool_name (string): The executor pool to use \n    # for workflow actions. Defaults to the default executor pool if not \n    # specified.\n    workflows_pool_name: ""\nsoci_artifact_store:\n    # soci_artifact_store.cache_seed (string): If set, this seed is hashed \n    # with container image IDs to generate cache keys storing soci indexes.\n    cache_seed: socicache-092872023\n    # soci_artifact_store.layer_storage (string): Directory in which to store \n    # pulled container image layers for indexing by soci artifact store.\n    layer_storage: /tmp/\nssl:\n    # ssl.cert_file (string): Path to a PEM encoded certificate file to use \n    # for TLS if not using ACME.\n    cert_file: ""\n    # ssl.client_ca_cert (string): PEM encoded certificate authority used to \n    # issue client certificates for mTLS auth.\n    client_ca_cert: ""\n    # ssl.client_ca_cert_file (string): Path to a PEM encoded certificate \n    # authority file used to issue client certificates for mTLS auth.\n    client_ca_cert_file: ""\n    # ssl.client_ca_key (string): PEM encoded certificate authority key used \n    # to issue client certificates for mTLS auth.\n    client_ca_key: ""\n    # ssl.client_ca_key_file (string): Path to a PEM encoded certificate \n    # authority key file used to issue client certificates for mTLS auth.\n    client_ca_key_file: ""\n    # ssl.client_cert_lifespan (time.Duration): The duration client \n    # certificates are valid for. Ex: \'730h\' for one month. If not set, \n    # defaults to 100 years.\n    client_cert_lifespan: 876000h0m0s\n    # ssl.default_host (string): Host name to use for ACME generated cert if \n    # TLS request does not contain SNI.\n    default_host: ""\n    # ssl.enable_ssl (bool): Whether or not to enable SSL/TLS on gRPC \n    # connections (gRPCS).\n    enable_ssl: false\n    # ssl.host_whitelist ([]string): Cloud-Only\n    host_whitelist: []\n    # ssl.key_file (string): Path to a PEM encoded key file to use for TLS if \n    # not using ACME.\n    key_file: ""\n    # ssl.self_signed (bool): If true, a self-signed cert will be generated \n    # for TLS termination.\n    self_signed: false\n    # ssl.upgrade_insecure (bool): True if http requests should be redirected \n    # to https. Assumes http traffic is served on port 80 and https traffic is \n    # served on port 443 (typically via an ingress / load balancer).\n    upgrade_insecure: false\n    # ssl.use_acme (bool): Whether or not to automatically configure SSL certs \n    # using ACME. If ACME is enabled, cert_file and key_file should not be \n    # set.\n    use_acme: false\nstorage:\n    aws_s3:\n        # storage.aws_s3.bucket (string): The AWS S3 bucket to store files in.\n        bucket: ""\n        # storage.aws_s3.credentials_profile (string): A custom credentials \n        # profile to use.\n        credentials_profile: ""\n        # storage.aws_s3.disable_ssl (bool): Disables the use of SSL, useful \n        # for configuring the use of MinIO. **DEPRECATED** Specify a non-HTTPS \n        # endpoint instead.\n        disable_ssl: false\n        # storage.aws_s3.endpoint (string): The AWS endpoint to use, useful \n        # for configuring the use of MinIO.\n        endpoint: ""\n        # storage.aws_s3.region (string): The AWS region.\n        region: ""\n        # storage.aws_s3.role_arn (string): The role ARN to use for web \n        # identity auth.\n        role_arn: ""\n        # storage.aws_s3.role_session_name (string): The role session name to \n        # use for web identity auth.\n        role_session_name: ""\n        # storage.aws_s3.s3_force_path_style (bool): Force path style urls for \n        # objects, useful for configuring the use of MinIO.\n        s3_force_path_style: false\n        # storage.aws_s3.static_credentials_id (string): Static credentials ID \n        # to use, useful for configuring the use of MinIO.\n        static_credentials_id: ""\n        # storage.aws_s3.static_credentials_secret (string): Static \n        # credentials secret to use, useful for configuring the use of MinIO.\n        static_credentials_secret: ""\n        # storage.aws_s3.static_credentials_token (string): Static credentials \n        # token to use, useful for configuring the use of MinIO.\n        static_credentials_token: ""\n        # storage.aws_s3.web_identity_token_file (string): The file path to \n        # the web identity token file.\n        web_identity_token_file: ""\n    azure:\n        # storage.azure.account_key (string): The key for the Azure storage \n        # account\n        account_key: ""\n        # storage.azure.account_name (string): The name of the Azure storage \n        # account\n        account_name: ""\n        # storage.azure.container_name (string): The name of the Azure storage \n        # container\n        container_name: ""\n    # storage.chunk_file_size_bytes (int): How many bytes to buffer in memory \n    # before flushing a chunk of build protocol data to disk.\n    chunk_file_size_bytes: 3000000\n    # storage.cleanup_batch_size (int): How many invocations to delete in each \n    # janitor cleanup task\n    cleanup_batch_size: 10\n    # storage.disable_persist_cache_artifacts (bool): If disabled, buildbuddy \n    # will not persist cache artifacts in the blobstore. This may make older \n    # invocations not display properly.\n    disable_persist_cache_artifacts: false\n    disk:\n        # storage.disk.root_directory (string): The root directory to store \n        # all blobs in, if using disk based storage.\n        root_directory: /tmp/buildbuddy\n        # storage.disk.use_v2_layout (bool): If enabled, files will be stored \n        # using the v2 layout. See disk_cache.MigrateToV2Layout for a \n        # description.\n        use_v2_layout: false\n    # storage.enable_chunked_event_logs (bool): If true, Event logs will be \n    # stored separately from the invocation proto in chunks.\n    enable_chunked_event_logs: true\n    execution:\n        # storage.execution.cleanup_batch_size (int): How many invocations to \n        # delete in each janitor cleanup task\n        cleanup_batch_size: 200\n        # storage.execution.cleanup_interval (time.Duration): How often the \n        # janitor cleanup tasks will run\n        cleanup_interval: 5m0s\n        # storage.execution.cleanup_workers (int): How many cleanup tasks to \n        # run\n        cleanup_workers: 1\n        # storage.execution.ttl (time.Duration): The time, in seconds, to keep \n        # invocations before deletion. 0 disables invocation deletion.\n        ttl: 0s\n    gcs:\n        # storage.gcs.bucket (string): The name of the GCS bucket to store \n        # build artifact files in.\n        bucket: ""\n        # storage.gcs.credentials (string): Credentials in JSON format that \n        # will be used to authenticate to GCS.\n        credentials: ""\n        # storage.gcs.credentials_file (string): A path to a JSON credentials \n        # file that will be used to authenticate to GCS.\n        credentials_file: ""\n        # storage.gcs.project_id (string): The Google Cloud project ID of the \n        # project owning the above credentials and GCS bucket.\n        project_id: ""\n    # storage.path_prefix (string): The prefix directory to store all blobs in\n    path_prefix: ""\n    # storage.tempdir (string): Root directory for temporary files. Defaults \n    # to the OS-specific temp dir.\n    tempdir: /tmp\n    # storage.ttl_seconds (int): The time, in seconds, to keep invocations \n    # before deletion. 0 disables invocation deletion.\n    ttl_seconds: 0\nvertexai:\n    # vertexai.credentials (string): The GCP credentials to use\n    credentials: ""\n    # vertexai.model (string): The model ID to use\n    model: codechat-bison\n    # vertexai.project (string): The GCP project ID to use\n    project: flame-build\n    # vertexai.region (string): The GCP region to use\n    region: us-central1\nworkspace:\n    # workspace.enabled (bool): If true, enable workspaces.\n    enabled: false\n    # workspace.use_blobstore (bool): If true, use blobstore to store \n    # workspaces. Otherwise the cache will be used\n    use_blobstore: true\n'})})}function c(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(l,{...e})}):l(e)}function d(e){const n={code:"code",pre:"pre",...(0,i.R)(),...e.components};return(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:'# Unstructured settings\n\n# debug_disable_firecracker_workspace_sync (bool): Do not sync the action \n# workspace to the guest, instead using the existing workspace from the VM \n# snapshot.\ndebug_disable_firecracker_workspace_sync: false\n# debug_enable_anonymous_runner_recycling (bool): Whether to enable runner \n# recycling for unauthenticated requests. For debugging purposes only - do not \n# use in production.\ndebug_enable_anonymous_runner_recycling: false\n# debug_force_remote_snapshots (bool): When remote snapshotting is enabled, \n# force remote snapshotting even for tasks which otherwise wouldn\'t support \n# it.\ndebug_force_remote_snapshots: false\n# debug_stream_command_outputs (bool): If true, stream command outputs to the \n# terminal. Intended for debugging purposes only and should not be used in \n# production.\ndebug_stream_command_outputs: false\n# debug_use_local_images_only (bool): Do not pull OCI images and only used \n# locally cached images. This can be set to test local image builds during \n# development without needing to push to a container registry. Not intended \n# for production use.\ndebug_use_local_images_only: false\n# docker_cap_add (string): Sets --cap-add= on the docker command. Comma \n# separated.\ndocker_cap_add: ""\n# drop_invocation_pk_cols (bool): If true, attempt to drop invocation PK cols\ndrop_invocation_pk_cols: false\n# grpc_client_origin_header (string): Header value to set for \n# x-buildbuddy-origin.\ngrpc_client_origin_header: ""\n# grpc_max_recv_msg_size_bytes (int): Configures the max GRPC receive message \n# size [bytes]\ngrpc_max_recv_msg_size_bytes: 50000000\n# grpc_port (int): The port to listen for gRPC traffic on\ngrpc_port: 1985\n# grpcs_port (int): The port to listen for gRPCS traffic on\ngrpcs_port: 1986\n# internal_grpc_port (int): The port to listen for internal gRPC traffic on\ninternal_grpc_port: 1987\n# internal_grpcs_port (int): The port to listen for internal gRPCS traffic on\ninternal_grpcs_port: 1988\n# listen (string): The interface to listen on (default: 0.0.0.0)\nlisten: 0.0.0.0\n# log_goroutine_profile_on_shutdown (bool): Whether to log all goroutine stack \n# traces on shutdown.\nlog_goroutine_profile_on_shutdown: false\n# max_shutdown_duration (time.Duration): Time to wait for shutdown\nmax_shutdown_duration: 25s\n# monitoring_port (int): The port to listen for monitoring traffic on\nmonitoring_port: 9090\n# port (int): The port to listen for HTTP traffic on\nport: 8080\n# redis_command_buffer_flush_period (time.Duration): How long to wait between \n# flushing buffered redis commands. Setting this to 0 will disable buffering \n# at the cost of higher redis QPS.\nredis_command_buffer_flush_period: 250ms\n# regions ([]region.Region): A list of regions that executors might be \n# connected to.\nregions: []\n# For example:\n# - name: "" # The user-friendly name of this region. Ex: Europe (type: string)\n#   server: "" # The http endpoint for this server, with the protocol. Ex: https://app.europe.buildbuddy.io (type: string)\n#   subdomains: "" # The format for subdomain urls of with a single * wildcard. Ex: https://*.europe.buildbuddy.io (type: string)\n\n# report_not_ready (bool): If set to true, the app will always report as being \n# unready.\nreport_not_ready: false\n# server_type (string): The server type to match on health checks\nserver_type: prod-buildbuddy-executor\n# shutdown_lameduck_duration (time.Duration): If set, the server will be \n# marked unready but not run shutdown functions until this period passes.\nshutdown_lameduck_duration: 0s\n# zone_override (string): A value that will override the auto-detected zone. \n# Ignored if empty\nzone_override: ""\n\n# Structured settings\n\napp:\n    # app.admin_only_create_group (bool): If true, only admins of an existing \n    # group can create a new groups.\n    admin_only_create_group: false\n    # app.build_buddy_url (URL): The external URL where your BuildBuddy \n    # instance can be found.\n    build_buddy_url: http://localhost:8080\n    # app.cache_api_url (URL): Overrides the default remote cache protocol \n    # gRPC address shown by BuildBuddy on the configuration screen.\n    cache_api_url: ""\n    client_identity:\n        # app.client_identity.client (string): The client identifier to place \n        # in the identity header.\n        client: ""\n        # app.client_identity.key (string): The key used to sign and verify \n        # identity JWTs.\n        key: ""\n        # app.client_identity.origin (string): The origin identifier to place \n        # in the identity header.\n        origin: ""\n    # app.code_editor_enabled (bool): If set, code editor functionality will \n    # be enabled.\n    code_editor_enabled: false\n    # app.code_editor_v2_enabled (bool): If set, show v2 of code editor that \n    # stores state on server instead of local storage.\n    code_editor_v2_enabled: false\n    # app.default_redis_target (string): A Redis target for storing remote \n    # shared state. To ease migration, the redis target from the remote \n    # execution config will be used if this value is not specified.\n    default_redis_target: ""\n    default_sharded_redis:\n        # app.default_sharded_redis.password (string): Redis password\n        password: ""\n        # app.default_sharded_redis.shards ([]string): Ordered list of Redis \n        # shard addresses.\n        shards: []\n        # app.default_sharded_redis.username (string): Redis username\n        username: ""\n    # app.default_subdomains ([]string): List of subdomains that should not be \n    # handled as user-owned subdomains.\n    default_subdomains: []\n    # app.enable_canaries (bool): If true, enable slow function canaries\n    enable_canaries: true\n    # app.enable_grpc_metrics_by_group_id (bool): If enabled, grpc metrics by \n    # group ID will be recorded\n    enable_grpc_metrics_by_group_id: false\n    # app.enable_prometheus_histograms (bool): If true, collect prometheus \n    # histograms for all RPCs\n    enable_prometheus_histograms: true\n    # app.enable_structured_logging (bool): If true, log messages will be \n    # json-formatted.\n    enable_structured_logging: false\n    # app.enable_subdomain_matching (bool): If true, request subdomain will be \n    # taken into account when determining what request restrictions should be \n    # applied.\n    enable_subdomain_matching: false\n    # app.events_api_url (URL): Overrides the default build event protocol \n    # gRPC address shown by BuildBuddy on the configuration screen.\n    events_api_url: ""\n    # app.grpc_max_recv_msg_size_bytes (int): DEPRECATED: use \n    # --grpc_max_recv_msg_size_bytes instead\n    grpc_max_recv_msg_size_bytes: 50000000\n    # app.grpc_over_http_port_enabled (bool): Enables grpc traffic to be \n    # served over the http port.\n    grpc_over_http_port_enabled: true\n    # app.ignore_forced_tracing_header (bool): If set, we will not honor the \n    # forced tracing header.\n    ignore_forced_tracing_header: false\n    # app.log_enable_gcp_logging_format (bool): If true, the output structured \n    # logs will be compatible with format expected by GCP Logging.\n    log_enable_gcp_logging_format: false\n    # app.log_enable_grpc_request (bool): If true, log grpc request when log \n    # level is default\n    log_enable_grpc_request: true\n    # app.log_error_stack_traces (bool): If true, stack traces will be printed \n    # for errors that have them.\n    log_error_stack_traces: false\n    # app.log_gcp_log_id (string): The log ID to log to in GCP (if any).\n    log_gcp_log_id: ""\n    # app.log_gcp_project_id (string): The project ID to log to in GCP (if \n    # any).\n    log_gcp_project_id: ""\n    # app.log_include_short_file_name (bool): If true, log messages will \n    # include shortened originating file name.\n    log_include_short_file_name: false\n    # app.log_level (string): The desired log level. Logs with a level >= this \n    # level will be emitted. One of {\'fatal\', \'error\', \'warn\', \'info\', \n    # \'debug\'}\n    log_level: info\n    # app.proxy_targets ([]grpc_forward.proxyPair)\n    proxy_targets: []\n    # For example:\n    # - prefix: "" # The gRPC method prefix to match. (type: string)\n    #   target: "" # The gRPC target to forward requests to. (type: string)\n\n    # app.strict_csp_enabled (bool): If set, set a strict CSP header. \n    # Violations are logged at warning level.\n    strict_csp_enabled: false\n    # app.trace_fraction (float64): Fraction of requests to sample for \n    # tracing.\n    trace_fraction: 0\n    # app.trace_fraction_overrides ([]string): Tracing fraction override based \n    # on name in format name=fraction.\n    trace_fraction_overrides: []\n    # app.trace_jaeger_collector (string): Address of the Jager collector \n    # endpoint where traces will be sent.\n    trace_jaeger_collector: ""\n    # app.trace_project_id (string): Optional GCP project ID to export traces \n    # to. If not specified, determined from default credentials or metadata \n    # server if running on GCP.\n    trace_project_id: ""\n    # app.trace_service_name (string): Name of the service to associate with \n    # traces.\n    trace_service_name: ""\nauth:\n    # auth.admin_group_id (string): ID of a group whose members can perform \n    # actions only accessible to server admins.\n    admin_group_id: ""\n    # auth.disable_refresh_token (bool): If true, the offline_access scope \n    # which requests refresh tokens will not be requested.\n    disable_refresh_token: false\n    # auth.domain_wide_cookies (bool): If true, cookies will have domain set \n    # so that they are accessible on domain and all subdomains.\n    domain_wide_cookies: false\n    # auth.enable_anonymous_usage (bool): If true, unauthenticated build \n    # uploads will still be allowed but won\'t be associated with your \n    # organization.\n    enable_anonymous_usage: false\n    # auth.enable_self_auth (bool): If true, enables a single user login via \n    # an oauth provider on the buildbuddy server. Recommend use only when \n    # server is behind a firewall; this option may allow anyone with access to \n    # the webpage admin rights to your buildbuddy installation. ** Enterprise \n    # only **\n    enable_self_auth: false\n    # auth.force_approval (bool): If true, when a user doesn\'t have a session \n    # (first time logging in, or manually logged out) force the auth provider \n    # to show the consent screen allowing the user to select an account if \n    # they have multiple. This isn\'t supported by all auth providers.\n    force_approval: false\n    # auth.https_only_cookies (bool): If true, cookies will only be set over \n    # https connections.\n    https_only_cookies: false\n    # auth.jwt_claims_cache_ttl (time.Duration): TTL for JWT string to parsed \n    # claims caching. Set to \'0\' to disable cache.\n    jwt_claims_cache_ttl: 15s\n    # auth.jwt_duration (time.Duration): Maximum lifetime of the generated \n    # JWT.\n    jwt_duration: 6h0m0s\n    # auth.jwt_key (string): The key to use when signing JWT tokens.\n    jwt_key: set_the_jwt_in_config\n    # auth.new_jwt_key (string): If set, JWT verifications will try both this \n    # and the old JWT key.\n    new_jwt_key: ""\n    # auth.oauth_providers ([]oidc.OauthProvider): The list of oauth providers \n    # to use to authenticate.\n    oauth_providers: []\n    # For example:\n    # - issuer_url: "" # The issuer URL of this OIDC Provider. (type: string)\n    #   client_id: "" # The oauth client ID. (type: string)\n    #   client_secret: "" # The oauth client secret. (type: string)\n    #   slug: "" # The slug of this OIDC Provider. (type: string)\n\n    saml:\n        # auth.saml.cert (string): PEM encoded certificate used for SAML auth.\n        cert: ""\n        # auth.saml.cert_file (string): Path to a PEM encoded certificate file \n        # used for SAML auth.\n        cert_file: ""\n        # auth.saml.key (string): PEM encoded certificate key used for SAML \n        # auth.\n        key: ""\n        # auth.saml.key_file (string): Path to a PEM encoded certificate key \n        # file used for SAML auth.\n        key_file: ""\n        # auth.saml.trusted_idp_cert_files ([]string): List of PEM-encoded \n        # trusted IDP certificates. Intended for testing and development only.\n        trusted_idp_cert_files: []\n    # auth.sign_using_new_jwt_key (bool): If true, new JWTs will be signed \n    # using the new JWT key.\n    sign_using_new_jwt_key: false\n    # auth.trust_xforwardedfor_header (bool): If true, client IP information \n    # will be retrieved from the X-Forwarded-For header. Should only be \n    # enabled if the BuildBuddy server is only accessible behind a trusted \n    # proxy.\n    trust_xforwardedfor_header: false\ncache:\n    client:\n        # cache.client.ac_rpc_timeout (time.Duration): Maximum time a single \n        # Action Cache RPC can take.\n        ac_rpc_timeout: 15s\n        # cache.client.cas_rpc_timeout (time.Duration): Maximum time a single \n        # batch RPC or a single ByteStream chunk read can take.\n        cas_rpc_timeout: 1m0s\n        # cache.client.enable_download_compression (bool): If true, enable \n        # compression of downloads from remote caches\n        enable_download_compression: true\n        # cache.client.enable_upload_compression (bool): If true, enable \n        # compression of uploads to remote caches\n        enable_upload_compression: true\n        # cache.client.filecache_link_parallelism (int): Number of goroutines \n        # to use when linking inputs from filecache. If 0 uses the value of \n        # GOMAXPROCS.\n        filecache_link_parallelism: 0\n        # cache.client.input_tree_setup_parallelism (int): Number of \n        # goroutines to use across all tasks when setting up the input tree \n        # structure. -1 means no queueing. 0 means GOMAXPROCS.\n        input_tree_setup_parallelism: -1\n    gcs:\n        # cache.gcs.bucket (string): The name of the GCS bucket to store cache \n        # files in.\n        bucket: ""\n        # cache.gcs.credentials_file (string): A path to a JSON credentials \n        # file that will be used to authenticate to GCS.\n        credentials_file: ""\n        # cache.gcs.project_id (string): The Google Cloud project ID of the \n        # project owning the above credentials and GCS bucket.\n        project_id: ""\n        # cache.gcs.ttl_days (int64): The period after which cache files \n        # should be TTLd. Disabled if 0.\n        ttl_days: 0\n    # cache.memcache_targets ([]string): Deprecated. Use Redis Target instead.\n    memcache_targets: []\n    redis:\n        # cache.redis.max_value_size_bytes (int64): The maximum value size to \n        # cache in redis (in bytes).\n        max_value_size_bytes: 10000000\n        # cache.redis.redis_target (string): A redis target for improved \n        # Caching/RBE performance. Target can be provided as either a redis \n        # connection URI or a host:port pair. URI schemas supported: \n        # redis[s]://[[USER][:PASSWORD]@][HOST][:PORT][/DATABASE] or \n        # unix://[[USER][:PASSWORD]@]SOCKET_PATH[?db=DATABASE] ** Enterprise \n        # only **\n        redis_target: ""\n        sharded:\n            # cache.redis.sharded.password (string): Redis password\n            password: ""\n            # cache.redis.sharded.shards ([]string): Ordered list of Redis \n            # shard addresses.\n            shards: []\n            # cache.redis.sharded.username (string): Redis username\n            username: ""\n    # cache.redis_target (string): A redis target for improved Caching/RBE \n    # performance. Target can be provided as either a redis connection URI or \n    # a host:port pair. URI schemas supported: \n    # redis[s]://[[USER][:PASSWORD]@][HOST][:PORT][/DATABASE] or \n    # unix://[[USER][:PASSWORD]@]SOCKET_PATH[?db=DATABASE] ** Enterprise only \n    # **\n    redis_target: ""\n    s3:\n        # cache.s3.bucket (string): The AWS S3 bucket to store files in.\n        bucket: ""\n        # cache.s3.credentials_profile (string): A custom credentials profile \n        # to use.\n        credentials_profile: ""\n        # cache.s3.disable_ssl (bool): Disables the use of SSL, useful for \n        # configuring the use of MinIO. **DEPRECATED** Specify a non-HTTPS \n        # endpoint instead.\n        disable_ssl: false\n        # cache.s3.endpoint (string): The AWS endpoint to use, useful for \n        # configuring the use of MinIO.\n        endpoint: ""\n        # cache.s3.path_prefix (string): Prefix inside the AWS S3 bucket to \n        # store files\n        path_prefix: ""\n        # cache.s3.region (string): The AWS region.\n        region: ""\n        # cache.s3.role_arn (string): The role ARN to use for web identity \n        # auth.\n        role_arn: ""\n        # cache.s3.role_session_name (string): The role session name to use \n        # for web identity auth.\n        role_session_name: ""\n        # cache.s3.s3_force_path_style (bool): Force path style urls for \n        # objects, useful for configuring the use of MinIO.\n        s3_force_path_style: false\n        # cache.s3.static_credentials_id (string): Static credentials ID to \n        # use, useful for configuring the use of MinIO.\n        static_credentials_id: ""\n        # cache.s3.static_credentials_secret (string): Static credentials \n        # secret to use, useful for configuring the use of MinIO.\n        static_credentials_secret: ""\n        # cache.s3.static_credentials_token (string): Static credentials token \n        # to use, useful for configuring the use of MinIO.\n        static_credentials_token: ""\n        # cache.s3.ttl_days (int): The period after which cache files should \n        # be TTLd. Disabled if 0.\n        ttl_days: 0\n        # cache.s3.web_identity_token_file (string): The file path to the web \n        # identity token file.\n        web_identity_token_file: ""\nexecutor:\n    # executor.api_key (string): API Key used to authorize the executor with \n    # the BuildBuddy app server.\n    api_key: ""\n    # executor.app_target (string): The GRPC url of a buildbuddy app server.\n    app_target: grpcs://remote.buildbuddy.io\n    bare:\n        # executor.bare.enable_log_files (bool): Whether to send bare runner \n        # output to log files for debugging. These files are stored adjacent \n        # to the task directory and are deleted when the task is complete.\n        enable_log_files: false\n        # executor.bare.enable_stats (bool): Whether to enable stats for bare \n        # command execution.\n        enable_stats: false\n    # executor.cache_target (string): The GRPC url of the remote cache to use. \n    # If empty, the value from --executor.app_target is used.\n    cache_target: ""\n    # executor.child_cgroups_enabled (bool): On startup, sets up separate \n    # child cgroups for the executor process and any action processes that it \n    # starts. When using this flag, the executor\'s starting cgroup must not \n    # have any other processes besides the executor. In particular, the \n    # executor cannot be run under tini when using this flag.\n    child_cgroups_enabled: false\n    # executor.container_registries ([]oci.Registry)\n    container_registries: []\n    # For example:\n    # - hostnames: [] # (type: []string)\n    #   username: "" # (type: string)\n    #   password: "" # (type: string)\n\n    # executor.container_registry_mirrors ([]oci.MirrorConfig)\n    container_registry_mirrors: []\n    # For example:\n    # - original_url: "" # (type: string)\n    #   mirror_url: "" # (type: string)\n\n    # executor.container_registry_region (string): All occurrences of \n    # \'{{region}}\' in container image names will be replaced with this string, \n    # if specified.\n    container_registry_region: ""\n    cpu_leaser:\n        # executor.cpu_leaser.cpuset (string): Manual override for the set of \n        # CPUs that may be leased. Ignored if empty. Ex. \'0-1,3\'\n        cpuset: ""\n        # executor.cpu_leaser.enable (bool): Enable cpu leaser functionality\n        enable: false\n        # executor.cpu_leaser.min_overhead (int): Always ensure at least this \n        # many extra cpus are included in a lease\n        min_overhead: 2\n        # executor.cpu_leaser.overhead (float64): The amount of extra CPU \n        # *above the task size* to include in a lease\n        overhead: 0.2\n    # executor.custom_resources ([]resources.CustomResource): Optional \n    # allocatable custom resources. This works similarly to bazel\'s \n    # local_extra_resources flag. Request these resources in exec_properties \n    # using the \'resources:<name>\': \'<value>\' syntax.\n    custom_resources: []\n    # For example:\n    # - name: "" # (type: string)\n    #   value: 0 # (type: float64)\n\n    # executor.default_image (string): The default docker image to use to warm \n    # up executors or if no platform property is set. Ex: \n    # gcr.io/flame-public/executor-docker-default:enterprise-v1.5.4\n    default_image: gcr.io/flame-public/executor-docker-default:enterprise-v1.6.0\n    # executor.default_isolation_type (string): The default workload isolation \n    # type when no type is specified in an action. If not set, we use the \n    # first of the following that is set: docker, podman, firecracker, or none \n    # (bare).\n    default_isolation_type: ""\n    # executor.default_task_timeout (time.Duration): Timeout to use for tasks \n    # that do not have a timeout set explicitly.\n    default_task_timeout: 8h0m0s\n    # executor.default_termination_grace_period (time.Duration): Default \n    # termination grace period for all actions. (Termination grace period is \n    # the time to wait between an action timing out and forcefully shutting it \n    # down.)\n    default_termination_grace_period: 0s\n    # executor.default_xcode_version (string): Sets the default Xcode version \n    # number to use if an action doesn\'t specify one. If not set, \n    # /Applications/Xcode.app/ is used.\n    default_xcode_version: ""\n    # executor.delete_build_root_on_startup (bool): If true, delete the build \n    # root on startup\n    delete_build_root_on_startup: false\n    # executor.delete_filecache_on_startup (bool): If true, delete the file \n    # cache on startup\n    delete_filecache_on_startup: false\n    # executor.delete_parallelism (int): Number of goroutines to use when \n    # deleting files.\n    delete_parallelism: 0\n    # executor.die_on_firecracker_failure (bool): Makes the host executor \n    # process die if any command orchestrating or running Firecracker fails. \n    # Useful for capturing failures preemptively. WARNING: using this option \n    # MAY leave the host machine in an unhealthy state on Firecracker failure; \n    # some post-hoc cleanup may be necessary.\n    die_on_firecracker_failure: false\n    # executor.disable_local_cache (bool): If true, a local file cache will \n    # not be used.\n    disable_local_cache: false\n    # executor.docker_devices ([]container.DockerDeviceMapping): Configure \n    # (docker) devices that will be available inside the sandbox container. \n    # Format is \n    # --executor.docker_devices=\'[{"PathOnHost":"/dev/foo","PathInContainer":"/some/dest","CgroupPermissions":"see,docker,docs"}]\'\n    docker_devices: []\n    # For example:\n    # - path_on_host: "" # path to device that should be mapped from the host. (type: string)\n    #   path_in_container: "" # path under which the device will be present in container. (type: string)\n    #   cgroup_permissions: "" # cgroup permissions that should be assigned to device. (type: string)\n\n    # executor.docker_gpus (string): If set to \'all\', run docker containers \n    # with a device request for all GPUs.\n    docker_gpus: ""\n    # executor.docker_inherit_user_ids (bool): If set, run docker containers \n    # using the same uid and gid as the user running the executor process.\n    docker_inherit_user_ids: false\n    # executor.docker_mount_mode (string): Sets the mount mode of volumes \n    # mounted to docker images. Useful if running on SELinux \n    # https://www.projectatomic.io/blog/2015/06/using-volumes-with-docker-can-cause-problems-with-selinux/\n    docker_mount_mode: ""\n    # executor.docker_net_host (bool): Sets --net=host on the docker command. \n    # Intended for local development only. **DEPRECATED** Use \n    # --executor.docker_network=host instead.\n    docker_net_host: false\n    # executor.docker_network (string): If set, set docker/podman --network to \n    # this value by default. Can be overridden per-action with the \n    # `dockerNetwork` exec property, which accepts values \'off\' \n    # (--network=none) or \'bridge\' (--network=<default>).\n    docker_network: ""\n    # executor.docker_sibling_containers (bool): If set, mount the configured \n    # Docker socket to containers spawned for each action, to enable \n    # Docker-out-of-Docker (DooD). Takes effect only if docker_socket is also \n    # set. Should not be set by executors that can run untrusted code.\n    docker_sibling_containers: false\n    # executor.docker_socket (string): If set, run execution commands in \n    # docker using the provided socket.\n    docker_socket: ""\n    # executor.docker_volumes ([]string): Additional --volume arguments to be \n    # passed to docker or podman.\n    docker_volumes: []\n    # executor.enable_bare_runner (bool): Enables running execution commands \n    # directly on the host without isolation.\n    enable_bare_runner: false\n    # executor.enable_fastcopy_reflinking (bool): If true, attempt to use `cp \n    # --reflink=auto` to link files\n    enable_fastcopy_reflinking: false\n    # executor.enable_firecracker (bool): Enables running execution commands \n    # inside of firecracker VMs\n    enable_firecracker: false\n    # executor.enable_lease_reconnect (bool): Enable task lease reconnection \n    # on scheduler server shutdown.\n    enable_lease_reconnect: true\n    # executor.enable_local_snapshot_sharing (bool): Enables local snapshot \n    # sharing for firecracker VMs. Also requires that \n    # executor.firecracker_enable_nbd is true.\n    enable_local_snapshot_sharing: false\n    # executor.enable_oci (bool): Enables running execution commands using an \n    # OCI runtime directly.\n    enable_oci: false\n    # executor.enable_podman (bool): Enables running execution commands inside \n    # podman containers.\n    enable_podman: false\n    # executor.enable_remote_snapshot_sharing (bool): Enables remote snapshot \n    # sharing for firecracker VMs. Also requires that \n    # executor.firecracker_enable_nbd and executor.firecracker_enable_uffd are \n    # true.\n    enable_remote_snapshot_sharing: false\n    # executor.enable_sandbox (bool): Enables running execution commands \n    # inside of sandbox-exec.\n    enable_sandbox: false\n    # executor.enable_vfs (bool): Whether FUSE based filesystem is enabled.\n    enable_vfs: false\n    # executor.excess_capacity_threshold (float64): A percentage (of RAM and \n    # CPU) utilization below which this executor may request additional work\n    excess_capacity_threshold: 0.4\n    # executor.exclusive_task_scheduling (bool): If true, only one task will \n    # be scheduled at a time. Default is false\n    exclusive_task_scheduling: false\n    # executor.extra_env_vars ([]string): Additional environment variables to \n    # pass to remotely executed actions. i.e. MY_ENV_VAR=foo\n    extra_env_vars: []\n    # executor.firecracker_cgroup_version (string): Specifies the cgroup \n    # version for firecracker to use.\n    firecracker_cgroup_version: ""\n    # executor.firecracker_debug_stream_vm_logs (bool): Stream firecracker VM \n    # logs to the terminal.\n    firecracker_debug_stream_vm_logs: false\n    # executor.firecracker_debug_terminal (bool): Run an interactive terminal \n    # in the Firecracker VM connected to the executor\'s controlling terminal. \n    # For debugging only.\n    firecracker_debug_terminal: false\n    # executor.firecracker_enable_cpu_weight (bool): Set cgroup CPU weight to \n    # match VM size\n    firecracker_enable_cpu_weight: false\n    # executor.firecracker_enable_merged_rootfs (bool): Merges the containerfs \n    # and scratchfs into a single rootfs, removing the need to use overlayfs \n    # for the guest\'s root filesystem. Requires NBD to also be enabled.\n    firecracker_enable_merged_rootfs: false\n    # executor.firecracker_enable_uffd (bool): Enables userfaultfd for \n    # firecracker VMs.\n    firecracker_enable_uffd: false\n    # executor.firecracker_enable_vbd (bool): Enables the FUSE-based virtual \n    # block device interface for block devices.\n    firecracker_enable_vbd: false\n    # executor.firecracker_health_check_interval (time.Duration): How often to \n    # run VM health checks while tasks are executing.\n    firecracker_health_check_interval: 10s\n    # executor.firecracker_health_check_timeout (time.Duration): Timeout for \n    # VM health check requests.\n    firecracker_health_check_timeout: 30s\n    # executor.firecracker_init_on_alloc_and_free (bool): Set init_on_alloc=1 \n    # and init_on_free=1 in firecracker vms\n    firecracker_init_on_alloc_and_free: false\n    # executor.firecracker_overprovision_cpus (int): Number of CPUs to \n    # overprovision for VMs. This allows VMs to more effectively utilize CPU \n    # resources on the host machine. Set to -1 to allow all VMs to use max \n    # CPU.\n    firecracker_overprovision_cpus: 3\n    # executor.firecracker_workspace_disk_slack_space_mb (int64): Extra space \n    # to allocate to firecracker workspace disks, in megabytes. ** \n    # Experimental **\n    firecracker_workspace_disk_slack_space_mb: 2000\n    # executor.forced_network_isolation_type (string): If set, run all \n    # commands that require networking with this isolation\n    forced_network_isolation_type: ""\n    # executor.host_id (string): Optional: Allows for manual specification of \n    # an executor\'s host id. If not set, a random UUID will be used.\n    host_id: ""\n    # executor.host_root_directory (string): Path on the host where the \n    # executor container root directory is mounted.\n    host_root_directory: ""\n    # executor.image_pull_timeout (time.Duration): How long to wait for the \n    # container image to be pulled before returning an Unavailable (retryable) \n    # error for an action execution attempt. Applies to all isolation types \n    # (docker, firecracker, etc.)\n    image_pull_timeout: 5m0s\n    # executor.include_subdir_prefix (bool): If true, store files under \n    # subdirs named by the first 4 chars of file digest\n    include_subdir_prefix: false\n    # executor.local_cache_always_clone (bool): If true, files from the \n    # filecache will always be cloned instead of hardlinked\n    local_cache_always_clone: false\n    # executor.local_cache_directory (string): A local on-disk cache \n    # directory. Must be on the same device (disk partition, Docker volume, \n    # etc.) as the configured root_directory, since files are hard-linked to \n    # this cache for performance reasons. Otherwise, \'Invalid cross-device \n    # link\' errors may result.\n    local_cache_directory: /tmp/buildbuddy/filecache\n    # executor.local_cache_size_bytes (int64): The maximum size, in bytes, to \n    # use for the local on-disk cache\n    local_cache_size_bytes: 1000000000\n    # executor.max_task_timeout (time.Duration): Max timeout that can be \n    # requested by a task. A value <= 0 means unlimited. An error will be \n    # returned if a task requests a timeout greater than this value.\n    max_task_timeout: 24h0m0s\n    # executor.max_termination_grace_period (time.Duration): Max termination \n    # grace period that actions can request. An error will be returned if a \n    # task requests a grace period greater than this value. (Termination grace \n    # period is the time to wait between an action timing out and forcefully \n    # shutting it down.)\n    max_termination_grace_period: 1m0s\n    # executor.memory_bytes (int64): Optional maximum memory to allocate to \n    # execution tasks (approximate). Cannot set both this option and the \n    # SYS_MEMORY_BYTES env var.\n    memory_bytes: 0\n    # executor.metadata_directory (string): Location where executor host_id \n    # and other metadata is stored. Defaults to \n    # executor.local_cache_directory/../\n    metadata_directory: ""\n    # executor.millicpu (int64): Optional maximum CPU milliseconds to allocate \n    # to execution tasks (approximate). Cannot set both this option and the \n    # SYS_CPU env var.\n    millicpu: 0\n    # executor.mmap_memory_bytes (int64): Maximum memory to be allocated \n    # towards mmapped files for Firecracker copy-on-write functionality. This \n    # is subtraced from the configured memory_bytes. Has no effect if \n    # firecracker is disabled or snapshot sharing is disabled.\n    mmap_memory_bytes: 10000000000\n    # executor.nat_source_port_range (string): If set, restrict the source \n    # ports for NATed traffic to this range. \n    nat_source_port_range: ""\n    # executor.network_lock_directory (string): If set, use this directory to \n    # store lockfiles for allocated IP ranges. This is required if running \n    # multiple executors within the same networking environment.\n    network_lock_directory: ""\n    oci:\n        # executor.oci.dns (string): Specifies a custom DNS server for use \n        # inside OCI containers. If set to the empty string, mount \n        # /etc/resolv.conf from the host.\n        dns: 8.8.8.8\n        # executor.oci.enable_lxcfs (bool): Use lxcfs to fake cpu info inside \n        # containers.\n        enable_lxcfs: false\n        # executor.oci.network_pool_size (int): Limit on the number of \n        # networks to be reused between containers. Setting to 0 disables \n        # pooling. Setting to -1 uses the recommended default.\n        network_pool_size: -1\n        # executor.oci.runtime (string): OCI runtime\n        runtime: ""\n        # executor.oci.runtime_root (string): Root directory for storage of \n        # container state (see <runtime> --help for default)\n        runtime_root: ""\n    podman:\n        # executor.podman.dns (string): Specifies a custom DNS server for \n        # podman to use. Defaults to 8.8.8.8. If set to empty, no --dns= flag \n        # will be passed to podman.\n        dns: 8.8.8.8\n        # executor.podman.enable_image_streaming (bool): If set, all public \n        # (non-authenticated) podman images are streamed using soci artifacts \n        # generated and stored in the apps.\n        enable_image_streaming: false\n        # executor.podman.enable_private_image_streaming (bool): If set and \n        # --executor.podman.enable_image_streaming is set, all private \n        # (authenticated) podman images are streamed using soci artifacts \n        # generated and stored in the apps.\n        enable_private_image_streaming: false\n        # executor.podman.enable_stats (bool): Whether to enable cgroup-based \n        # podman stats.\n        enable_stats: true\n        # executor.podman.gpus (string): Specifies the value of the --gpus= \n        # flag to pass to podman. Set to \'all\' to pass all GPUs.\n        gpus: ""\n        # executor.podman.parallel_pulls (int): The system-wide maximum number \n        # of image layers to be pulled from remote container registries \n        # simultaneously. If set to 0, no value is set and podman will use its \n        # default value.\n        parallel_pulls: 0\n        # executor.podman.pids_limit (string): Specifies the value of the \n        # --pids-limit= flag to pass to podman. Set to \'-1\' for unlimited \n        # PIDs. The default is 2048 on systems that support pids cgroup \n        # controller.\n        pids_limit: ""\n        # executor.podman.pull_log_level (string): Level at which to log \n        # `podman pull` command output. Should be one of the standard log \n        # levels, all lowercase.\n        pull_log_level: ""\n        # executor.podman.pull_timeout (time.Duration): Timeout for image \n        # pulls.\n        pull_timeout: 10m0s\n        # executor.podman.runtime (string): Enables running podman with other \n        # runtimes, like gVisor (runsc).\n        runtime: ""\n        # executor.podman.soci_artifact_store_target (string): The GRPC url to \n        # use to access the SociArtifactStore GRPC service.\n        soci_artifact_store_target: ""\n        # executor.podman.soci_store_binary (string): The name of the \n        # soci-store binary to run. If empty, soci-store is not started even \n        # if it\'s needed (for local development).\n        soci_store_binary: soci-store\n        # executor.podman.soci_store_keychain_port (int): The port on which \n        # the soci-store local keychain service is exposed, for sharing \n        # credentials for streaming private container images.\n        soci_store_keychain_port: 1989\n        # executor.podman.soci_store_log_level (string): The level at which \n        # the soci-store should log. Should be one of the standard log levels, \n        # all lowercase.\n        soci_store_log_level: ""\n        # executor.podman.storage_driver (string): The podman storage driver \n        # to use.\n        storage_driver: overlay\n        # executor.podman.transient_store (bool): Enables --transient-store \n        # for podman commands. **DEPRECATED** --transient-store is now always \n        # applied if the podman version supports it\n        transient_store: false\n        # executor.podman.warmup_default_images (bool): Whether to warmup the \n        # default podman images or not.\n        warmup_default_images: true\n    # executor.pool (string): Executor pool name. Only one of this config \n    # option or the MY_POOL environment variable should be specified.\n    pool: ""\n    # executor.preserve_existing_netns (bool): Preserve existing bb-executor \n    # net namespaces. By default all "bb-executor" net namespaces are removed \n    # on executor startup, but if multiple executors are running on the same \n    # machine this behavior should be disabled to prevent them interfering \n    # with each other.\n    preserve_existing_netns: false\n    # executor.record_usage_timelines (bool): Capture resource usage \n    # timeseries data in UsageStats for each task.\n    record_usage_timelines: false\n    # executor.remote_snapshot_readonly (bool): Disables remote snapshot \n    # writes.\n    remote_snapshot_readonly: false\n    # executor.root_directory (string): The root directory to use for build \n    # files.\n    root_directory: /tmp/buildbuddy/remote_build\n    # executor.route_prefix (string): The prefix in the ip route to locate a \n    # device: either \'default\' or the ip range of the subnet e.g. \n    # 172.24.0.0/18\n    route_prefix: default\n    runner_pool:\n        # executor.runner_pool.max_runner_count (int): Maximum number of \n        # recycled RBE runners that can be pooled at once. Defaults to a value \n        # derived from estimated CPU usage, max RAM, allocated CPU, and \n        # allocated memory.\n        max_runner_count: 0\n        # executor.runner_pool.max_runner_disk_size_bytes (int64): Maximum \n        # disk size for a recycled runner; runners exceeding this threshold \n        # are not recycled. Defaults to 16GB.\n        max_runner_disk_size_bytes: 16000000000\n        # executor.runner_pool.max_runner_memory_usage_bytes (int64): Maximum \n        # memory usage for a recycled runner; runners exceeding this threshold \n        # are not recycled.\n        max_runner_memory_usage_bytes: 0\n    # executor.shutdown_cleanup_duration (time.Duration): The minimum duration \n    # during the shutdown window to allocate for cleaning up containers. This \n    # is capped to the value of `max_shutdown_duration`.\n    shutdown_cleanup_duration: 15s\n    # executor.slow_task_threshold (time.Duration): Warn about tasks that take \n    # longer than this threshold.\n    slow_task_threshold: 1h0m0s\n    # executor.snaploader_eager_fetch_concurrency (int): Max number of \n    # goroutines allowed to run concurrently when eagerly fetching chunks.\n    snaploader_eager_fetch_concurrency: 32\n    # executor.snaploader_max_eager_fetches_per_sec (int): Max number of \n    # chunks snaploader can eagerly fetch in the background per second.\n    snaploader_max_eager_fetches_per_sec: 1000\n    # executor.startup_warmup_max_wait_secs (int64): Maximum time to block \n    # startup while waiting for default image to be pulled. Default is no \n    # wait.\n    startup_warmup_max_wait_secs: 0\n    # executor.task_ip_range (string): Subnet to allocate IP addresses from \n    # for actions that require network access. Must be a /16 range.\n    task_ip_range: 192.168.0.0/16\n    # executor.task_progress_publish_interval (time.Duration): How often tasks \n    # should publish progress updates to the app.\n    task_progress_publish_interval: 1m0s\n    # executor.verbose_snapshot_logs (bool): Enables extra-verbose snapshot \n    # logs (even at debug log level)\n    verbose_snapshot_logs: false\n    vfs:\n        # executor.vfs.log_fuse_latency_stats (bool): Enables logging of \n        # per-operation latency stats when VFS is unmounted. Implicitly \n        # enabled by --executor.vfs.verbose.\n        log_fuse_latency_stats: false\n        # executor.vfs.log_fuse_per_file_stats (bool): Enables tracking and \n        # logging of per-file per-operation stats. Logged when VFS is \n        # unmounted.\n        log_fuse_per_file_stats: false\n        # executor.vfs.verbose (bool): Enables verbose logs for VFS \n        # operations.\n        verbose: false\n        # executor.vfs.verbose_fuse (bool): Enables low-level verbose logs in \n        # the go-fuse library.\n        verbose_fuse: false\n    # executor.warmup_additional_images ([]string): List of container images \n    # to warm up alongside the executor default images on executor start up.\n    warmup_additional_images: []\n    # executor.warmup_timeout_secs (int64): The default time (in seconds) to \n    # wait for an executor to warm up i.e. download the default docker image. \n    # Default is 120s\n    warmup_timeout_secs: 120\n    # executor.warmup_workflow_images (bool): Whether to warm up the Linux \n    # workflow images (firecracker only).\n    warmup_workflow_images: false\n    workspace:\n        # executor.workspace.overlayfs_enabled (bool): Enable overlayfs \n        # support for anonymous action workspaces. ** UNSTABLE **\n        overlayfs_enabled: false\ngithub:\n    # github.access_token (string): The GitHub access token used to post \n    # GitHub commit statuses. ** Enterprise only **\n    access_token: ""\n    # github.client_id (string): The client ID of your GitHub Oauth App. ** \n    # Enterprise only **\n    client_id: ""\n    # github.client_secret (string): The client secret of your GitHub Oauth \n    # App. ** Enterprise only **\n    client_secret: ""\n    # github.enterprise_host (string): The Github enterprise hostname to use \n    # if using GitHub enterprise server, not including https:// and no \n    # trailing slash.\n    enterprise_host: ""\n    # github.jwt_key (string): The key to use when signing JWT tokens for \n    # github auth.\n    jwt_key: ""\n    # github.status_name_suffix (string): Suffix to be appended to all \n    # reported GitHub status names. Useful for differentiating BuildBuddy \n    # deployments. For example: \'(dev)\' ** Enterprise only **\n    status_name_suffix: ""\ngrpc_client:\n    # grpc_client.pool_size (int): Number of connections to create to each \n    # target.\n    pool_size: 15\nmetrics:\n    # metrics.observed_group_ids ([]string): Group IDs allowed in group_id \n    # metrics labels. Groups not in this list will be labeled as \'ANON\' if \n    # unauthenticated or \'other\' if authenticated.\n    observed_group_ids: []\nmonitoring:\n    basic_auth:\n        # monitoring.basic_auth.password (string): Optional password for basic \n        # auth on the monitoring port.\n        password: ""\n        # monitoring.basic_auth.username (string): Optional username for basic \n        # auth on the monitoring port.\n        username: ""\n    # monitoring.ssl_port (int): If non-negative, the SSL port to listen for \n    # monitoring traffic on. `ssl` config must have `ssl_enabled: true` and be \n    # properly configured.\n    ssl_port: -1\nolap_database:\n    # olap_database.cluster_name (string): The cluster name of the database\n    cluster_name: \'{cluster}\'\n    # olap_database.enable_data_replication (bool): If true, data replication \n    # is enabled.\n    enable_data_replication: false\n    # olap_database.replica_name (string): The replica name of the table in \n    # zookeeper\n    replica_name: \'{replica}\'\n    # olap_database.zoo_path (string): The path to the table name in \n    # zookeeper, used to set up data replication\n    zoo_path: /clickhouse/{installation}/{cluster}/tables/{shard}/{database}/{table}\nremote_execution:\n    # remote_execution.ci_runner_default_timeout (time.Duration): Default \n    # timeout applied to all ci runners.\n    ci_runner_default_timeout: 8h0m0s\n    # remote_execution.ci_runner_recycling_max_wait (time.Duration): Max \n    # duration that a ci_runner task should wait for a warm runner before \n    # running on a potentially cold runner.\n    ci_runner_recycling_max_wait: 3s\n    # remote_execution.cpu_quota_limit (time.Duration): Maximum CPU time \n    # allowed for each quota period.\n    cpu_quota_limit: 3s\n    # remote_execution.cpu_quota_period (time.Duration): How often the CPU \n    # quota is refreshed.\n    cpu_quota_period: 100ms\n    # remote_execution.enable_remote_exec (bool): If true, enable remote-exec. \n    # ** Enterprise only **\n    enable_remote_exec: true\n    # remote_execution.pids_limit (int64): Maximum number of processes allowed \n    # per task at any time.\n    pids_limit: 2048\n    # remote_execution.redis_target (string): A Redis target for storing \n    # remote execution state. Falls back to app.default_redis_target if \n    # unspecified. Required for remote execution. To ease migration, the redis \n    # target from the cache config will be used if neither this value nor \n    # app.default_redis_target are specified.\n    redis_target: ""\n    sharded_redis:\n        # remote_execution.sharded_redis.password (string): Redis password\n        password: ""\n        # remote_execution.sharded_redis.shards ([]string): Ordered list of \n        # Redis shard addresses.\n        shards: []\n        # remote_execution.sharded_redis.username (string): Redis username\n        username: ""\n    # remote_execution.stored_task_size_millicpu_limit (int64): Limit placed \n    # on milliCPU calculated from task execution statistics.\n    stored_task_size_millicpu_limit: 7500\n    task_size_model:\n        # remote_execution.task_size_model.enabled (bool): Whether to enable \n        # model-based task size prediction.\n        enabled: false\n        # remote_execution.task_size_model.features_config_path (string): Path \n        # pointing to features.json config file.\n        features_config_path: ""\n        # remote_execution.task_size_model.serving_address (string): gRPC \n        # address pointing to TensorFlow Serving prediction service with task \n        # size models (cpu, mem).\n        serving_address: ""\n    # remote_execution.task_size_psi_correction (float64): What percentage of \n    # full-stall time should be subtracted from the execution duration.\n    task_size_psi_correction: 1\n    # remote_execution.use_measured_task_sizes (bool): Whether to use measured \n    # usage stats to determine task sizes.\n    use_measured_task_sizes: false\nssl:\n    # ssl.cert_file (string): Path to a PEM encoded certificate file to use \n    # for TLS if not using ACME.\n    cert_file: ""\n    # ssl.client_ca_cert (string): PEM encoded certificate authority used to \n    # issue client certificates for mTLS auth.\n    client_ca_cert: ""\n    # ssl.client_ca_cert_file (string): Path to a PEM encoded certificate \n    # authority file used to issue client certificates for mTLS auth.\n    client_ca_cert_file: ""\n    # ssl.client_ca_key (string): PEM encoded certificate authority key used \n    # to issue client certificates for mTLS auth.\n    client_ca_key: ""\n    # ssl.client_ca_key_file (string): Path to a PEM encoded certificate \n    # authority key file used to issue client certificates for mTLS auth.\n    client_ca_key_file: ""\n    # ssl.client_cert_lifespan (time.Duration): The duration client \n    # certificates are valid for. Ex: \'730h\' for one month. If not set, \n    # defaults to 100 years.\n    client_cert_lifespan: 876000h0m0s\n    # ssl.default_host (string): Host name to use for ACME generated cert if \n    # TLS request does not contain SNI.\n    default_host: ""\n    # ssl.enable_ssl (bool): Whether or not to enable SSL/TLS on gRPC \n    # connections (gRPCS).\n    enable_ssl: false\n    # ssl.host_whitelist ([]string): Cloud-Only\n    host_whitelist: []\n    # ssl.key_file (string): Path to a PEM encoded key file to use for TLS if \n    # not using ACME.\n    key_file: ""\n    # ssl.self_signed (bool): If true, a self-signed cert will be generated \n    # for TLS termination.\n    self_signed: false\n    # ssl.upgrade_insecure (bool): True if http requests should be redirected \n    # to https. Assumes http traffic is served on port 80 and https traffic is \n    # served on port 443 (typically via an ingress / load balancer).\n    upgrade_insecure: false\n    # ssl.use_acme (bool): Whether or not to automatically configure SSL certs \n    # using ACME. If ACME is enabled, cert_file and key_file should not be \n    # set.\n    use_acme: false\n'})})}function _(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}const u={id:"config-all-options",title:"All Options",sidebar_label:"All Options"},p=void 0,h={},f=[{value:"BuildBuddy Server (FOSS)",id:"buildbuddy-server-foss",level:3},{value:"BuildBuddy Server (Enterprise)",id:"buildbuddy-server-enterprise",level:3},{value:"BuildBuddy Executor",id:"buildbuddy-executor",level:3}];function g(e){const n={code:"code",h3:"h3",p:"p",pre:"pre",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsxs)(n.p,{children:["Provided below are working, documented YAML configs for each BuildBuddy binary\ncontaining every option that that binary accepts, each set to the default value\nfor that option. Any option that can be specified in the YAML config can also be\npassed on the command line. For nested options, be sure to write out the full\nYAML path, with a ",(0,a.jsx)(n.code,{children:"."})," separating each part."]}),"\n",(0,a.jsx)(n.p,{children:"For example:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",metastring:'title="config.yaml"',children:"storage:\n  disk:\n    root_directory: /tmp/buildbuddy\n"})}),"\n",(0,a.jsx)(n.p,{children:"becomes:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-sh",children:'buildbuddy -storage.disk.root_directory="/tmp/buildbuddy"\n'})}),"\n",(0,a.jsx)(n.p,{children:"For specifying lists of structures using flags on the command line, use the JSON\nrepresentation of the list you wish to concatenate to the end or the element you\nwish to append:"}),"\n",(0,a.jsx)(n.p,{children:"For example, given the following schema:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",metastring:'title="config.yaml"',children:'cache:\n  disk:\n    partitions: [] # type: []disk.Partition\n    # e.g.:\n    # - id: "" # type: string\n    #   max_size_bytes: 0 # type: int\n'})}),"\n",(0,a.jsxs)(n.p,{children:["We see that ",(0,a.jsx)(n.code,{children:"cache.disk.partitions"})," is configured as a list of ",(0,a.jsx)(n.code,{children:"disk.Partition"}),". In YAML, we'd normally configure it like this:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",metastring:'title="config.yaml"',children:'cache:\n  disk:\n    partitions:\n      - id: "1GB"\n        max_size_bytes: 1073741824\n      - id: "2GB"\n        max_size_bytes: 2147483648\n'})}),"\n",(0,a.jsx)(n.p,{children:"The flag equivalent of this example would be:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-sh",children:'buildbuddy -cache.disk.partitions=\'{"id": "1GB", "max_size_bytes": 1073741824}\' -cache.disk.partitions=\'{"id": "2GB", "max_size_bytes": 2147483648}\'\n'})}),"\n",(0,a.jsx)(n.p,{children:"or"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-sh",children:'buildbuddy -cache.disk.partitions=\'[{"id": "1GB", "max_size_bytes": 1073741824}, {"id": "2GB", "max_size_bytes": 2147483648}]\'\n'})}),"\n",(0,a.jsx)(n.h3,{id:"buildbuddy-server-foss",children:"BuildBuddy Server (FOSS)"}),"\n",(0,a.jsx)(s,{}),"\n",(0,a.jsx)(n.h3,{id:"buildbuddy-server-enterprise",children:"BuildBuddy Server (Enterprise)"}),"\n",(0,a.jsx)(c,{}),"\n",(0,a.jsx)(n.h3,{id:"buildbuddy-executor",children:"BuildBuddy Executor"}),"\n",(0,a.jsx)(_,{})]})}function b(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(g,{...e})}):g(e)}},28453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>s});var o=t(96540);const a={},i=o.createContext(a);function r(e){const n=o.useContext(i);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),o.createElement(i.Provider,{value:n},e.children)}}}]);
"use strict";(self.webpackChunkbuildbuddy_docs_website=self.webpackChunkbuildbuddy_docs_website||[]).push([[2234],{3905:function(e,t,n){n.d(t,{Zo:function(){return d},kt:function(){return h}});var a=n(67294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,o=function(e,t){if(null==e)return{};var n,a,o={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var l=a.createContext({}),c=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},d=function(e){var t=c(e.components);return a.createElement(l.Provider,{value:t},e.children)},_="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},p=a.forwardRef((function(e,t){var n=e.components,o=e.mdxType,i=e.originalType,l=e.parentName,d=s(e,["components","mdxType","originalType","parentName"]),_=c(n),p=o,h=_["".concat(l,".").concat(p)]||_[p]||u[p]||i;return n?a.createElement(h,r(r({ref:t},d),{},{components:n})):a.createElement(h,r({ref:t},d))}));function h(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var i=n.length,r=new Array(i);r[0]=p;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[_]="string"==typeof e?e:o,r[1]=s;for(var c=2;c<i;c++)r[c]=n[c];return a.createElement.apply(null,r)}return a.createElement.apply(null,n)}p.displayName="MDXCreateElement"},31564:function(e,t,n){n.r(t),n.d(t,{assets:function(){return v},contentTitle:function(){return w},default:function(){return S},frontMatter:function(){return y},metadata:function(){return k},toc:function(){return x}});var a=n(83117),o=n(80102),i=(n(67294),n(3905)),r=["components"],s={toc:[]},l="wrapper";function c(e){var t=e.components,n=(0,o.Z)(e,r);return(0,i.kt)(l,(0,a.Z)({},s,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-yaml"},'# Unstructured settings\n\n# app_directory (string): the directory containing app binary files to host\napp_directory: ""\n# auto_migrate_db (bool): If true, attempt to automigrate the db when \n# connecting\nauto_migrate_db: true\n# auto_migrate_db_and_exit (bool): If true, attempt to automigrate the db when \n# connecting, then exit the program.\nauto_migrate_db_and_exit: false\n# cache_stats_finalization_delay (time.Duration): The time allowed for all \n# metrics collectors across all apps to flush their local cache stats to the \n# backing storage, before finalizing stats in the DB.\ncache_stats_finalization_delay: 500ms\n# cleanup_interval (time.Duration): How often the janitor cleanup tasks will \n# run\ncleanup_interval: 10m0s\n# cleanup_workers (int): How many cleanup tasks to run\ncleanup_workers: 1\n# disable_ga (bool): If true; ga will be disabled\ndisable_ga: false\n# disable_telemetry (bool): If true; telemetry will be disabled\ndisable_telemetry: false\n# drop_invocation_pk_cols (bool): If true, attempt to drop invocation PK cols\ndrop_invocation_pk_cols: false\n# exit_when_ready (bool): If set, the app will exit as soon as it becomes \n# ready (useful for migrations)\nexit_when_ready: false\n# grpc_client_origin_header (string): Header value to set for \n# x-buildbuddy-origin.\ngrpc_client_origin_header: ""\n# grpc_port (int): The port to listen for gRPC traffic on\ngrpc_port: 1985\n# grpcs_port (int): The port to listen for gRPCS traffic on\ngrpcs_port: 1986\n# internal_grpc_port (int): The port to listen for internal gRPC traffic on\ninternal_grpc_port: 1987\n# internal_grpcs_port (int): The port to listen for internal gRPCS traffic on\ninternal_grpcs_port: 1988\n# internal_http_port (int): The port to listen for internal HTTP traffic\ninternal_http_port: 0\n# js_entry_point_path (string): Absolute URL path of the app JS entry point\njs_entry_point_path: /app/app_bundle/app.js?hash={APP_BUNDLE_HASH}\n# listen (string): The interface to listen on (default: 0.0.0.0)\nlisten: 0.0.0.0\n# log_deletion_errors (bool): If true; log errors when ttl-deleting expired \n# data\nlog_deletion_errors: false\n# log_goroutine_profile_on_shutdown (bool): Whether to log all goroutine stack \n# traces on shutdown.\nlog_goroutine_profile_on_shutdown: false\n# max_shutdown_duration (time.Duration): Time to wait for shutdown\nmax_shutdown_duration: 25s\n# migrate_disk_cache_to_v2_and_exit (bool): If true, attempt to migrate disk \n# cache to v2 layout.\nmigrate_disk_cache_to_v2_and_exit: false\n# monitoring_port (int): The port to listen for monitoring traffic on\nmonitoring_port: 9090\n# port (int): The port to listen for HTTP traffic on\nport: 8080\n# report_not_ready (bool): If set to true, the app will always report as being \n# unready.\nreport_not_ready: false\n# server_type (string): The server type to match on health checks\nserver_type: buildbuddy-server\n# shutdown_lameduck_duration (time.Duration): If set, the server will be \n# marked unready but not run shutdown functions until this period passes.\nshutdown_lameduck_duration: 0s\n# ssl_port (int): The port to listen for HTTPS traffic on\nssl_port: 8081\n# static_directory (string): the directory containing static files to host\nstatic_directory: ""\n# telemetry_endpoint (string): The telemetry endpoint to use\ntelemetry_endpoint: grpcs://t.buildbuddy.io:443\n# telemetry_interval (time.Duration): How often telemetry data will be \n# reported\ntelemetry_interval: 24h0m0s\n# verbose_telemetry_client (bool): If true; print telemetry client information\nverbose_telemetry_client: false\n\n# Structured settings\n\napi:\n    # api.api_key (string): The default API key to use for on-prem enterprise \n    # deploys with a single organization/group. **DEPRECATED** Manual API key \n    # specification is no longer supported; to retrieve specific API keys \n    # programmatically, please use the API key table. This field will still \n    # specify an API key to redact in case a manual API key was specified when \n    # buildbuddy was first set up.\n    api_key: ""\napp:\n    # app.admin_only_create_group (bool): If true, only admins of an existing \n    # group can create a new groups.\n    admin_only_create_group: false\n    # app.audit_logs_enabled (bool): Whether to log administrative events to \n    # an audit log. Requires OLAP database to be configured.\n    audit_logs_enabled: false\n    # app.audit_logs_ui_enabled (bool): If set, the audit logs UI will be \n    # accessible from the sidebar.\n    audit_logs_ui_enabled: false\n    # app.build_buddy_url (URL): The external URL where your BuildBuddy \n    # instance can be found.\n    build_buddy_url: http://localhost:8080\n    # app.cache_api_url (URL): Overrides the default remote cache protocol \n    # gRPC address shown by BuildBuddy on the configuration screen.\n    cache_api_url: ""\n    # app.code_editor_enabled (bool): If set, code editor functionality will \n    # be enabled.\n    code_editor_enabled: false\n    # app.customer_managed_encryption_keys_enabled (bool): If set, show \n    # customer-managed encryption configuration UI.\n    customer_managed_encryption_keys_enabled: false\n    # app.default_subdomains ([]string): List of subdomains that should not be \n    # handled as user-owned subdomains.\n    default_subdomains: []\n    # app.default_to_dense_mode (bool): Enables the dense UI mode by default.\n    default_to_dense_mode: false\n    # app.disable_cert_config (bool): If true, the certificate based auth \n    # option will not be shown in the config widget.\n    disable_cert_config: false\n    # app.enable_grpc_metrics_by_group_id (bool): If enabled, grpc metrics by \n    # group ID will be recorded\n    enable_grpc_metrics_by_group_id: false\n    # app.enable_prometheus_histograms (bool): If true, collect prometheus \n    # histograms for all RPCs\n    enable_prometheus_histograms: true\n    # app.enable_read_target_statuses_from_olap_db (bool): If enabled, read \n    # target statuses from OLAP DB\n    enable_read_target_statuses_from_olap_db: false\n    # app.enable_structured_logging (bool): If true, log messages will be \n    # json-formatted.\n    enable_structured_logging: false\n    # app.enable_subdomain_matching (bool): If true, request subdomain will be \n    # taken into account when determining what request restrictions should be \n    # applied.\n    enable_subdomain_matching: false\n    # app.enable_target_tracking (bool): Cloud-Only\n    enable_target_tracking: false\n    # app.enable_write_executions_to_olap_db (bool): If enabled, complete \n    # Executions will be flushed to OLAP DB\n    enable_write_executions_to_olap_db: false\n    # app.enable_write_test_target_statuses_to_olap_db (bool): If enabled, \n    # test target statuses will be flushed to OLAP DB\n    enable_write_test_target_statuses_to_olap_db: false\n    # app.enable_write_to_olap_db (bool): If enabled, complete invocations \n    # will be flushed to OLAP DB\n    enable_write_to_olap_db: true\n    # app.events_api_url (URL): Overrides the default build event protocol \n    # gRPC address shown by BuildBuddy on the configuration screen.\n    events_api_url: ""\n    # app.execution_search_enabled (bool): If set, fetch lists of executions \n    # from the OLAP DB in the trends UI.\n    execution_search_enabled: true\n    # app.expanded_suggestions_enabled (bool): If set, enable more build \n    # suggestions in the UI.\n    expanded_suggestions_enabled: false\n    # app.grpc_max_recv_msg_size_bytes (int): Configures the max GRPC receive \n    # message size [bytes]\n    grpc_max_recv_msg_size_bytes: 50000000\n    # app.grpc_over_http_port_enabled (bool): Cloud-Only\n    grpc_over_http_port_enabled: false\n    # app.ignore_forced_tracing_header (bool): If set, we will not honor the \n    # forced tracing header.\n    ignore_forced_tracing_header: false\n    # app.log_enable_gcp_logging_format (bool): If true, the output structured \n    # logs will be compatible with format expected by GCP Logging.\n    log_enable_gcp_logging_format: false\n    # app.log_error_stack_traces (bool): If true, stack traces will be printed \n    # for errors that have them.\n    log_error_stack_traces: false\n    # app.log_gcp_log_id (string): The log ID to log to in GCP (if any).\n    log_gcp_log_id: ""\n    # app.log_gcp_project_id (string): The project ID to log to in GCP (if \n    # any).\n    log_gcp_project_id: ""\n    # app.log_include_short_file_name (bool): If true, log messages will \n    # include shortened originating file name.\n    log_include_short_file_name: false\n    # app.log_level (string): The desired log level. Logs with a level >= this \n    # level will be emitted. One of {\'fatal\', \'error\', \'warn\', \'info\', \n    # \'debug\'}\n    log_level: info\n    # app.paginate_invocations (bool): If true, paginate invocations returned \n    # to the UI.\n    paginate_invocations: false\n    # app.pattern_filter_enabled (bool): If set, allow filtering by pattern in \n    # the client.\n    pattern_filter_enabled: true\n    # app.remote_execution_api_url (URL): Overrides the default remote \n    # execution protocol gRPC address shown by BuildBuddy on the configuration \n    # screen.\n    remote_execution_api_url: ""\n    # app.tags_enabled (bool): Enable setting tags on invocations via \n    # build_metadata\n    tags_enabled: false\n    # app.tags_ui_enabled (bool): If set, expose tags data and let users \n    # filter by tag.\n    tags_ui_enabled: false\n    # app.test_grid_v2_enabled (bool): Whether to enable test grid V2\n    test_grid_v2_enabled: true\n    # app.test_output_manifests_enabled (bool): If set, the target page will \n    # render the contents of test output zips.\n    test_output_manifests_enabled: true\n    # app.timeseries_charts_in_timing_profile_enabled (bool): If set, charts \n    # with sampled time series data (such as CPU and memory usage) will be \n    # shown\n    timeseries_charts_in_timing_profile_enabled: true\n    # app.trace_fraction (float64): Fraction of requests to sample for \n    # tracing.\n    trace_fraction: 0\n    # app.trace_fraction_overrides ([]string): Tracing fraction override based \n    # on name in format name=fraction.\n    trace_fraction_overrides: []\n    # app.trace_jaeger_collector (string): Address of the Jager collector \n    # endpoint where traces will be sent.\n    trace_jaeger_collector: ""\n    # app.trace_project_id (string): Optional GCP project ID to export traces \n    # to. If not specified, determined from default credentials or metadata \n    # server if running on GCP.\n    trace_project_id: ""\n    # app.trace_service_name (string): Name of the service to associate with \n    # traces.\n    trace_service_name: ""\n    # app.trends_heatmap_enabled (bool): If set, enable a fancy heatmap UI for \n    # exploring build trends.\n    trends_heatmap_enabled: true\n    # app.trends_summary_enabled (bool): If set, show the new \'summary\' \n    # section at the top of the trends UI.\n    trends_summary_enabled: false\n    # app.usage_enabled (bool): If set, the usage page will be enabled in the \n    # UI.\n    usage_enabled: false\n    # app.user_management_enabled (bool): If set, the user management page \n    # will be enabled in the UI.\n    user_management_enabled: true\n    # app.workflow_history_enabled (bool): If set, information about past \n    # action runs for workflows will be shown on the workflows page.\n    workflow_history_enabled: false\nauth:\n    # auth.domain_wide_cookies (bool): If true, cookies will have domain set \n    # so that they are accessible on domain and all subdomains.\n    domain_wide_cookies: false\n    # auth.https_only_cookies (bool): If true, cookies will only be set over \n    # https connections.\n    https_only_cookies: false\n    # auth.jwt_key (string): The key to use when signing JWT tokens.\n    jwt_key: set_the_jwt_in_config\nbuild_event_proxy:\n    # build_event_proxy.buffer_size (int): The number of build events to \n    # buffer locally when proxying build events.\n    buffer_size: 100\n    # build_event_proxy.hosts ([]string): The list of hosts to pass build \n    # events onto.\n    hosts: []\ncache:\n    client:\n        # cache.client.enable_upload_compression (bool): If true, enable \n        # compression of uploads to remote caches\n        enable_upload_compression: true\n    # cache.detailed_stats_enabled (bool): Whether to enable detailed stats \n    # recording for all cache requests.\n    detailed_stats_enabled: false\n    disk:\n        # cache.disk.partition_mappings ([]disk.PartitionMapping)\n        partition_mappings: []\n        # For example:\n        # - group_id: "" # The Group ID to which this mapping applies. (type: string)\n        #   prefix: "" # The remote instance name prefix used to select this partition. (type: string)\n        #   partition_id: "" # The partition to use if the Group ID and prefix match. (type: string)\n\n        # cache.disk.partitions ([]disk.Partition)\n        partitions: []\n        # For example:\n        # - id: "" # The ID of the partition. (type: string)\n        #   max_size_bytes: 0 # Maximum size of the partition. (type: int64)\n        #   encryption_supported: false # Whether encrypted data can be stored on this partition. (type: bool)\n\n        # cache.disk.root_directory (string): The root directory to store all \n        # blobs in, if using disk based storage.\n        root_directory: ""\n        # cache.disk.use_v2_layout (bool): If enabled, files will be stored \n        # using the v2 layout. See disk_cache.MigrateToV2Layout for a \n        # description.\n        use_v2_layout: false\n    # cache.enable_tree_caching (bool): If true, cache GetTree responses (full \n    # and partial)\n    enable_tree_caching: true\n    # cache.in_memory (bool): Whether or not to use the in_memory cache.\n    in_memory: false\n    # cache.max_size_bytes (int64): How big to allow the cache to be (in \n    # bytes).\n    max_size_bytes: 10000000000\n    # cache.max_tree_cache_set_duration (time.Duration): The max amount of \n    # time to wait for unfinished tree cache entries to be set.\n    max_tree_cache_set_duration: 1s\n    # cache.tree_cache_min_descendents (int): The min number of descendents a \n    # node must parent in order to be cached\n    tree_cache_min_descendents: 10\n    # cache.tree_cache_min_level (int): The min level at which the tree may be \n    # cached. 0 is the root\n    tree_cache_min_level: 1\n    # cache.tree_cache_seed (string): If set, hash this with digests before \n    # caching / reading from tree cache\n    tree_cache_seed: treecache-03011023\n    # cache.zstd_transcoding_enabled (bool): Whether to accept requests to \n    # read/write zstd-compressed blobs, compressing/decompressing \n    # outgoing/incoming blobs on the fly.\n    zstd_transcoding_enabled: true\ndatabase:\n    # database.advanced_data_source (db.AdvancedConfig): Alternative to the \n    # database.data_source flag that allows finer control over database \n    # settings as well as allowing use of AWS IAM credentials. For most users, \n    # database.data_source is a simpler configuration method.\n    advanced_data_source:\n        driver: "" # The driver to use: one of sqlite3, mysql, or postgresql. (type: string)\n        endpoint: "" # Typically the host:port combination of the database server. (type: string)\n        username: "" # Username to use when connecting. (type: string)\n        password: "" # Password to use when connecting. Not used if AWS IAM is enabled. (type: string)\n        db_name: "" # The name of the database to use for BuildBuddy data. (type: string)\n        region: "" # Region of the database instance. Required if AWS IAM is enabled. (type: string)\n        use_aws_iam: false # If enabled, AWS IAM authentication is used instead of fixed credentials. Make sure the endpoint includes the port, otherwise IAM-based auth will fail. (type: bool)\n        params: "" # Optional parameters to pass to the database driver (in format key1=val1&key2=val2) (type: string)\n    # database.advanced_read_replica (db.AdvancedConfig): Advanced alternative \n    # to database.read_replica. Refer to database.advanced for more \n    # information.\n    advanced_read_replica:\n        driver: "" # The driver to use: one of sqlite3, mysql, or postgresql. (type: string)\n        endpoint: "" # Typically the host:port combination of the database server. (type: string)\n        username: "" # Username to use when connecting. (type: string)\n        password: "" # Password to use when connecting. Not used if AWS IAM is enabled. (type: string)\n        db_name: "" # The name of the database to use for BuildBuddy data. (type: string)\n        region: "" # Region of the database instance. Required if AWS IAM is enabled. (type: string)\n        use_aws_iam: false # If enabled, AWS IAM authentication is used instead of fixed credentials. Make sure the endpoint includes the port, otherwise IAM-based auth will fail. (type: bool)\n        params: "" # Optional parameters to pass to the database driver (in format key1=val1&key2=val2) (type: string)\n    # database.conn_max_lifetime_seconds (int): The maximum lifetime of a \n    # connection to the db\n    conn_max_lifetime_seconds: 0\n    # database.data_source (string): The SQL database to connect to, specified \n    # as a connection string.\n    data_source: sqlite3:///tmp/buildbuddy.db\n    # database.log_queries (bool): If true, log all queries\n    log_queries: false\n    # database.max_idle_conns (int): The maximum number of idle connections to \n    # maintain to the db\n    max_idle_conns: 0\n    # database.max_open_conns (int): The maximum number of open connections to \n    # maintain to the db\n    max_open_conns: 0\n    # database.print_schema_changes_and_exit (bool): If set, print schema \n    # changes from auto-migration, then exit the program.\n    print_schema_changes_and_exit: false\n    # database.read_replica (string): A secondary, read-only SQL database to \n    # connect to, specified as a connection string.\n    read_replica: ""\n    # database.slow_query_threshold (time.Duration): Queries longer than this \n    # duration will be logged with a \'Slow SQL\' warning.\n    slow_query_threshold: 500ms\n    # database.stats_poll_interval (time.Duration): How often to poll the DB \n    # client for connection stats (default: \'5s\').\n    stats_poll_interval: 5s\ngithub:\n    # github.access_token (string): The GitHub access token used to post \n    # GitHub commit statuses. ** Enterprise only **\n    access_token: ""\n    app:\n        # github.app.client_id (string): GitHub app OAuth client ID.\n        client_id: ""\n        # github.app.client_secret (string): GitHub app OAuth client secret.\n        client_secret: ""\n        # github.app.enabled (bool): Whether to enable the BuildBuddy GitHub \n        # app server.\n        enabled: false\n        # github.app.id (string): GitHub app ID.\n        id: ""\n        # github.app.private_key (string): GitHub app private key.\n        private_key: ""\n        # github.app.public_link (string): GitHub app installation URL.\n        public_link: ""\n        # github.app.webhook_secret (string): GitHub app webhook secret used \n        # to verify that webhook payload contents were sent by GitHub.\n        webhook_secret: ""\n    # github.client_id (string): The client ID of your GitHub Oauth App. ** \n    # Enterprise only **\n    client_id: ""\n    # github.client_secret (string): The client secret of your GitHub Oauth \n    # App. ** Enterprise only **\n    client_secret: ""\n    # github.jwt_key (string): The key to use when signing JWT tokens for \n    # github auth.\n    jwt_key: ""\n    # github.status_name_suffix (string): Suffix to be appended to all \n    # reported GitHub status names. Useful for differentiating BuildBuddy \n    # deployments. For example: \'(dev)\' ** Enterprise only **\n    status_name_suffix: ""\n    # github.status_per_test_target (bool): If true, report status per test \n    # target. ** Enterprise only **\n    status_per_test_target: false\nintegrations:\n    invocation_upload:\n        # integrations.invocation_upload.enabled (bool): Whether to upload \n        # webhook data to the webhook URL configured per-Group. ** Enterprise \n        # only **\n        enabled: false\n        # integrations.invocation_upload.gcs_credentials (string): Credentials \n        # JSON for the Google service account used to authenticate when GCS is \n        # used as the invocation upload target. ** Enterprise only **\n        gcs_credentials: ""\n    slack:\n        # integrations.slack.webhook_url (string): A Slack webhook url to post \n        # build update messages to.\n        webhook_url: ""\nmonitoring:\n    basic_auth:\n        # monitoring.basic_auth.password (string): Optional password for basic \n        # auth on the monitoring port.\n        password: ""\n        # monitoring.basic_auth.username (string): Optional username for basic \n        # auth on the monitoring port.\n        username: ""\nolap_database:\n    # olap_database.cluster_name (string): The cluster name of the database\n    cluster_name: \'{cluster}\'\n    # olap_database.enable_data_replication (bool): If true, data replication \n    # is enabled.\n    enable_data_replication: false\n    # olap_database.replica_name (string): The replica name of the table in \n    # zookeeper\n    replica_name: \'{replica}\'\n    # olap_database.zoo_path (string): The path to the table name in \n    # zookeeper, used to set up data replication\n    zoo_path: /clickhouse/{installation}/{cluster}/tables/{shard}/{database}/{table}\nremote_execution:\n    # remote_execution.enable_executor_key_creation (bool): If enabled, UI \n    # will allow executor keys to be created.\n    enable_executor_key_creation: false\n    # remote_execution.enable_remote_exec (bool): If true, enable remote-exec. \n    # ** Enterprise only **\n    enable_remote_exec: true\n    # remote_execution.enable_user_owned_executors (bool): If enabled, users \n    # can register their own executors with the scheduler.\n    enable_user_owned_executors: false\n    # remote_execution.enable_workflows (bool): Whether to enable BuildBuddy \n    # workflows.\n    enable_workflows: false\n    # remote_execution.force_user_owned_darwin_executors (bool): If enabled, \n    # darwin actions will always run on user-owned executors.\n    force_user_owned_darwin_executors: false\nssl:\n    # ssl.cert_file (string): Path to a PEM encoded certificate file to use \n    # for TLS if not using ACME.\n    cert_file: ""\n    # ssl.client_ca_cert (string): PEM encoded certificate authority used to \n    # issue client certificates for mTLS auth.\n    client_ca_cert: ""\n    # ssl.client_ca_cert_file (string): Path to a PEM encoded certificate \n    # authority file used to issue client certificates for mTLS auth.\n    client_ca_cert_file: ""\n    # ssl.client_ca_key (string): PEM encoded certificate authority key used \n    # to issue client certificates for mTLS auth.\n    client_ca_key: ""\n    # ssl.client_ca_key_file (string): Path to a PEM encoded certificate \n    # authority key file used to issue client certificates for mTLS auth.\n    client_ca_key_file: ""\n    # ssl.client_cert_lifespan (time.Duration): The duration client \n    # certificates are valid for. Ex: \'730h\' for one month. If not set, \n    # defaults to 100 years.\n    client_cert_lifespan: 876000h0m0s\n    # ssl.default_host (string): Host name to use for ACME generated cert if \n    # TLS request does not contain SNI.\n    default_host: ""\n    # ssl.enable_ssl (bool): Whether or not to enable SSL/TLS on gRPC \n    # connections (gRPCS).\n    enable_ssl: false\n    # ssl.host_whitelist ([]string): Cloud-Only\n    host_whitelist: []\n    # ssl.key_file (string): Path to a PEM encoded key file to use for TLS if \n    # not using ACME.\n    key_file: ""\n    # ssl.self_signed (bool): If true, a self-signed cert will be generated \n    # for TLS termination.\n    self_signed: false\n    # ssl.upgrade_insecure (bool): True if http requests should be redirected \n    # to https. Assumes http traffic is served on port 80 and https traffic is \n    # served on port 443 (typically via an ingress / load balancer).\n    upgrade_insecure: false\n    # ssl.use_acme (bool): Whether or not to automatically configure SSL certs \n    # using ACME. If ACME is enabled, cert_file and key_file should not be \n    # set.\n    use_acme: false\nstorage:\n    aws_s3:\n        # storage.aws_s3.bucket (string): The AWS S3 bucket to store files in.\n        bucket: ""\n        # storage.aws_s3.credentials_profile (string): A custom credentials \n        # profile to use.\n        credentials_profile: ""\n        # storage.aws_s3.disable_ssl (bool): Disables the use of SSL, useful \n        # for configuring the use of MinIO. **DEPRECATED** Specify a non-HTTPS \n        # endpoint instead.\n        disable_ssl: false\n        # storage.aws_s3.endpoint (string): The AWS endpoint to use, useful \n        # for configuring the use of MinIO.\n        endpoint: ""\n        # storage.aws_s3.region (string): The AWS region.\n        region: ""\n        # storage.aws_s3.role_arn (string): The role ARN to use for web \n        # identity auth.\n        role_arn: ""\n        # storage.aws_s3.role_session_name (string): The role session name to \n        # use for web identity auth.\n        role_session_name: ""\n        # storage.aws_s3.s3_force_path_style (bool): Force path style urls for \n        # objects, useful for configuring the use of MinIO.\n        s3_force_path_style: false\n        # storage.aws_s3.static_credentials_id (string): Static credentials ID \n        # to use, useful for configuring the use of MinIO.\n        static_credentials_id: ""\n        # storage.aws_s3.static_credentials_secret (string): Static \n        # credentials secret to use, useful for configuring the use of MinIO.\n        static_credentials_secret: ""\n        # storage.aws_s3.static_credentials_token (string): Static credentials \n        # token to use, useful for configuring the use of MinIO.\n        static_credentials_token: ""\n        # storage.aws_s3.web_identity_token_file (string): The file path to \n        # the web identity token file.\n        web_identity_token_file: ""\n    azure:\n        # storage.azure.account_key (string): The key for the Azure storage \n        # account\n        account_key: ""\n        # storage.azure.account_name (string): The name of the Azure storage \n        # account\n        account_name: ""\n        # storage.azure.container_name (string): The name of the Azure storage \n        # container\n        container_name: ""\n    # storage.chunk_file_size_bytes (int): How many bytes to buffer in memory \n    # before flushing a chunk of build protocol data to disk.\n    chunk_file_size_bytes: 3000000\n    # storage.cleanup_batch_size (int): How many invocations to delete in each \n    # janitor cleanup task\n    cleanup_batch_size: 10\n    # storage.disable_persist_cache_artifacts (bool): If disabled, buildbuddy \n    # will not persist cache artifacts in the blobstore. This may make older \n    # invocations not diaplay properly.\n    disable_persist_cache_artifacts: false\n    disk:\n        # storage.disk.root_directory (string): The root directory to store \n        # all blobs in, if using disk based storage.\n        root_directory: /tmp/buildbuddy\n        # storage.disk.use_v2_layout (bool): If enabled, files will be stored \n        # using the v2 layout. See disk_cache.MigrateToV2Layout for a \n        # description.\n        use_v2_layout: false\n    # storage.enable_chunked_event_logs (bool): If true, Event logs will be \n    # stored separately from the invocation proto in chunks.\n    enable_chunked_event_logs: false\n    execution:\n        # storage.execution.cleanup_batch_size (int): How many invocations to \n        # delete in each janitor cleanup task\n        cleanup_batch_size: 200\n        # storage.execution.cleanup_interval (time.Duration): How often the \n        # janitor cleanup tasks will run\n        cleanup_interval: 5m0s\n        # storage.execution.cleanup_workers (int): How many cleanup tasks to \n        # run\n        cleanup_workers: 1\n        # storage.execution.ttl (time.Duration): The time, in seconds, to keep \n        # invocations before deletion. 0 disables invocation deletion.\n        ttl: 0s\n    gcs:\n        # storage.gcs.bucket (string): The name of the GCS bucket to store \n        # build artifact files in.\n        bucket: ""\n        # storage.gcs.credentials (string): Credentials in JSON format that \n        # will be used to authenticate to GCS.\n        credentials: ""\n        # storage.gcs.credentials_file (string): A path to a JSON credentials \n        # file that will be used to authenticate to GCS.\n        credentials_file: ""\n        # storage.gcs.project_id (string): The Google Cloud project ID of the \n        # project owning the above credentials and GCS bucket.\n        project_id: ""\n    # storage.path_prefix (string): The prefix directory to store all blobs in\n    path_prefix: ""\n    # storage.tempdir (string): Root directory for temporary files. Defaults \n    # to the OS-specific temp dir.\n    tempdir: /tmp\n    # storage.ttl_seconds (int): The time, in seconds, to keep invocations \n    # before deletion. 0 disables invocation deletion.\n    ttl_seconds: 0\n')))}c.isMDXComponent=!0;var d=["components"],_={toc:[]},u="wrapper";function p(e){var t=e.components,n=(0,o.Z)(e,d);return(0,i.kt)(u,(0,a.Z)({},_,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-yaml"},'# Unstructured settings\n\n# app_directory (string): the directory containing app binary files to host\napp_directory: ""\n# auto_migrate_db (bool): If true, attempt to automigrate the db when \n# connecting\nauto_migrate_db: true\n# auto_migrate_db_and_exit (bool): If true, attempt to automigrate the db when \n# connecting, then exit the program.\nauto_migrate_db_and_exit: false\n# cache_stats_finalization_delay (time.Duration): The time allowed for all \n# metrics collectors across all apps to flush their local cache stats to the \n# backing storage, before finalizing stats in the DB.\ncache_stats_finalization_delay: 500ms\n# cleanup_interval (time.Duration): How often the janitor cleanup tasks will \n# run\ncleanup_interval: 10m0s\n# cleanup_workers (int): How many cleanup tasks to run\ncleanup_workers: 1\n# disable_ga (bool): If true; ga will be disabled\ndisable_ga: false\n# disable_telemetry (bool): If true; telemetry will be disabled\ndisable_telemetry: false\n# drop_invocation_pk_cols (bool): If true, attempt to drop invocation PK cols\ndrop_invocation_pk_cols: false\n# enable_cache_delete_api (bool): If true, enable access to cache delete API.\nenable_cache_delete_api: false\n# exit_when_ready (bool): If set, the app will exit as soon as it becomes \n# ready (useful for migrations)\nexit_when_ready: false\n# grpc_client_origin_header (string): Header value to set for \n# x-buildbuddy-origin.\ngrpc_client_origin_header: ""\n# grpc_port (int): The port to listen for gRPC traffic on\ngrpc_port: 1985\n# grpcs_port (int): The port to listen for gRPCS traffic on\ngrpcs_port: 1986\n# internal_grpc_port (int): The port to listen for internal gRPC traffic on\ninternal_grpc_port: 1987\n# internal_grpcs_port (int): The port to listen for internal gRPCS traffic on\ninternal_grpcs_port: 1988\n# internal_http_port (int): The port to listen for internal HTTP traffic\ninternal_http_port: 0\n# js_entry_point_path (string): Absolute URL path of the app JS entry point\njs_entry_point_path: /app/app_bundle/app.js?hash={APP_BUNDLE_HASH}\n# listen (string): The interface to listen on (default: 0.0.0.0)\nlisten: 0.0.0.0\n# log_deletion_errors (bool): If true; log errors when ttl-deleting expired \n# data\nlog_deletion_errors: false\n# log_goroutine_profile_on_shutdown (bool): Whether to log all goroutine stack \n# traces on shutdown.\nlog_goroutine_profile_on_shutdown: false\n# max_shutdown_duration (time.Duration): Time to wait for shutdown\nmax_shutdown_duration: 25s\n# migrate_disk_cache_to_v2_and_exit (bool): If true, attempt to migrate disk \n# cache to v2 layout.\nmigrate_disk_cache_to_v2_and_exit: false\n# monitoring_port (int): The port to listen for monitoring traffic on\nmonitoring_port: 9090\n# port (int): The port to listen for HTTP traffic on\nport: 8080\n# redis_command_buffer_flush_period (time.Duration): How long to wait between \n# flushing buffered redis commands. Setting this to 0 will disable buffering \n# at the cost of higher redis QPS.\nredis_command_buffer_flush_period: 250ms\n# report_not_ready (bool): If set to true, the app will always report as being \n# unready.\nreport_not_ready: false\n# server_type (string): The server type to match on health checks\nserver_type: buildbuddy-server\n# shutdown_lameduck_duration (time.Duration): If set, the server will be \n# marked unready but not run shutdown functions until this period passes.\nshutdown_lameduck_duration: 0s\n# ssl_port (int): The port to listen for HTTPS traffic on\nssl_port: 8081\n# static_directory (string): the directory containing static files to host\nstatic_directory: ""\n# telemetry_endpoint (string): The telemetry endpoint to use\ntelemetry_endpoint: grpcs://t.buildbuddy.io:443\n# telemetry_interval (time.Duration): How often telemetry data will be \n# reported\ntelemetry_interval: 24h0m0s\n# telemetry_port (int): The port on which to listen for telemetry events\ntelemetry_port: 9099\n# verbose_telemetry_client (bool): If true; print telemetry client information\nverbose_telemetry_client: false\n# verbose_telemetry_server (bool): If true; print telemetry server information\nverbose_telemetry_server: false\n# zone_override (string): A value that will override the auto-detected zone. \n# Ignored if empty\nzone_override: ""\n\n# Structured settings\n\napi:\n    # api.api_key (string): The default API key to use for on-prem enterprise \n    # deploys with a single organization/group. **DEPRECATED** Manual API key \n    # specification is no longer supported; to retrieve specific API keys \n    # programmatically, please use the API key table. This field will still \n    # specify an API key to redact in case a manual API key was specified when \n    # buildbuddy was first set up.\n    api_key: ""\n    # api.enable_api (bool): Whether or not to enable the BuildBuddy API.\n    enable_api: true\n    # api.enable_cache (bool): Whether or not to enable the API cache.\n    enable_cache: false\n    # api.enable_metrics_api (bool): If true, enable access to metrics API.\n    enable_metrics_api: false\napp:\n    # app.add_user_to_domain_group (bool): Cloud-Only\n    add_user_to_domain_group: false\n    # app.admin_only_create_group (bool): If true, only admins of an existing \n    # group can create a new groups.\n    admin_only_create_group: false\n    # app.audit_logs_enabled (bool): Whether to log administrative events to \n    # an audit log. Requires OLAP database to be configured.\n    audit_logs_enabled: false\n    # app.audit_logs_ui_enabled (bool): If set, the audit logs UI will be \n    # accessible from the sidebar.\n    audit_logs_ui_enabled: false\n    # app.build_buddy_url (URL): The external URL where your BuildBuddy \n    # instance can be found.\n    build_buddy_url: http://localhost:8080\n    # app.cache_api_url (URL): Overrides the default remote cache protocol \n    # gRPC address shown by BuildBuddy on the configuration screen.\n    cache_api_url: ""\n    # app.code_editor_enabled (bool): If set, code editor functionality will \n    # be enabled.\n    code_editor_enabled: false\n    # app.create_group_per_user (bool): Cloud-Only\n    create_group_per_user: false\n    # app.customer_managed_encryption_keys_enabled (bool): If set, show \n    # customer-managed encryption configuration UI.\n    customer_managed_encryption_keys_enabled: false\n    # app.default_redis_target (string): A Redis target for storing remote \n    # shared state. To ease migration, the redis target from the remote \n    # execution config will be used if this value is not specified.\n    default_redis_target: ""\n    default_sharded_redis:\n        # app.default_sharded_redis.password (string): Redis password\n        password: ""\n        # app.default_sharded_redis.shards ([]string): Ordered list of Redis \n        # shard addresses.\n        shards: []\n        # app.default_sharded_redis.username (string): Redis username\n        username: ""\n    # app.default_subdomains ([]string): List of subdomains that should not be \n    # handled as user-owned subdomains.\n    default_subdomains: []\n    # app.default_to_dense_mode (bool): Enables the dense UI mode by default.\n    default_to_dense_mode: false\n    # app.disable_cert_config (bool): If true, the certificate based auth \n    # option will not be shown in the config widget.\n    disable_cert_config: false\n    # app.enable_execution_trends (bool): If enabled, fill execution trend \n    # stats in GetTrendResponse\n    enable_execution_trends: true\n    # app.enable_grpc_metrics_by_group_id (bool): If enabled, grpc metrics by \n    # group ID will be recorded\n    enable_grpc_metrics_by_group_id: false\n    # app.enable_invocation_stat_percentiles (bool): If enabled, provide \n    # percentile breakdowns for invocation stats in GetTrendResponse\n    enable_invocation_stat_percentiles: true\n    # app.enable_prometheus_histograms (bool): If true, collect prometheus \n    # histograms for all RPCs\n    enable_prometheus_histograms: true\n    # app.enable_quota_management (bool): If set, quota management will be \n    # enabled\n    enable_quota_management: false\n    # app.enable_read_from_olap_db (bool): If enabled, read from OLAP DB\n    enable_read_from_olap_db: true\n    # app.enable_read_target_statuses_from_olap_db (bool): If enabled, read \n    # target statuses from OLAP DB\n    enable_read_target_statuses_from_olap_db: false\n    # app.enable_secret_service (bool): If set, secret service will be enabled\n    enable_secret_service: false\n    # app.enable_structured_logging (bool): If true, log messages will be \n    # json-formatted.\n    enable_structured_logging: false\n    # app.enable_subdomain_matching (bool): If true, request subdomain will be \n    # taken into account when determining what request restrictions should be \n    # applied.\n    enable_subdomain_matching: false\n    # app.enable_target_tracking (bool): Cloud-Only\n    enable_target_tracking: false\n    # app.enable_write_executions_to_olap_db (bool): If enabled, complete \n    # Executions will be flushed to OLAP DB\n    enable_write_executions_to_olap_db: false\n    # app.enable_write_test_target_statuses_to_olap_db (bool): If enabled, \n    # test target statuses will be flushed to OLAP DB\n    enable_write_test_target_statuses_to_olap_db: false\n    # app.enable_write_to_olap_db (bool): If enabled, complete invocations \n    # will be flushed to OLAP DB\n    enable_write_to_olap_db: true\n    # app.events_api_url (URL): Overrides the default build event protocol \n    # gRPC address shown by BuildBuddy on the configuration screen.\n    events_api_url: ""\n    # app.execution_search_enabled (bool): If set, fetch lists of executions \n    # from the OLAP DB in the trends UI.\n    execution_search_enabled: true\n    # app.expanded_suggestions_enabled (bool): If set, enable more build \n    # suggestions in the UI.\n    expanded_suggestions_enabled: false\n    # app.fetch_tags_drilldown_data (bool): If enabled, \n    # DrilldownType_TAG_DRILLDOWN_TYPE can be returned in \n    # GetStatDrilldownRequests\n    fetch_tags_drilldown_data: true\n    # app.grpc_max_recv_msg_size_bytes (int): Configures the max GRPC receive \n    # message size [bytes]\n    grpc_max_recv_msg_size_bytes: 50000000\n    # app.grpc_over_http_port_enabled (bool): Cloud-Only\n    grpc_over_http_port_enabled: false\n    # app.ignore_forced_tracing_header (bool): If set, we will not honor the \n    # forced tracing header.\n    ignore_forced_tracing_header: false\n    # app.invocation_summary_available_usec (int64): The timstamp when the \n    # invocation summary is available in the DB\n    invocation_summary_available_usec: 0\n    # app.log_enable_gcp_logging_format (bool): If true, the output structured \n    # logs will be compatible with format expected by GCP Logging.\n    log_enable_gcp_logging_format: false\n    # app.log_error_stack_traces (bool): If true, stack traces will be printed \n    # for errors that have them.\n    log_error_stack_traces: false\n    # app.log_gcp_log_id (string): The log ID to log to in GCP (if any).\n    log_gcp_log_id: ""\n    # app.log_gcp_project_id (string): The project ID to log to in GCP (if \n    # any).\n    log_gcp_project_id: ""\n    # app.log_include_short_file_name (bool): If true, log messages will \n    # include shortened originating file name.\n    log_include_short_file_name: false\n    # app.log_level (string): The desired log level. Logs with a level >= this \n    # level will be emitted. One of {\'fatal\', \'error\', \'warn\', \'info\', \n    # \'debug\'}\n    log_level: info\n    # app.no_default_user_group (bool): Cloud-Only\n    no_default_user_group: false\n    # app.olap_invocation_search_enabled (bool): If true, \n    # InvocationSearchService will query clickhouse for some queries.\n    olap_invocation_search_enabled: true\n    # app.paginate_invocations (bool): If true, paginate invocations returned \n    # to the UI.\n    paginate_invocations: false\n    # app.pattern_filter_enabled (bool): If set, allow filtering by pattern in \n    # the client.\n    pattern_filter_enabled: true\n    # app.region (string): The region in which the app is running.\n    region: ""\n    # app.remote_execution_api_url (URL): Overrides the default remote \n    # execution protocol gRPC address shown by BuildBuddy on the configuration \n    # screen.\n    remote_execution_api_url: ""\n    # app.tags_enabled (bool): Enable setting tags on invocations via \n    # build_metadata\n    tags_enabled: false\n    # app.tags_ui_enabled (bool): If set, expose tags data and let users \n    # filter by tag.\n    tags_ui_enabled: false\n    # app.test_grid_v2_enabled (bool): Whether to enable test grid V2\n    test_grid_v2_enabled: true\n    # app.test_output_manifests_enabled (bool): If set, the target page will \n    # render the contents of test output zips.\n    test_output_manifests_enabled: true\n    # app.timeseries_charts_in_timing_profile_enabled (bool): If set, charts \n    # with sampled time series data (such as CPU and memory usage) will be \n    # shown\n    timeseries_charts_in_timing_profile_enabled: true\n    # app.trace_fraction (float64): Fraction of requests to sample for \n    # tracing.\n    trace_fraction: 0\n    # app.trace_fraction_overrides ([]string): Tracing fraction override based \n    # on name in format name=fraction.\n    trace_fraction_overrides: []\n    # app.trace_jaeger_collector (string): Address of the Jager collector \n    # endpoint where traces will be sent.\n    trace_jaeger_collector: ""\n    # app.trace_project_id (string): Optional GCP project ID to export traces \n    # to. If not specified, determined from default credentials or metadata \n    # server if running on GCP.\n    trace_project_id: ""\n    # app.trace_service_name (string): Name of the service to associate with \n    # traces.\n    trace_service_name: ""\n    # app.trends_heatmap_enabled (bool): If set, enable a fancy heatmap UI for \n    # exploring build trends.\n    trends_heatmap_enabled: true\n    # app.trends_summary_enabled (bool): If set, show the new \'summary\' \n    # section at the top of the trends UI.\n    trends_summary_enabled: false\n    # app.usage_enabled (bool): If set, the usage page will be enabled in the \n    # UI.\n    usage_enabled: false\n    # app.usage_start_date (string): If set, usage data will only be viewable \n    # on or after this timestamp. Specified in RFC3339 format, like \n    # 2021-10-01T00:00:00Z\n    usage_start_date: ""\n    # app.usage_tracking_enabled (bool): If set, enable usage data collection.\n    usage_tracking_enabled: false\n    # app.use_timezone_in_heatmap_queries (bool): If enabled, use timezone \n    # instead of \'timezone offset\' to compute day boundaries in heatmap \n    # queries.\n    use_timezone_in_heatmap_queries: true\n    # app.user_management_enabled (bool): If set, the user management page \n    # will be enabled in the UI.\n    user_management_enabled: true\n    # app.user_owned_keys_enabled (bool): If true, enable user-owned API keys.\n    user_owned_keys_enabled: false\n    # app.workflow_history_enabled (bool): If set, information about past \n    # action runs for workflows will be shown on the workflows page.\n    workflow_history_enabled: false\nauth:\n    # auth.admin_group_id (string): ID of a group whose members can perform \n    # actions only accessible to server admins.\n    admin_group_id: ""\n    api_key_encryption:\n        # auth.api_key_encryption.encrypt_new_keys (bool): If enabled, all new \n        # API keys will be written in an encrypted format.\n        encrypt_new_keys: false\n        # auth.api_key_encryption.encrypt_old_keys (bool): If enabled, all \n        # existing unencrypted keys will be encrypted on startup. The \n        # unencrypted keys will remain in the database and will need to be \n        # cleared manually after verifying the success of the migration.\n        encrypt_old_keys: false\n        # auth.api_key_encryption.key (string): Base64-encoded 256-bit \n        # encryption key for API keys.\n        key: ""\n    # auth.api_key_group_cache_ttl (time.Duration): TTL for API Key to Group \n    # caching. Set to \'0\' to disable cache.\n    api_key_group_cache_ttl: 5m0s\n    # auth.disable_refresh_token (bool): If true, the offline_access scope \n    # which requests refresh tokens will not be requested.\n    disable_refresh_token: false\n    # auth.domain_wide_cookies (bool): If true, cookies will have domain set \n    # so that they are accessible on domain and all subdomains.\n    domain_wide_cookies: false\n    # auth.enable_anonymous_usage (bool): If true, unauthenticated build \n    # uploads will still be allowed but won\'t be associated with your \n    # organization.\n    enable_anonymous_usage: false\n    # auth.enable_self_auth (bool): If true, enables a single user login via \n    # an oauth provider on the buildbuddy server. Recommend use only when \n    # server is behind a firewall; this option may allow anyone with access to \n    # the webpage admin rights to your buildbuddy installation. ** Enterprise \n    # only **\n    enable_self_auth: false\n    # auth.force_approval (bool): If true, when a user doesn\'t have a session \n    # (first time logging in, or manually logged out) force the auth provider \n    # to show the consent screen allowing the user to select an account if \n    # they have multiple. This isn\'t supported by all auth providers.\n    force_approval: false\n    # auth.https_only_cookies (bool): If true, cookies will only be set over \n    # https connections.\n    https_only_cookies: false\n    # auth.jwt_claims_cache_ttl (time.Duration): TTL for JWT string to parsed \n    # claims caching. Set to \'0\' to disable cache.\n    jwt_claims_cache_ttl: 15s\n    # auth.jwt_key (string): The key to use when signing JWT tokens.\n    jwt_key: set_the_jwt_in_config\n    # auth.oauth_providers ([]oidc.OauthProvider): The list of oauth providers \n    # to use to authenticate.\n    oauth_providers: []\n    # For example:\n    # - issuer_url: "" # The issuer URL of this OIDC Provider. (type: string)\n    #   client_id: "" # The oauth client ID. (type: string)\n    #   client_secret: "" # The oauth client secret. (type: string)\n    #   slug: "" # The slug of this OIDC Provider. (type: string)\n\n    saml:\n        # auth.saml.cert (string): PEM encoded certificate used for SAML auth.\n        cert: ""\n        # auth.saml.cert_file (string): Path to a PEM encoded certificate file \n        # used for SAML auth.\n        cert_file: ""\n        # auth.saml.key (string): PEM encoded certificate key used for SAML \n        # auth.\n        key: ""\n        # auth.saml.key_file (string): Path to a PEM encoded certificate key \n        # file used for SAML auth.\n        key_file: ""\nbuild_event_proxy:\n    # build_event_proxy.buffer_size (int): The number of build events to \n    # buffer locally when proxying build events.\n    buffer_size: 100\n    # build_event_proxy.hosts ([]string): The list of hosts to pass build \n    # events onto.\n    hosts: []\ncache:\n    client:\n        # cache.client.enable_upload_compression (bool): If true, enable \n        # compression of uploads to remote caches\n        enable_upload_compression: true\n    # cache.detailed_stats_enabled (bool): Whether to enable detailed stats \n    # recording for all cache requests.\n    detailed_stats_enabled: false\n    disk:\n        # cache.disk.partition_mappings ([]disk.PartitionMapping)\n        partition_mappings: []\n        # For example:\n        # - group_id: "" # The Group ID to which this mapping applies. (type: string)\n        #   prefix: "" # The remote instance name prefix used to select this partition. (type: string)\n        #   partition_id: "" # The partition to use if the Group ID and prefix match. (type: string)\n\n        # cache.disk.partitions ([]disk.Partition)\n        partitions: []\n        # For example:\n        # - id: "" # The ID of the partition. (type: string)\n        #   max_size_bytes: 0 # Maximum size of the partition. (type: int64)\n        #   encryption_supported: false # Whether encrypted data can be stored on this partition. (type: bool)\n\n        # cache.disk.root_directory (string): The root directory to store all \n        # blobs in, if using disk based storage.\n        root_directory: ""\n        # cache.disk.use_v2_layout (bool): If enabled, files will be stored \n        # using the v2 layout. See disk_cache.MigrateToV2Layout for a \n        # description.\n        use_v2_layout: false\n    distributed_cache:\n        # cache.distributed_cache.cluster_size (int): The total number of \n        # nodes in this cluster. Required for health checking. ** Enterprise \n        # only **\n        cluster_size: 0\n        # cache.distributed_cache.enable_local_compression_lookup (bool): If \n        # enabled, checks the local cache for compression support. If not set, \n        # distributed compression defaults to off.\n        enable_local_compression_lookup: true\n        # cache.distributed_cache.enable_local_writes (bool): If enabled, \n        # shortcuts distributed writes that belong to the local shard to local \n        # cache instead of making an RPC.\n        enable_local_writes: false\n        # cache.distributed_cache.extra_nodes ([]string): The hardcoded list \n        # of extra nodes to add data too. Useful for migrations. ** Enterprise \n        # only **\n        extra_nodes: []\n        # cache.distributed_cache.group_name (string): A unique name for this \n        # distributed cache group. ** Enterprise only **\n        group_name: ""\n        # cache.distributed_cache.listen_addr (string): The address to listen \n        # for local BuildBuddy distributed cache traffic on.\n        listen_addr: ""\n        # cache.distributed_cache.nodes ([]string): The hardcoded list of peer \n        # distributed cache nodes. If this is set, redis_target will be \n        # ignored. ** Enterprise only **\n        nodes: []\n        # cache.distributed_cache.redis_target (string): A redis target for \n        # improved Caching/RBE performance. Target can be provided as either a \n        # redis connection URI or a host:port pair. URI schemas supported: \n        # redis[s]://[[USER][:PASSWORD]@][HOST][:PORT][/DATABASE] or \n        # unix://[[USER][:PASSWORD]@]SOCKET_PATH[?db=DATABASE] ** Enterprise \n        # only **\n        redis_target: ""\n        # cache.distributed_cache.replication_factor (int): How many total \n        # servers the data should be replicated to. Must be >= 1. ** \n        # Enterprise only **\n        replication_factor: 0\n    # cache.enable_tree_caching (bool): If true, cache GetTree responses (full \n    # and partial)\n    enable_tree_caching: true\n    gcs:\n        # cache.gcs.bucket (string): The name of the GCS bucket to store cache \n        # files in.\n        bucket: ""\n        # cache.gcs.credentials_file (string): A path to a JSON credentials \n        # file that will be used to authenticate to GCS.\n        credentials_file: ""\n        # cache.gcs.project_id (string): The Google Cloud project ID of the \n        # project owning the above credentials and GCS bucket.\n        project_id: ""\n        # cache.gcs.ttl_days (int64): The period after which cache files \n        # should be TTLd. Disabled if 0.\n        ttl_days: 0\n    # cache.in_memory (bool): Whether or not to use the in_memory cache.\n    in_memory: false\n    # cache.max_size_bytes (int64): How big to allow the cache to be (in \n    # bytes).\n    max_size_bytes: 10000000000\n    # cache.max_tree_cache_set_duration (time.Duration): The max amount of \n    # time to wait for unfinished tree cache entries to be set.\n    max_tree_cache_set_duration: 1s\n    # cache.memcache_targets ([]string): Deprecated. Use Redis Target instead.\n    memcache_targets: []\n    # cache.migration (migration_cache.MigrationConfig): Config to specify the \n    # details of a cache migration\n    migration:\n        src: null # (type: migration_cache.CacheConfig)\n        # For example:\n        #     disk: null # (type: migration_cache.DiskCacheConfig)\n        #     # For example:\n        #     #     root_directory: "" # (type: string)\n        #     #     partitions: [] # (type: []disk.Partition)\n        #     #     # For example:\n        #     #     # - id: "" # The ID of the partition. (type: string)\n        #     #     #   max_size_bytes: 0 # Maximum size of the partition. (type: int64)\n        #     #     #   encryption_supported: false # Whether encrypted data can be stored on this partition. (type: bool)\n        #     #     \n        #     #     partition_mappings: [] # (type: []disk.PartitionMapping)\n        #     #     # For example:\n        #     #     # - group_id: "" # The Group ID to which this mapping applies. (type: string)\n        #     #     #   prefix: "" # The remote instance name prefix used to select this partition. (type: string)\n        #     #     #   partition_id: "" # The partition to use if the Group ID and prefix match. (type: string)\n        #     #     \n        #     #     use_v2_layout: false # (type: bool)\n        #     #     \n        #     \n        #     pebble: null # (type: migration_cache.PebbleCacheConfig)\n        #     # For example:\n        #     #     name: "" # (type: string)\n        #     #     root_directory: "" # (type: string)\n        #     #     partitions: [] # (type: []disk.Partition)\n        #     #     # For example:\n        #     #     # - id: "" # The ID of the partition. (type: string)\n        #     #     #   max_size_bytes: 0 # Maximum size of the partition. (type: int64)\n        #     #     #   encryption_supported: false # Whether encrypted data can be stored on this partition. (type: bool)\n        #     #     \n        #     #     partition_mappings: [] # (type: []disk.PartitionMapping)\n        #     #     # For example:\n        #     #     # - group_id: "" # The Group ID to which this mapping applies. (type: string)\n        #     #     #   prefix: "" # The remote instance name prefix used to select this partition. (type: string)\n        #     #     #   partition_id: "" # The partition to use if the Group ID and prefix match. (type: string)\n        #     #     \n        #     #     max_size_bytes: 0 # (type: int64)\n        #     #     block_cache_size_bytes: 0 # (type: int64)\n        #     #     max_inline_file_size_bytes: 0 # (type: int64)\n        #     #     atime_update_threshold: null # (type: time.Duration)\n        #     #     atime_buffer_size: null # (type: int)\n        #     #     min_eviction_age: null # (type: time.Duration)\n        #     #     min_bytes_auto_zstd_compression: 0 # (type: int64)\n        #     #     average_chunk_size_bytes: 0 # (type: int)\n        #     #     \n        #     \n\n        dest: null # (type: migration_cache.CacheConfig)\n        # For example:\n        #     disk: null # (type: migration_cache.DiskCacheConfig)\n        #     # For example:\n        #     #     root_directory: "" # (type: string)\n        #     #     partitions: [] # (type: []disk.Partition)\n        #     #     # For example:\n        #     #     # - id: "" # The ID of the partition. (type: string)\n        #     #     #   max_size_bytes: 0 # Maximum size of the partition. (type: int64)\n        #     #     #   encryption_supported: false # Whether encrypted data can be stored on this partition. (type: bool)\n        #     #     \n        #     #     partition_mappings: [] # (type: []disk.PartitionMapping)\n        #     #     # For example:\n        #     #     # - group_id: "" # The Group ID to which this mapping applies. (type: string)\n        #     #     #   prefix: "" # The remote instance name prefix used to select this partition. (type: string)\n        #     #     #   partition_id: "" # The partition to use if the Group ID and prefix match. (type: string)\n        #     #     \n        #     #     use_v2_layout: false # (type: bool)\n        #     #     \n        #     \n        #     pebble: null # (type: migration_cache.PebbleCacheConfig)\n        #     # For example:\n        #     #     name: "" # (type: string)\n        #     #     root_directory: "" # (type: string)\n        #     #     partitions: [] # (type: []disk.Partition)\n        #     #     # For example:\n        #     #     # - id: "" # The ID of the partition. (type: string)\n        #     #     #   max_size_bytes: 0 # Maximum size of the partition. (type: int64)\n        #     #     #   encryption_supported: false # Whether encrypted data can be stored on this partition. (type: bool)\n        #     #     \n        #     #     partition_mappings: [] # (type: []disk.PartitionMapping)\n        #     #     # For example:\n        #     #     # - group_id: "" # The Group ID to which this mapping applies. (type: string)\n        #     #     #   prefix: "" # The remote instance name prefix used to select this partition. (type: string)\n        #     #     #   partition_id: "" # The partition to use if the Group ID and prefix match. (type: string)\n        #     #     \n        #     #     max_size_bytes: 0 # (type: int64)\n        #     #     block_cache_size_bytes: 0 # (type: int64)\n        #     #     max_inline_file_size_bytes: 0 # (type: int64)\n        #     #     atime_update_threshold: null # (type: time.Duration)\n        #     #     atime_buffer_size: null # (type: int)\n        #     #     min_eviction_age: null # (type: time.Duration)\n        #     #     min_bytes_auto_zstd_compression: 0 # (type: int64)\n        #     #     average_chunk_size_bytes: 0 # (type: int)\n        #     #     \n        #     \n\n        double_read_percentage: 0 # (type: float64)\n        log_not_found_errors: false # (type: bool)\n        copy_chan_buffer_size: 0 # (type: int)\n        copy_chan_full_warning_interval_min: 0 # (type: int64)\n        max_copies_per_sec: 0 # (type: int)\n    pebble:\n        # cache.pebble.ac_eviction_enabled (bool): Whether AC eviction is \n        # enabled.\n        ac_eviction_enabled: false\n        # cache.pebble.active_key_version (int64): The key version new data \n        # will be written with\n        active_key_version: 0\n        # cache.pebble.atime_buffer_size (int): Buffer up to this many atime \n        # updates in a channel before dropping atime updates\n        atime_buffer_size: 100000\n        # cache.pebble.atime_update_threshold (time.Duration): Don\'t update \n        # atime if it was updated more recently than this\n        atime_update_threshold: 10m0s\n        # cache.pebble.average_chunk_size_bytes (int): Average size of chunks \n        # that\'s stored in the cache. Disabled if 0.\n        average_chunk_size_bytes: 0\n        # cache.pebble.background_repair_frequency (time.Duration): How \n        # frequently to run period background repair tasks.\n        background_repair_frequency: 24h0m0s\n        # cache.pebble.background_repair_qps_limit (int): QPS limit for \n        # background repair modifications.\n        background_repair_qps_limit: 100\n        # cache.pebble.block_cache_size_bytes (int64): How much ram to give \n        # the block cache\n        block_cache_size_bytes: 1000000000\n        # cache.pebble.copy_partition_data (string): If set, all data will be \n        # copied from the source partition to the destination partition on \n        # startup. The cache will not serve data while the copy is in \n        # progress. Specified in format \n        # source_partition_id:destination_partition_id,\n        copy_partition_data: ""\n        # cache.pebble.delete_ac_entries_older_than (time.Duration): If set, \n        # the background repair will delete AC entries older than this time.\n        delete_ac_entries_older_than: 0s\n        # cache.pebble.deletes_per_eviction (int): Maximum number keys to \n        # delete in one eviction attempt before resampling.\n        deletes_per_eviction: 5\n        # cache.pebble.dir_deletion_delay (time.Duration): How old directories \n        # must be before being eligible for deletion when empty\n        dir_deletion_delay: 1h0m0s\n        # cache.pebble.eviction_rate_limit (int): Maximum number of entries to \n        # evict per second (per partition).\n        eviction_rate_limit: 300\n        # cache.pebble.force_calculate_metadata (bool): If set, partition size \n        # and counts will be calculated even if cached information is \n        # available.\n        force_calculate_metadata: false\n        # cache.pebble.force_compaction (bool): If set, compact the DB when \n        # it\'s created\n        force_compaction: false\n        # cache.pebble.groupid_sampling_enabled (bool): Whether AC entries are \n        # sampled and exported via metrics. Not yet used for eviction.\n        groupid_sampling_enabled: false\n        # cache.pebble.include_metadata_size (bool): If true, include metadata \n        # size\n        include_metadata_size: false\n        # cache.pebble.max_inline_file_size_bytes (int64): Files smaller than \n        # this may be inlined directly into pebble\n        max_inline_file_size_bytes: 1024\n        # cache.pebble.migration_qps_limit (int): QPS limit for data version \n        # migration\n        migration_qps_limit: 50\n        # cache.pebble.min_bytes_auto_zstd_compression (int64): Blobs larger \n        # than this will be zstd compressed before written to disk.\n        min_bytes_auto_zstd_compression: 0\n        # cache.pebble.min_eviction_age (time.Duration): Don\'t evict anything \n        # unless it\'s been idle for at least this long\n        min_eviction_age: 6h0m0s\n        # cache.pebble.name (string): The name used in reporting cache metrics \n        # and status.\n        name: pebble_cache\n        # cache.pebble.num_groupid_samples_on_startup (int): How many group \n        # IDs to sample to on startup.\n        num_groupid_samples_on_startup: 10\n        # cache.pebble.orphan_delete_dry_run (bool): If set, log orphaned \n        # files instead of deleting them\n        orphan_delete_dry_run: true\n        # cache.pebble.partition_mappings ([]disk.PartitionMapping)\n        partition_mappings: []\n        # For example:\n        # - group_id: "" # The Group ID to which this mapping applies. (type: string)\n        #   prefix: "" # The remote instance name prefix used to select this partition. (type: string)\n        #   partition_id: "" # The partition to use if the Group ID and prefix match. (type: string)\n\n        # cache.pebble.partitions ([]disk.Partition)\n        partitions: []\n        # For example:\n        # - id: "" # The ID of the partition. (type: string)\n        #   max_size_bytes: 0 # Maximum size of the partition. (type: int64)\n        #   encryption_supported: false # Whether encrypted data can be stored on this partition. (type: bool)\n\n        # cache.pebble.root_directory (string): The root directory to store \n        # the database in.\n        root_directory: ""\n        # cache.pebble.sample_pool_size (int): How many deletion candidates to \n        # maintain between evictions\n        sample_pool_size: 500\n        # cache.pebble.samples_per_eviction (int): How many records to sample \n        # on each eviction\n        samples_per_eviction: 20\n        # cache.pebble.samples_per_groupid (int): How many samples to use when \n        # approximating AC item count for groups.\n        samples_per_groupid: 20\n        # cache.pebble.scan_for_missing_files (bool): If set, scan all keys \n        # and check if external files are missing on disk. Deletes keys with \n        # missing files.\n        scan_for_missing_files: true\n        # cache.pebble.scan_for_orphaned_files (bool): If true, scan for \n        # orphaned files\n        scan_for_orphaned_files: false\n        # cache.pebble.warn_about_leaks (bool): If set, warn about leaked DB \n        # handles\n        warn_about_leaks: true\n    # cache.pebble_groupid_sample_frequency (time.Duration): How often to \n    # perform a new group ID / approximate count sampling.\n    pebble_groupid_sample_frequency: 5s\n    raft:\n        # cache.raft.atime_buffer_size (int): Buffer up to this many atime \n        # updates in a channel before dropping atime updates\n        atime_buffer_size: 1000\n        # cache.raft.atime_update_threshold (time.Duration): Don\'t update \n        # atime if it was updated more recently than this\n        atime_update_threshold: 10m0s\n        # cache.raft.atime_write_batch_size (int): Buffer this many writes \n        # before writing atime data\n        atime_write_batch_size: 100\n        # cache.raft.clear_cache_on_startup (bool): If set, remove all raft + \n        # cache data on start\n        clear_cache_on_startup: false\n        # cache.raft.dead_replica_timeout (time.Duration): After this time, \n        # consider a node dead\n        dead_replica_timeout: 5m0s\n        # cache.raft.driver_poll_interval (time.Duration): Poll the cluster \n        # for moves/replacements this often\n        driver_poll_interval: 10s\n        # cache.raft.driver_startup_delay (time.Duration): Don\'t allow driver \n        # to propose any changes until this window has passed\n        driver_startup_delay: 1m0s\n        # cache.raft.enable_moving_replicas (bool): If set, allow moving \n        # replicas between nodes\n        enable_moving_replicas: true\n        # cache.raft.enable_replacing_replicas (bool): If set, allow replacing \n        # dead / down replicas\n        enable_replacing_replicas: true\n        # cache.raft.enable_splitting_replicas (bool): If set, allow splitting \n        # oversize replicas\n        enable_splitting_replicas: true\n        # cache.raft.grpc_addr (string): The address to listen for internal \n        # API traffic on. Ex. \'1993\'\n        grpc_addr: ""\n        # cache.raft.http_addr (string): The address to listen for HTTP raft \n        # traffic. Ex. \'1992\'\n        http_addr: ""\n        # cache.raft.join ([]string): The list of nodes to use when joining \n        # clusters Ex. \'1.2.3.4:1991,2.3.4.5:1991...\'\n        join: []\n        # cache.raft.listen_addr (string): The address to listen for local \n        # gossip traffic on. Ex. \'localhost:1991\n        listen_addr: ""\n        # cache.raft.partition_mappings ([]disk.PartitionMapping)\n        partition_mappings: []\n        # For example:\n        # - group_id: "" # The Group ID to which this mapping applies. (type: string)\n        #   prefix: "" # The remote instance name prefix used to select this partition. (type: string)\n        #   partition_id: "" # The partition to use if the Group ID and prefix match. (type: string)\n\n        # cache.raft.partition_usage_delta_bytes_threshold (int): Gossip \n        # partition usage information if it has changed by more than this \n        # amount since the last gossip.\n        partition_usage_delta_bytes_threshold: 100000000\n        # cache.raft.partitions ([]disk.Partition)\n        partitions: []\n        # For example:\n        # - id: "" # The ID of the partition. (type: string)\n        #   max_size_bytes: 0 # Maximum size of the partition. (type: int64)\n        #   encryption_supported: false # Whether encrypted data can be stored on this partition. (type: bool)\n\n        # cache.raft.replica_split_size_bytes (int64): Split replicas after \n        # they reach this size\n        replica_split_size_bytes: 20000000\n        # cache.raft.root_directory (string): The root directory to use for \n        # storing cached data.\n        root_directory: ""\n        # cache.raft.sample_pool_size (int): How many deletion candidates to \n        # maintain between evictions\n        sample_pool_size: 500\n        # cache.raft.samples_per_eviction (int): How many records to sample on \n        # each eviction\n        samples_per_eviction: 20\n    redis:\n        # cache.redis.max_value_size_bytes (int64): The maximum value size to \n        # cache in redis (in bytes).\n        max_value_size_bytes: 10000000\n        # cache.redis.redis_target (string): A redis target for improved \n        # Caching/RBE performance. Target can be provided as either a redis \n        # connection URI or a host:port pair. URI schemas supported: \n        # redis[s]://[[USER][:PASSWORD]@][HOST][:PORT][/DATABASE] or \n        # unix://[[USER][:PASSWORD]@]SOCKET_PATH[?db=DATABASE] ** Enterprise \n        # only **\n        redis_target: ""\n        sharded:\n            # cache.redis.sharded.password (string): Redis password\n            password: ""\n            # cache.redis.sharded.shards ([]string): Ordered list of Redis \n            # shard addresses.\n            shards: []\n            # cache.redis.sharded.username (string): Redis username\n            username: ""\n    # cache.redis_target (string): A redis target for improved Caching/RBE \n    # performance. Target can be provided as either a redis connection URI or \n    # a host:port pair. URI schemas supported: \n    # redis[s]://[[USER][:PASSWORD]@][HOST][:PORT][/DATABASE] or \n    # unix://[[USER][:PASSWORD]@]SOCKET_PATH[?db=DATABASE] ** Enterprise only \n    # **\n    redis_target: ""\n    s3:\n        # cache.s3.bucket (string): The AWS S3 bucket to store files in.\n        bucket: ""\n        # cache.s3.credentials_profile (string): A custom credentials profile \n        # to use.\n        credentials_profile: ""\n        # cache.s3.disable_ssl (bool): Disables the use of SSL, useful for \n        # configuring the use of MinIO. **DEPRECATED** Specify a non-HTTPS \n        # endpoint instead.\n        disable_ssl: false\n        # cache.s3.endpoint (string): The AWS endpoint to use, useful for \n        # configuring the use of MinIO.\n        endpoint: ""\n        # cache.s3.path_prefix (string): Prefix inside the AWS S3 bucket to \n        # store files\n        path_prefix: ""\n        # cache.s3.region (string): The AWS region.\n        region: ""\n        # cache.s3.role_arn (string): The role ARN to use for web identity \n        # auth.\n        role_arn: ""\n        # cache.s3.role_session_name (string): The role session name to use \n        # for web identity auth.\n        role_session_name: ""\n        # cache.s3.s3_force_path_style (bool): Force path style urls for \n        # objects, useful for configuring the use of MinIO.\n        s3_force_path_style: false\n        # cache.s3.static_credentials_id (string): Static credentials ID to \n        # use, useful for configuring the use of MinIO.\n        static_credentials_id: ""\n        # cache.s3.static_credentials_secret (string): Static credentials \n        # secret to use, useful for configuring the use of MinIO.\n        static_credentials_secret: ""\n        # cache.s3.static_credentials_token (string): Static credentials token \n        # to use, useful for configuring the use of MinIO.\n        static_credentials_token: ""\n        # cache.s3.ttl_days (int): The period after which cache files should \n        # be TTLd. Disabled if 0.\n        ttl_days: 0\n        # cache.s3.web_identity_token_file (string): The file path to the web \n        # identity token file.\n        web_identity_token_file: ""\n    # cache.tree_cache_min_descendents (int): The min number of descendents a \n    # node must parent in order to be cached\n    tree_cache_min_descendents: 10\n    # cache.tree_cache_min_level (int): The min level at which the tree may be \n    # cached. 0 is the root\n    tree_cache_min_level: 1\n    # cache.tree_cache_seed (string): If set, hash this with digests before \n    # caching / reading from tree cache\n    tree_cache_seed: treecache-03011023\n    # cache.zstd_transcoding_enabled (bool): Whether to accept requests to \n    # read/write zstd-compressed blobs, compressing/decompressing \n    # outgoing/incoming blobs on the fly.\n    zstd_transcoding_enabled: true\ncrypter:\n    # crypter.key_reencrypt_interval (time.Duration): How frequently keys will \n    # be re-encrypted (to support key rotation).\n    key_reencrypt_interval: 6h0m0s\n    # crypter.key_ttl (time.Duration): The maximum amount of time a key can be \n    # cached without being re-verified before it is considered invalid.\n    key_ttl: 10m0s\ndatabase:\n    # database.advanced_data_source (db.AdvancedConfig): Alternative to the \n    # database.data_source flag that allows finer control over database \n    # settings as well as allowing use of AWS IAM credentials. For most users, \n    # database.data_source is a simpler configuration method.\n    advanced_data_source:\n        driver: "" # The driver to use: one of sqlite3, mysql, or postgresql. (type: string)\n        endpoint: "" # Typically the host:port combination of the database server. (type: string)\n        username: "" # Username to use when connecting. (type: string)\n        password: "" # Password to use when connecting. Not used if AWS IAM is enabled. (type: string)\n        db_name: "" # The name of the database to use for BuildBuddy data. (type: string)\n        region: "" # Region of the database instance. Required if AWS IAM is enabled. (type: string)\n        use_aws_iam: false # If enabled, AWS IAM authentication is used instead of fixed credentials. Make sure the endpoint includes the port, otherwise IAM-based auth will fail. (type: bool)\n        params: "" # Optional parameters to pass to the database driver (in format key1=val1&key2=val2) (type: string)\n    # database.advanced_read_replica (db.AdvancedConfig): Advanced alternative \n    # to database.read_replica. Refer to database.advanced for more \n    # information.\n    advanced_read_replica:\n        driver: "" # The driver to use: one of sqlite3, mysql, or postgresql. (type: string)\n        endpoint: "" # Typically the host:port combination of the database server. (type: string)\n        username: "" # Username to use when connecting. (type: string)\n        password: "" # Password to use when connecting. Not used if AWS IAM is enabled. (type: string)\n        db_name: "" # The name of the database to use for BuildBuddy data. (type: string)\n        region: "" # Region of the database instance. Required if AWS IAM is enabled. (type: string)\n        use_aws_iam: false # If enabled, AWS IAM authentication is used instead of fixed credentials. Make sure the endpoint includes the port, otherwise IAM-based auth will fail. (type: bool)\n        params: "" # Optional parameters to pass to the database driver (in format key1=val1&key2=val2) (type: string)\n    # database.conn_max_lifetime_seconds (int): The maximum lifetime of a \n    # connection to the db\n    conn_max_lifetime_seconds: 0\n    # database.data_source (string): The SQL database to connect to, specified \n    # as a connection string.\n    data_source: sqlite3:///tmp/buildbuddy.db\n    # database.log_queries (bool): If true, log all queries\n    log_queries: false\n    # database.max_idle_conns (int): The maximum number of idle connections to \n    # maintain to the db\n    max_idle_conns: 0\n    # database.max_open_conns (int): The maximum number of open connections to \n    # maintain to the db\n    max_open_conns: 0\n    # database.print_schema_changes_and_exit (bool): If set, print schema \n    # changes from auto-migration, then exit the program.\n    print_schema_changes_and_exit: false\n    # database.read_replica (string): A secondary, read-only SQL database to \n    # connect to, specified as a connection string.\n    read_replica: ""\n    # database.slow_query_threshold (time.Duration): Queries longer than this \n    # duration will be logged with a \'Slow SQL\' warning.\n    slow_query_threshold: 500ms\n    # database.stats_poll_interval (time.Duration): How often to poll the DB \n    # client for connection stats (default: \'5s\').\n    stats_poll_interval: 5s\nexecutor:\n    # executor.default_image (string): The default docker image to use to warm \n    # up executors or if no platform property is set. Ex: \n    # gcr.io/flame-public/executor-docker-default:enterprise-v1.5.4\n    default_image: gcr.io/flame-public/executor-docker-default:enterprise-v1.6.0\n    # executor.default_isolation_type (string): The default workload isolation \n    # type when no type is specified in an action. If not set, we use the \n    # first of the following that is set: docker, firecracker, podman, or \n    # barerunner\n    default_isolation_type: ""\n    # executor.default_xcode_version (string): Sets the default Xcode version \n    # number to use if an action doesn\'t specify one. If not set, \n    # /Applications/Xcode.app/ is used.\n    default_xcode_version: ""\n    # executor.docker_socket (string): If set, run execution commands in \n    # docker using the provided socket.\n    docker_socket: ""\n    # executor.enable_bare_runner (bool): Enables running execution commands \n    # directly on the host without isolation.\n    enable_bare_runner: false\n    # executor.enable_firecracker (bool): Enables running execution commands \n    # inside of firecracker VMs\n    enable_firecracker: false\n    # executor.enable_podman (bool): Enables running execution commands inside \n    # podman container.\n    enable_podman: false\n    # executor.enable_sandbox (bool): Enables running execution commands \n    # inside of sandbox-exec.\n    enable_sandbox: false\n    # executor.enable_vfs (bool): Whether FUSE based filesystem is enabled.\n    enable_vfs: false\n    # executor.extra_env_vars ([]string): Additional environment variables to \n    # pass to remotely executed actions. i.e. MY_ENV_VAR=foo\n    extra_env_vars: []\n    # executor.forced_network_isolation_type (string): If set, run all \n    # commands that require networking with this isolation\n    forced_network_isolation_type: ""\n    # executor.memory_bytes (int64): Optional maximum memory to allocate to \n    # execution tasks (approximate). Cannot set both this option and the \n    # SYS_MEMORY_BYTES env var.\n    memory_bytes: 0\n    # executor.millicpu (int64): Optional maximum CPU milliseconds to allocate \n    # to execution tasks (approximate). Cannot set both this option and the \n    # SYS_MILLICPU env var.\n    millicpu: 0\ngithub:\n    # github.access_token (string): The GitHub access token used to post \n    # GitHub commit statuses. ** Enterprise only **\n    access_token: ""\n    app:\n        # github.app.client_id (string): GitHub app OAuth client ID.\n        client_id: ""\n        # github.app.client_secret (string): GitHub app OAuth client secret.\n        client_secret: ""\n        # github.app.enabled (bool): Whether to enable the BuildBuddy GitHub \n        # app server.\n        enabled: false\n        # github.app.id (string): GitHub app ID.\n        id: ""\n        # github.app.private_key (string): GitHub app private key.\n        private_key: ""\n        # github.app.public_link (string): GitHub app installation URL.\n        public_link: ""\n        # github.app.webhook_secret (string): GitHub app webhook secret used \n        # to verify that webhook payload contents were sent by GitHub.\n        webhook_secret: ""\n    # github.client_id (string): The client ID of your GitHub Oauth App. ** \n    # Enterprise only **\n    client_id: ""\n    # github.client_secret (string): The client secret of your GitHub Oauth \n    # App. ** Enterprise only **\n    client_secret: ""\n    # github.jwt_key (string): The key to use when signing JWT tokens for \n    # github auth.\n    jwt_key: ""\n    # github.status_name_suffix (string): Suffix to be appended to all \n    # reported GitHub status names. Useful for differentiating BuildBuddy \n    # deployments. For example: \'(dev)\' ** Enterprise only **\n    status_name_suffix: ""\n    # github.status_per_test_target (bool): If true, report status per test \n    # target. ** Enterprise only **\n    status_per_test_target: false\nintegrations:\n    invocation_upload:\n        # integrations.invocation_upload.enabled (bool): Whether to upload \n        # webhook data to the webhook URL configured per-Group. ** Enterprise \n        # only **\n        enabled: false\n        # integrations.invocation_upload.gcs_credentials (string): Credentials \n        # JSON for the Google service account used to authenticate when GCS is \n        # used as the invocation upload target. ** Enterprise only **\n        gcs_credentials: ""\n    slack:\n        # integrations.slack.webhook_url (string): A Slack webhook url to post \n        # build update messages to.\n        webhook_url: ""\nkeystore:\n    aws:\n        # keystore.aws.credentials (string): AWS CSV credentials that will be \n        # used to authenticate. If not specified, credentials will be \n        # retrieved as described by \n        # https://docs.aws.amazon.com/sdkref/latest/guide/standardized-credentials.html\n        credentials: ""\n        # keystore.aws.credentials_file (string): A path to a AWS CSV \n        # credentials file that will be used to authenticate. If not \n        # specified, credentials will be retrieved as described by \n        # https://docs.aws.amazon.com/sdkref/latest/guide/standardized-credentials.html\n        credentials_file: ""\n        # keystore.aws.enabled (bool): Whether AWS KMS support should be \n        # enabled. Implicitly enabled if the master key URI references an AWS \n        # KMS URI.\n        enabled: false\n    gcp:\n        # keystore.gcp.credentials (string): GCP JSON credentials that will be \n        # used to authenticate.\n        credentials: ""\n        # keystore.gcp.credentials_file (string): A path to a gcp JSON \n        # credentials file that will be used to authenticate.\n        credentials_file: ""\n        # keystore.gcp.enabled (bool): Whether GCP KMS support should be \n        # enabled. Implicitly enabled if the master key URI references a GCP \n        # KMS URI.\n        enabled: false\n    # keystore.local_insecure_kms_directory (string): For development only. If \n    # set, keys in format local-insecure-kms://[id] are read from this \n    # directory.\n    local_insecure_kms_directory: ""\n    # keystore.master_key_uri (string): The master key URI (see tink docs for \n    # example)\n    master_key_uri: ""\nmonitoring:\n    basic_auth:\n        # monitoring.basic_auth.password (string): Optional password for basic \n        # auth on the monitoring port.\n        password: ""\n        # monitoring.basic_auth.username (string): Optional username for basic \n        # auth on the monitoring port.\n        username: ""\nolap_database:\n    # olap_database.auto_migrate_db (bool): If true, attempt to automigrate \n    # the db when connecting\n    auto_migrate_db: true\n    # olap_database.cluster_name (string): The cluster name of the database\n    cluster_name: \'{cluster}\'\n    # olap_database.conn_max_lifetime (time.Duration): The maximum lifetime of \n    # a connection to clickhouse\n    conn_max_lifetime: 0s\n    # olap_database.data_source (string): The clickhouse database to connect \n    # to, specified a a connection string\n    data_source: ""\n    # olap_database.enable_data_replication (bool): If true, data replication \n    # is enabled.\n    enable_data_replication: false\n    # olap_database.max_idle_conns (int): The maximum number of idle \n    # connections to maintain to the db\n    max_idle_conns: 0\n    # olap_database.max_open_conns (int): The maximum number of open \n    # connections to maintain to the db\n    max_open_conns: 0\n    # olap_database.print_schema_changes_and_exit (bool): If set, print schema \n    # changes from auto-migration, then exit the program.\n    print_schema_changes_and_exit: false\n    # olap_database.replica_name (string): The replica name of the table in \n    # zookeeper\n    replica_name: \'{replica}\'\n    # olap_database.zoo_path (string): The path to the table name in \n    # zookeeper, used to set up data replication\n    zoo_path: /clickhouse/{installation}/{cluster}/tables/{shard}/{database}/{table}\nopenai:\n    # openai.api_key (string): OpenAI API key\n    api_key: ""\n    # openai.model (string): OpenAI model name to use. Find them here: \n    # https://platform.openai.com/docs/models\n    model: gpt-3.5-turbo\norg:\n    # org.domain (string): Your organization\'s email domain. If this is set, \n    # only users with email addresses in this domain will be able to register \n    # for a BuildBuddy account.\n    domain: ""\n    # org.name (string): The name of your organization, which is displayed on \n    # your organization\'s build history.\n    name: Organization\nprometheus:\n    # prometheus.address (string): the address of the promethus HTTP API\n    address: ""\nremote_execution:\n    # remote_execution.default_pool_name (string): The default executor pool \n    # to use if one is not specified.\n    default_pool_name: ""\n    # remote_execution.enable_action_merging (bool): If enabled, identical \n    # actions being executed concurrently are merged into a single execution.\n    enable_action_merging: true\n    # remote_execution.enable_executor_key_creation (bool): If enabled, UI \n    # will allow executor keys to be created.\n    enable_executor_key_creation: false\n    # remote_execution.enable_redis_availability_monitoring (bool): If \n    # enabled, the execution server will detect if Redis has lost state and \n    # will ask Bazel to retry executions.\n    enable_redis_availability_monitoring: false\n    # remote_execution.enable_remote_exec (bool): If true, enable remote-exec. \n    # ** Enterprise only **\n    enable_remote_exec: true\n    # remote_execution.enable_user_owned_executors (bool): If enabled, users \n    # can register their own executors with the scheduler.\n    enable_user_owned_executors: false\n    # remote_execution.enable_workflows (bool): Whether to enable BuildBuddy \n    # workflows.\n    enable_workflows: false\n    # remote_execution.force_user_owned_darwin_executors (bool): If enabled, \n    # darwin actions will always run on user-owned executors.\n    force_user_owned_darwin_executors: false\n    # remote_execution.redis_pubsub_pool_size (int): Maximum number of \n    # connections used for waiting for execution updates.\n    redis_pubsub_pool_size: 10000\n    # remote_execution.redis_target (string): A Redis target for storing \n    # remote execution state. Falls back to app.default_redis_target if \n    # unspecified. Required for remote execution. To ease migration, the redis \n    # target from the cache config will be used if neither this value nor \n    # app.default_redis_target are specified.\n    redis_target: ""\n    # remote_execution.require_executor_authorization (bool): If true, \n    # executors connecting to this server must provide a valid executor API \n    # key.\n    require_executor_authorization: false\n    sharded_redis:\n        # remote_execution.sharded_redis.password (string): Redis password\n        password: ""\n        # remote_execution.sharded_redis.shards ([]string): Ordered list of \n        # Redis shard addresses.\n        shards: []\n        # remote_execution.sharded_redis.username (string): Redis username\n        username: ""\n    # remote_execution.shared_executor_pool_group_id (string): Group ID that \n    # owns the shared executor pool.\n    shared_executor_pool_group_id: ""\n    task_size_model:\n        # remote_execution.task_size_model.enabled (bool): Whether to enable \n        # model-based task size prediction.\n        enabled: false\n        # remote_execution.task_size_model.features_config_path (string): Path \n        # pointing to features.json config file.\n        features_config_path: ""\n        # remote_execution.task_size_model.serving_address (string): gRPC \n        # address pointing to TensorFlow Serving prediction service with task \n        # size models (cpu, mem).\n        serving_address: ""\n    # remote_execution.use_measured_task_sizes (bool): Whether to use measured \n    # usage stats to determine task sizes.\n    use_measured_task_sizes: false\n    # remote_execution.workflows_ci_runner_bazel_command (string): Bazel \n    # command to be used by the CI runner.\n    workflows_ci_runner_bazel_command: ""\n    # remote_execution.workflows_ci_runner_debug (bool): Whether to run the CI \n    # runner in debug mode.\n    workflows_ci_runner_debug: false\n    # remote_execution.workflows_default_image (string): The default \n    # container-image property to use for workflows. Must include docker:// \n    # prefix if applicable.\n    workflows_default_image: docker://gcr.io/flame-public/buildbuddy-ci-runner@sha256:ba33bd1b3acdfe980b958baf7d05c2041c9d4183d15fdf665dd236289d777709\n    # remote_execution.workflows_enable_firecracker (bool): Whether to enable \n    # firecracker for Linux workflow actions.\n    workflows_enable_firecracker: false\n    # remote_execution.workflows_linux_compute_units (int): Number of \n    # BuildBuddy compute units (BCU) to reserve for Linux workflow actions.\n    workflows_linux_compute_units: 3\n    # remote_execution.workflows_mac_compute_units (int): Number of BuildBuddy \n    # compute units (BCU) to reserve for Mac workflow actions.\n    workflows_mac_compute_units: 3\n    # remote_execution.workflows_pool_name (string): The executor pool to use \n    # for workflow actions. Defaults to the default executor pool if not \n    # specified.\n    workflows_pool_name: ""\nsoci_artifact_store:\n    # soci_artifact_store.cache_seed (string): If set, this seed is hashed \n    # with container image IDs to generate cache keys storing soci indexes.\n    cache_seed: socicache-06052023\n    # soci_artifact_store.layer_storage (string): Directory in which to store \n    # pulled container image layers for indexing by soci artifact store.\n    layer_storage: /tmp/\nssl:\n    # ssl.cert_file (string): Path to a PEM encoded certificate file to use \n    # for TLS if not using ACME.\n    cert_file: ""\n    # ssl.client_ca_cert (string): PEM encoded certificate authority used to \n    # issue client certificates for mTLS auth.\n    client_ca_cert: ""\n    # ssl.client_ca_cert_file (string): Path to a PEM encoded certificate \n    # authority file used to issue client certificates for mTLS auth.\n    client_ca_cert_file: ""\n    # ssl.client_ca_key (string): PEM encoded certificate authority key used \n    # to issue client certificates for mTLS auth.\n    client_ca_key: ""\n    # ssl.client_ca_key_file (string): Path to a PEM encoded certificate \n    # authority key file used to issue client certificates for mTLS auth.\n    client_ca_key_file: ""\n    # ssl.client_cert_lifespan (time.Duration): The duration client \n    # certificates are valid for. Ex: \'730h\' for one month. If not set, \n    # defaults to 100 years.\n    client_cert_lifespan: 876000h0m0s\n    # ssl.default_host (string): Host name to use for ACME generated cert if \n    # TLS request does not contain SNI.\n    default_host: ""\n    # ssl.enable_ssl (bool): Whether or not to enable SSL/TLS on gRPC \n    # connections (gRPCS).\n    enable_ssl: false\n    # ssl.host_whitelist ([]string): Cloud-Only\n    host_whitelist: []\n    # ssl.key_file (string): Path to a PEM encoded key file to use for TLS if \n    # not using ACME.\n    key_file: ""\n    # ssl.self_signed (bool): If true, a self-signed cert will be generated \n    # for TLS termination.\n    self_signed: false\n    # ssl.upgrade_insecure (bool): True if http requests should be redirected \n    # to https. Assumes http traffic is served on port 80 and https traffic is \n    # served on port 443 (typically via an ingress / load balancer).\n    upgrade_insecure: false\n    # ssl.use_acme (bool): Whether or not to automatically configure SSL certs \n    # using ACME. If ACME is enabled, cert_file and key_file should not be \n    # set.\n    use_acme: false\nstorage:\n    aws_s3:\n        # storage.aws_s3.bucket (string): The AWS S3 bucket to store files in.\n        bucket: ""\n        # storage.aws_s3.credentials_profile (string): A custom credentials \n        # profile to use.\n        credentials_profile: ""\n        # storage.aws_s3.disable_ssl (bool): Disables the use of SSL, useful \n        # for configuring the use of MinIO. **DEPRECATED** Specify a non-HTTPS \n        # endpoint instead.\n        disable_ssl: false\n        # storage.aws_s3.endpoint (string): The AWS endpoint to use, useful \n        # for configuring the use of MinIO.\n        endpoint: ""\n        # storage.aws_s3.region (string): The AWS region.\n        region: ""\n        # storage.aws_s3.role_arn (string): The role ARN to use for web \n        # identity auth.\n        role_arn: ""\n        # storage.aws_s3.role_session_name (string): The role session name to \n        # use for web identity auth.\n        role_session_name: ""\n        # storage.aws_s3.s3_force_path_style (bool): Force path style urls for \n        # objects, useful for configuring the use of MinIO.\n        s3_force_path_style: false\n        # storage.aws_s3.static_credentials_id (string): Static credentials ID \n        # to use, useful for configuring the use of MinIO.\n        static_credentials_id: ""\n        # storage.aws_s3.static_credentials_secret (string): Static \n        # credentials secret to use, useful for configuring the use of MinIO.\n        static_credentials_secret: ""\n        # storage.aws_s3.static_credentials_token (string): Static credentials \n        # token to use, useful for configuring the use of MinIO.\n        static_credentials_token: ""\n        # storage.aws_s3.web_identity_token_file (string): The file path to \n        # the web identity token file.\n        web_identity_token_file: ""\n    azure:\n        # storage.azure.account_key (string): The key for the Azure storage \n        # account\n        account_key: ""\n        # storage.azure.account_name (string): The name of the Azure storage \n        # account\n        account_name: ""\n        # storage.azure.container_name (string): The name of the Azure storage \n        # container\n        container_name: ""\n    # storage.chunk_file_size_bytes (int): How many bytes to buffer in memory \n    # before flushing a chunk of build protocol data to disk.\n    chunk_file_size_bytes: 3000000\n    # storage.cleanup_batch_size (int): How many invocations to delete in each \n    # janitor cleanup task\n    cleanup_batch_size: 10\n    # storage.disable_persist_cache_artifacts (bool): If disabled, buildbuddy \n    # will not persist cache artifacts in the blobstore. This may make older \n    # invocations not diaplay properly.\n    disable_persist_cache_artifacts: false\n    disk:\n        # storage.disk.root_directory (string): The root directory to store \n        # all blobs in, if using disk based storage.\n        root_directory: /tmp/buildbuddy\n        # storage.disk.use_v2_layout (bool): If enabled, files will be stored \n        # using the v2 layout. See disk_cache.MigrateToV2Layout for a \n        # description.\n        use_v2_layout: false\n    # storage.enable_chunked_event_logs (bool): If true, Event logs will be \n    # stored separately from the invocation proto in chunks.\n    enable_chunked_event_logs: false\n    execution:\n        # storage.execution.cleanup_batch_size (int): How many invocations to \n        # delete in each janitor cleanup task\n        cleanup_batch_size: 200\n        # storage.execution.cleanup_interval (time.Duration): How often the \n        # janitor cleanup tasks will run\n        cleanup_interval: 5m0s\n        # storage.execution.cleanup_workers (int): How many cleanup tasks to \n        # run\n        cleanup_workers: 1\n        # storage.execution.ttl (time.Duration): The time, in seconds, to keep \n        # invocations before deletion. 0 disables invocation deletion.\n        ttl: 0s\n    gcs:\n        # storage.gcs.bucket (string): The name of the GCS bucket to store \n        # build artifact files in.\n        bucket: ""\n        # storage.gcs.credentials (string): Credentials in JSON format that \n        # will be used to authenticate to GCS.\n        credentials: ""\n        # storage.gcs.credentials_file (string): A path to a JSON credentials \n        # file that will be used to authenticate to GCS.\n        credentials_file: ""\n        # storage.gcs.project_id (string): The Google Cloud project ID of the \n        # project owning the above credentials and GCS bucket.\n        project_id: ""\n    # storage.path_prefix (string): The prefix directory to store all blobs in\n    path_prefix: ""\n    # storage.tempdir (string): Root directory for temporary files. Defaults \n    # to the OS-specific temp dir.\n    tempdir: /tmp\n    # storage.ttl_seconds (int): The time, in seconds, to keep invocations \n    # before deletion. 0 disables invocation deletion.\n    ttl_seconds: 0\nvertexai:\n    # vertexai.credentials (string): The GCP credentials to use\n    credentials: ""\n    # vertexai.model (string): The model ID to use\n    model: chat-bison@001\n    # vertexai.project (string): The GCP project ID to use\n    project: flame-build\n    # vertexai.region (string): The GCP region to use\n    region: us-central1\n')))}p.isMDXComponent=!0;var h=["components"],f={toc:[]},g="wrapper";function b(e){var t=e.components,n=(0,o.Z)(e,h);return(0,i.kt)(g,(0,a.Z)({},f,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-yaml"},'# Unstructured settings\n\n# auto_migrate_db (bool): If true, attempt to automigrate the db when \n# connecting\nauto_migrate_db: true\n# auto_migrate_db_and_exit (bool): If true, attempt to automigrate the db when \n# connecting, then exit the program.\nauto_migrate_db_and_exit: false\n# debug_stream_command_outputs (bool): If true, stream command outputs to the \n# terminal. Intended for debugging purposes only and should not be used in \n# production.\ndebug_stream_command_outputs: false\n# debug_use_local_images_only (bool): Do not pull OCI images and only used \n# locally cached images. This can be set to test local image builds during \n# development without needing to push to a container registry. Not intended \n# for production use.\ndebug_use_local_images_only: false\n# docker_cap_add (string): Sets --cap-add= on the docker command. Comma \n# separated.\ndocker_cap_add: ""\n# drop_invocation_pk_cols (bool): If true, attempt to drop invocation PK cols\ndrop_invocation_pk_cols: false\n# grpc_client_origin_header (string): Header value to set for \n# x-buildbuddy-origin.\ngrpc_client_origin_header: ""\n# grpc_port (int): The port to listen for gRPC traffic on\ngrpc_port: 1985\n# grpcs_port (int): The port to listen for gRPCS traffic on\ngrpcs_port: 1986\n# internal_grpc_port (int): The port to listen for internal gRPC traffic on\ninternal_grpc_port: 1987\n# internal_grpcs_port (int): The port to listen for internal gRPCS traffic on\ninternal_grpcs_port: 1988\n# listen (string): The interface to listen on (default: 0.0.0.0)\nlisten: 0.0.0.0\n# log_goroutine_profile_on_shutdown (bool): Whether to log all goroutine stack \n# traces on shutdown.\nlog_goroutine_profile_on_shutdown: false\n# max_shutdown_duration (time.Duration): Time to wait for shutdown\nmax_shutdown_duration: 25s\n# monitoring_port (int): The port to listen for monitoring traffic on\nmonitoring_port: 9090\n# port (int): The port to listen for HTTP traffic on\nport: 8080\n# redis_command_buffer_flush_period (time.Duration): How long to wait between \n# flushing buffered redis commands. Setting this to 0 will disable buffering \n# at the cost of higher redis QPS.\nredis_command_buffer_flush_period: 250ms\n# report_not_ready (bool): If set to true, the app will always report as being \n# unready.\nreport_not_ready: false\n# server_type (string): The server type to match on health checks\nserver_type: prod-buildbuddy-executor\n# shutdown_lameduck_duration (time.Duration): If set, the server will be \n# marked unready but not run shutdown functions until this period passes.\nshutdown_lameduck_duration: 0s\n# zone_override (string): A value that will override the auto-detected zone. \n# Ignored if empty\nzone_override: ""\n\n# Structured settings\n\napp:\n    # app.admin_only_create_group (bool): If true, only admins of an existing \n    # group can create a new groups.\n    admin_only_create_group: false\n    # app.build_buddy_url (URL): The external URL where your BuildBuddy \n    # instance can be found.\n    build_buddy_url: http://localhost:8080\n    # app.cache_api_url (URL): Overrides the default remote cache protocol \n    # gRPC address shown by BuildBuddy on the configuration screen.\n    cache_api_url: ""\n    # app.default_redis_target (string): A Redis target for storing remote \n    # shared state. To ease migration, the redis target from the remote \n    # execution config will be used if this value is not specified.\n    default_redis_target: ""\n    default_sharded_redis:\n        # app.default_sharded_redis.password (string): Redis password\n        password: ""\n        # app.default_sharded_redis.shards ([]string): Ordered list of Redis \n        # shard addresses.\n        shards: []\n        # app.default_sharded_redis.username (string): Redis username\n        username: ""\n    # app.default_subdomains ([]string): List of subdomains that should not be \n    # handled as user-owned subdomains.\n    default_subdomains: []\n    # app.enable_grpc_metrics_by_group_id (bool): If enabled, grpc metrics by \n    # group ID will be recorded\n    enable_grpc_metrics_by_group_id: false\n    # app.enable_prometheus_histograms (bool): If true, collect prometheus \n    # histograms for all RPCs\n    enable_prometheus_histograms: true\n    # app.enable_structured_logging (bool): If true, log messages will be \n    # json-formatted.\n    enable_structured_logging: false\n    # app.enable_subdomain_matching (bool): If true, request subdomain will be \n    # taken into account when determining what request restrictions should be \n    # applied.\n    enable_subdomain_matching: false\n    # app.events_api_url (URL): Overrides the default build event protocol \n    # gRPC address shown by BuildBuddy on the configuration screen.\n    events_api_url: ""\n    # app.grpc_max_recv_msg_size_bytes (int): Configures the max GRPC receive \n    # message size [bytes]\n    grpc_max_recv_msg_size_bytes: 50000000\n    # app.grpc_over_http_port_enabled (bool): Cloud-Only\n    grpc_over_http_port_enabled: false\n    # app.ignore_forced_tracing_header (bool): If set, we will not honor the \n    # forced tracing header.\n    ignore_forced_tracing_header: false\n    # app.log_enable_gcp_logging_format (bool): If true, the output structured \n    # logs will be compatible with format expected by GCP Logging.\n    log_enable_gcp_logging_format: false\n    # app.log_error_stack_traces (bool): If true, stack traces will be printed \n    # for errors that have them.\n    log_error_stack_traces: false\n    # app.log_gcp_log_id (string): The log ID to log to in GCP (if any).\n    log_gcp_log_id: ""\n    # app.log_gcp_project_id (string): The project ID to log to in GCP (if \n    # any).\n    log_gcp_project_id: ""\n    # app.log_include_short_file_name (bool): If true, log messages will \n    # include shortened originating file name.\n    log_include_short_file_name: false\n    # app.log_level (string): The desired log level. Logs with a level >= this \n    # level will be emitted. One of {\'fatal\', \'error\', \'warn\', \'info\', \n    # \'debug\'}\n    log_level: info\n    # app.trace_fraction (float64): Fraction of requests to sample for \n    # tracing.\n    trace_fraction: 0\n    # app.trace_fraction_overrides ([]string): Tracing fraction override based \n    # on name in format name=fraction.\n    trace_fraction_overrides: []\n    # app.trace_jaeger_collector (string): Address of the Jager collector \n    # endpoint where traces will be sent.\n    trace_jaeger_collector: ""\n    # app.trace_project_id (string): Optional GCP project ID to export traces \n    # to. If not specified, determined from default credentials or metadata \n    # server if running on GCP.\n    trace_project_id: ""\n    # app.trace_service_name (string): Name of the service to associate with \n    # traces.\n    trace_service_name: ""\nauth:\n    # auth.admin_group_id (string): ID of a group whose members can perform \n    # actions only accessible to server admins.\n    admin_group_id: ""\n    # auth.disable_refresh_token (bool): If true, the offline_access scope \n    # which requests refresh tokens will not be requested.\n    disable_refresh_token: false\n    # auth.domain_wide_cookies (bool): If true, cookies will have domain set \n    # so that they are accessible on domain and all subdomains.\n    domain_wide_cookies: false\n    # auth.enable_anonymous_usage (bool): If true, unauthenticated build \n    # uploads will still be allowed but won\'t be associated with your \n    # organization.\n    enable_anonymous_usage: false\n    # auth.enable_self_auth (bool): If true, enables a single user login via \n    # an oauth provider on the buildbuddy server. Recommend use only when \n    # server is behind a firewall; this option may allow anyone with access to \n    # the webpage admin rights to your buildbuddy installation. ** Enterprise \n    # only **\n    enable_self_auth: false\n    # auth.force_approval (bool): If true, when a user doesn\'t have a session \n    # (first time logging in, or manually logged out) force the auth provider \n    # to show the consent screen allowing the user to select an account if \n    # they have multiple. This isn\'t supported by all auth providers.\n    force_approval: false\n    # auth.https_only_cookies (bool): If true, cookies will only be set over \n    # https connections.\n    https_only_cookies: false\n    # auth.jwt_claims_cache_ttl (time.Duration): TTL for JWT string to parsed \n    # claims caching. Set to \'0\' to disable cache.\n    jwt_claims_cache_ttl: 15s\n    # auth.jwt_key (string): The key to use when signing JWT tokens.\n    jwt_key: set_the_jwt_in_config\n    # auth.oauth_providers ([]oidc.OauthProvider): The list of oauth providers \n    # to use to authenticate.\n    oauth_providers: []\n    # For example:\n    # - issuer_url: "" # The issuer URL of this OIDC Provider. (type: string)\n    #   client_id: "" # The oauth client ID. (type: string)\n    #   client_secret: "" # The oauth client secret. (type: string)\n    #   slug: "" # The slug of this OIDC Provider. (type: string)\n\n    saml:\n        # auth.saml.cert (string): PEM encoded certificate used for SAML auth.\n        cert: ""\n        # auth.saml.cert_file (string): Path to a PEM encoded certificate file \n        # used for SAML auth.\n        cert_file: ""\n        # auth.saml.key (string): PEM encoded certificate key used for SAML \n        # auth.\n        key: ""\n        # auth.saml.key_file (string): Path to a PEM encoded certificate key \n        # file used for SAML auth.\n        key_file: ""\ncache:\n    client:\n        # cache.client.enable_download_compression (bool): If true, enable \n        # compression of downloads from remote caches\n        enable_download_compression: true\n        # cache.client.enable_upload_compression (bool): If true, enable \n        # compression of uploads to remote caches\n        enable_upload_compression: true\n    gcs:\n        # cache.gcs.bucket (string): The name of the GCS bucket to store cache \n        # files in.\n        bucket: ""\n        # cache.gcs.credentials_file (string): A path to a JSON credentials \n        # file that will be used to authenticate to GCS.\n        credentials_file: ""\n        # cache.gcs.project_id (string): The Google Cloud project ID of the \n        # project owning the above credentials and GCS bucket.\n        project_id: ""\n        # cache.gcs.ttl_days (int64): The period after which cache files \n        # should be TTLd. Disabled if 0.\n        ttl_days: 0\n    # cache.memcache_targets ([]string): Deprecated. Use Redis Target instead.\n    memcache_targets: []\n    redis:\n        # cache.redis.max_value_size_bytes (int64): The maximum value size to \n        # cache in redis (in bytes).\n        max_value_size_bytes: 10000000\n        # cache.redis.redis_target (string): A redis target for improved \n        # Caching/RBE performance. Target can be provided as either a redis \n        # connection URI or a host:port pair. URI schemas supported: \n        # redis[s]://[[USER][:PASSWORD]@][HOST][:PORT][/DATABASE] or \n        # unix://[[USER][:PASSWORD]@]SOCKET_PATH[?db=DATABASE] ** Enterprise \n        # only **\n        redis_target: ""\n        sharded:\n            # cache.redis.sharded.password (string): Redis password\n            password: ""\n            # cache.redis.sharded.shards ([]string): Ordered list of Redis \n            # shard addresses.\n            shards: []\n            # cache.redis.sharded.username (string): Redis username\n            username: ""\n    # cache.redis_target (string): A redis target for improved Caching/RBE \n    # performance. Target can be provided as either a redis connection URI or \n    # a host:port pair. URI schemas supported: \n    # redis[s]://[[USER][:PASSWORD]@][HOST][:PORT][/DATABASE] or \n    # unix://[[USER][:PASSWORD]@]SOCKET_PATH[?db=DATABASE] ** Enterprise only \n    # **\n    redis_target: ""\n    s3:\n        # cache.s3.bucket (string): The AWS S3 bucket to store files in.\n        bucket: ""\n        # cache.s3.credentials_profile (string): A custom credentials profile \n        # to use.\n        credentials_profile: ""\n        # cache.s3.disable_ssl (bool): Disables the use of SSL, useful for \n        # configuring the use of MinIO. **DEPRECATED** Specify a non-HTTPS \n        # endpoint instead.\n        disable_ssl: false\n        # cache.s3.endpoint (string): The AWS endpoint to use, useful for \n        # configuring the use of MinIO.\n        endpoint: ""\n        # cache.s3.path_prefix (string): Prefix inside the AWS S3 bucket to \n        # store files\n        path_prefix: ""\n        # cache.s3.region (string): The AWS region.\n        region: ""\n        # cache.s3.role_arn (string): The role ARN to use for web identity \n        # auth.\n        role_arn: ""\n        # cache.s3.role_session_name (string): The role session name to use \n        # for web identity auth.\n        role_session_name: ""\n        # cache.s3.s3_force_path_style (bool): Force path style urls for \n        # objects, useful for configuring the use of MinIO.\n        s3_force_path_style: false\n        # cache.s3.static_credentials_id (string): Static credentials ID to \n        # use, useful for configuring the use of MinIO.\n        static_credentials_id: ""\n        # cache.s3.static_credentials_secret (string): Static credentials \n        # secret to use, useful for configuring the use of MinIO.\n        static_credentials_secret: ""\n        # cache.s3.static_credentials_token (string): Static credentials token \n        # to use, useful for configuring the use of MinIO.\n        static_credentials_token: ""\n        # cache.s3.ttl_days (int): The period after which cache files should \n        # be TTLd. Disabled if 0.\n        ttl_days: 0\n        # cache.s3.web_identity_token_file (string): The file path to the web \n        # identity token file.\n        web_identity_token_file: ""\ndatabase:\n    # database.advanced_data_source (db.AdvancedConfig): Alternative to the \n    # database.data_source flag that allows finer control over database \n    # settings as well as allowing use of AWS IAM credentials. For most users, \n    # database.data_source is a simpler configuration method.\n    advanced_data_source:\n        driver: "" # The driver to use: one of sqlite3, mysql, or postgresql. (type: string)\n        endpoint: "" # Typically the host:port combination of the database server. (type: string)\n        username: "" # Username to use when connecting. (type: string)\n        password: "" # Password to use when connecting. Not used if AWS IAM is enabled. (type: string)\n        db_name: "" # The name of the database to use for BuildBuddy data. (type: string)\n        region: "" # Region of the database instance. Required if AWS IAM is enabled. (type: string)\n        use_aws_iam: false # If enabled, AWS IAM authentication is used instead of fixed credentials. Make sure the endpoint includes the port, otherwise IAM-based auth will fail. (type: bool)\n        params: "" # Optional parameters to pass to the database driver (in format key1=val1&key2=val2) (type: string)\n    # database.advanced_read_replica (db.AdvancedConfig): Advanced alternative \n    # to database.read_replica. Refer to database.advanced for more \n    # information.\n    advanced_read_replica:\n        driver: "" # The driver to use: one of sqlite3, mysql, or postgresql. (type: string)\n        endpoint: "" # Typically the host:port combination of the database server. (type: string)\n        username: "" # Username to use when connecting. (type: string)\n        password: "" # Password to use when connecting. Not used if AWS IAM is enabled. (type: string)\n        db_name: "" # The name of the database to use for BuildBuddy data. (type: string)\n        region: "" # Region of the database instance. Required if AWS IAM is enabled. (type: string)\n        use_aws_iam: false # If enabled, AWS IAM authentication is used instead of fixed credentials. Make sure the endpoint includes the port, otherwise IAM-based auth will fail. (type: bool)\n        params: "" # Optional parameters to pass to the database driver (in format key1=val1&key2=val2) (type: string)\n    # database.conn_max_lifetime_seconds (int): The maximum lifetime of a \n    # connection to the db\n    conn_max_lifetime_seconds: 0\n    # database.data_source (string): The SQL database to connect to, specified \n    # as a connection string.\n    data_source: sqlite3:///tmp/buildbuddy.db\n    # database.log_queries (bool): If true, log all queries\n    log_queries: false\n    # database.max_idle_conns (int): The maximum number of idle connections to \n    # maintain to the db\n    max_idle_conns: 0\n    # database.max_open_conns (int): The maximum number of open connections to \n    # maintain to the db\n    max_open_conns: 0\n    # database.print_schema_changes_and_exit (bool): If set, print schema \n    # changes from auto-migration, then exit the program.\n    print_schema_changes_and_exit: false\n    # database.read_replica (string): A secondary, read-only SQL database to \n    # connect to, specified as a connection string.\n    read_replica: ""\n    # database.slow_query_threshold (time.Duration): Queries longer than this \n    # duration will be logged with a \'Slow SQL\' warning.\n    slow_query_threshold: 500ms\n    # database.stats_poll_interval (time.Duration): How often to poll the DB \n    # client for connection stats (default: \'5s\').\n    stats_poll_interval: 5s\nexecutor:\n    # executor.api_key (string): API Key used to authorize the executor with \n    # the BuildBuddy app server.\n    api_key: ""\n    # executor.app_target (string): The GRPC url of a buildbuddy app server.\n    app_target: grpcs://remote.buildbuddy.io\n    bare:\n        # executor.bare.enable_stats (bool): Whether to enable stats for bare \n        # command execution.\n        enable_stats: false\n    # executor.container_registries ([]container.ContainerRegistry)\n    container_registries: []\n    # For example:\n    # - hostnames: [] # (type: []string)\n    #   username: "" # (type: string)\n    #   password: "" # (type: string)\n\n    # executor.context_based_shutdown_enabled (bool): Whether to remove \n    # runners using context cancelation. This is a transitional flag that will \n    # be removed in a future executor version.\n    context_based_shutdown_enabled: true\n    # executor.default_image (string): The default docker image to use to warm \n    # up executors or if no platform property is set. Ex: \n    # gcr.io/flame-public/executor-docker-default:enterprise-v1.5.4\n    default_image: gcr.io/flame-public/executor-docker-default:enterprise-v1.6.0\n    # executor.default_isolation_type (string): The default workload isolation \n    # type when no type is specified in an action. If not set, we use the \n    # first of the following that is set: docker, firecracker, podman, or \n    # barerunner\n    default_isolation_type: ""\n    # executor.default_xcode_version (string): Sets the default Xcode version \n    # number to use if an action doesn\'t specify one. If not set, \n    # /Applications/Xcode.app/ is used.\n    default_xcode_version: ""\n    # executor.die_on_firecracker_failure (bool): Makes the host executor \n    # process die if any command orchestrating or running Firecracker fails. \n    # Useful for capturing failures preemptively. WARNING: using this option \n    # MAY leave the host machine in an unhealthy state on Firecracker failure; \n    # some post-hoc cleanup may be necessary.\n    die_on_firecracker_failure: false\n    # executor.disable_local_cache (bool): If true, a local file cache will \n    # not be used.\n    disable_local_cache: false\n    # executor.docker_devices ([]container.DockerDeviceMapping): Configure \n    # (docker) devices that will be available inside the sandbox container. \n    # Format is \n    # --executor.docker_devices=\'[{"PathOnHost":"/dev/foo","PathInContainer":"/some/dest","CgroupPermissions":"see,docker,docs"}]\'\n    docker_devices: []\n    # For example:\n    # - path_on_host: "" # path to device that should be mapped from the host. (type: string)\n    #   path_in_container: "" # path under which the device will be present in container. (type: string)\n    #   cgroup_permissions: "" # cgroup permissions that should be assigned to device. (type: string)\n\n    # executor.docker_inherit_user_ids (bool): If set, run docker containers \n    # using the same uid and gid as the user running the executor process.\n    docker_inherit_user_ids: false\n    # executor.docker_mount_mode (string): Sets the mount mode of volumes \n    # mounted to docker images. Useful if running on SELinux \n    # https://www.projectatomic.io/blog/2015/06/using-volumes-with-docker-can-cause-problems-with-selinux/\n    docker_mount_mode: ""\n    # executor.docker_net_host (bool): Sets --net=host on the docker command. \n    # Intended for local development only. **DEPRECATED** Use \n    # --executor.docker_network=host instead.\n    docker_net_host: false\n    # executor.docker_network (string): If set, set docker/podman --network to \n    # this value by default. Can be overridden per-action with the \n    # `dockerNetwork` exec property, which accepts values \'off\' \n    # (--network=none) or \'bridge\' (--network=<default>).\n    docker_network: ""\n    # executor.docker_sibling_containers (bool): If set, mount the configured \n    # Docker socket to containers spawned for each action, to enable \n    # Docker-out-of-Docker (DooD). Takes effect only if docker_socket is also \n    # set. Should not be set by executors that can run untrusted code.\n    docker_sibling_containers: false\n    # executor.docker_socket (string): If set, run execution commands in \n    # docker using the provided socket.\n    docker_socket: ""\n    # executor.docker_volumes ([]string): Additional --volume arguments to be \n    # passed to docker or podman.\n    docker_volumes: []\n    # executor.enable_bare_runner (bool): Enables running execution commands \n    # directly on the host without isolation.\n    enable_bare_runner: false\n    # executor.enable_firecracker (bool): Enables running execution commands \n    # inside of firecracker VMs\n    enable_firecracker: false\n    # executor.enable_podman (bool): Enables running execution commands inside \n    # podman container.\n    enable_podman: false\n    # executor.enable_sandbox (bool): Enables running execution commands \n    # inside of sandbox-exec.\n    enable_sandbox: false\n    # executor.enable_vfs (bool): Whether FUSE based filesystem is enabled.\n    enable_vfs: false\n    # executor.exclusive_task_scheduling (bool): If true, only one task will \n    # be scheduled at a time. Default is false\n    exclusive_task_scheduling: false\n    # executor.extra_env_vars ([]string): Additional environment variables to \n    # pass to remotely executed actions. i.e. MY_ENV_VAR=foo\n    extra_env_vars: []\n    # executor.firecracker_cgroup_version (string): Specifies the cgroup \n    # version for firecracker to use.\n    firecracker_cgroup_version: ""\n    # executor.firecracker_debug_stream_vm_logs (bool): Stream firecracker VM \n    # logs to the terminal.\n    firecracker_debug_stream_vm_logs: false\n    # executor.firecracker_debug_terminal (bool): Run an interactive terminal \n    # in the Firecracker VM connected to the executor\'s controlling terminal. \n    # For debugging only.\n    firecracker_debug_terminal: false\n    # executor.firecracker_enable_nbd (bool): Enables network block devices \n    # for firecracker VMs.\n    firecracker_enable_nbd: false\n    # executor.firecracker_enable_uffd (bool): Enables userfaultfd for \n    # firecracker VMs.\n    firecracker_enable_uffd: false\n    # executor.firecracker_mount_workspace_file (bool): Enables mounting \n    # workspace filesystem to improve performance of copying action outputs.\n    firecracker_mount_workspace_file: false\n    # executor.forced_network_isolation_type (string): If set, run all \n    # commands that require networking with this isolation\n    forced_network_isolation_type: ""\n    # executor.host_root_directory (string): Path on the host where the \n    # executor container root directory is mounted.\n    host_root_directory: ""\n    # executor.local_cache_directory (string): A local on-disk cache \n    # directory. Must be on the same device (disk partition, Docker volume, \n    # etc.) as the configured root_directory, since files are hard-linked to \n    # this cache for performance reasons. Otherwise, \'Invalid cross-device \n    # link\' errors may result.\n    local_cache_directory: /tmp/buildbuddy/filecache\n    # executor.local_cache_size_bytes (int64): The maximum size, in bytes, to \n    # use for the local on-disk cache\n    local_cache_size_bytes: 1000000000\n    # executor.memory_bytes (int64): Optional maximum memory to allocate to \n    # execution tasks (approximate). Cannot set both this option and the \n    # SYS_MEMORY_BYTES env var.\n    memory_bytes: 0\n    # executor.millicpu (int64): Optional maximum CPU milliseconds to allocate \n    # to execution tasks (approximate). Cannot set both this option and the \n    # SYS_MILLICPU env var.\n    millicpu: 0\n    podman:\n        # executor.podman.cpu_usage_path_template (string): Go template \n        # specifying a path pointing to a container\'s total CPU usage, in CPU \n        # nanoseconds. Templated with `ContainerID`.\n        cpu_usage_path_template: /sys/fs/cgroup/cpuacct/libpod_parent/libpod-{{.ContainerID}}/cpuacct.usage\n        # executor.podman.enable_image_streaming (bool): If set, all public \n        # (non-authenticated) podman images are streamed using soci artifacts \n        # generated and stored in the apps.\n        enable_image_streaming: false\n        # executor.podman.enable_private_image_streaming (bool): If set and \n        # --executor.podman.enable_image_streaming is set, all private \n        # (authenticated) podman images are streamed using soci artifacts \n        # generated and stored in the apps.\n        enable_private_image_streaming: false\n        # executor.podman.enable_stats (bool): Whether to enable cgroup-based \n        # podman stats.\n        enable_stats: false\n        # executor.podman.memory_usage_path_template (string): Go template \n        # specifying a path pointing to a container\'s current memory usage, in \n        # bytes. Templated with `ContainerID`.\n        memory_usage_path_template: /sys/fs/cgroup/memory/libpod_parent/libpod-{{.ContainerID}}/memory.usage_in_bytes\n        # executor.podman.pull_log_level (string): Level at which to log \n        # `podman pull` command output. Should be one of the standard log \n        # levels, all lowercase.\n        pull_log_level: ""\n        # executor.podman.pull_timeout (time.Duration): Timeout for image \n        # pulls.\n        pull_timeout: 10m0s\n        # executor.podman.run_soci_store (bool): If true, runs the soci store \n        # locally if needed for image streaming.\n        run_soci_store: true\n        # executor.podman.runtime (string): Enables running podman with other \n        # runtimes, like gVisor (runsc).\n        runtime: ""\n        # executor.podman.soci_artifact_store_target (string): The GRPC url to \n        # use to access the SociArtifactStore GRPC service.\n        soci_artifact_store_target: ""\n        # executor.podman.soci_store_keychain_port (int): The port on which \n        # the soci-store local keychain service is exposed, for sharing \n        # credentials for streaming private container images.\n        soci_store_keychain_port: 1989\n        # executor.podman.soci_store_log_level (string): The level at which \n        # the soci-store should log. Should be one of the standard log levels, \n        # all lowercase.\n        soci_store_log_level: ""\n        # executor.podman.warmup_default_images (bool): Whether to warmup the \n        # default podman images or not.\n        warmup_default_images: true\n    # executor.pool (string): Executor pool name. Only one of this config \n    # option or the MY_POOL environment variable should be specified.\n    pool: ""\n    # executor.preserve_existing_netns (bool): Preserve existing bb-executor \n    # net namespaces. By default all "bb-executor" net namespaces are removed \n    # on executor startup, but if multiple executors are running on the same \n    # machine this behavior should be disabled to prevent them interfering \n    # with each other.\n    preserve_existing_netns: false\n    # executor.root_directory (string): The root directory to use for build \n    # files.\n    root_directory: /tmp/buildbuddy/remote_build\n    # executor.route_prefix (string): The prefix in the ip route to locate a \n    # device: either \'default\' or the ip range of the subnet e.g. \n    # 172.24.0.0/18\n    route_prefix: default\n    runner_pool:\n        # executor.runner_pool.max_runner_count (int): Maximum number of \n        # recycled RBE runners that can be pooled at once. Defaults to a value \n        # derived from estimated CPU usage, max RAM, allocated CPU, and \n        # allocated memory.\n        max_runner_count: 0\n        # executor.runner_pool.max_runner_disk_size_bytes (int64): Maximum \n        # disk size for a recycled runner; runners exceeding this threshold \n        # are not recycled. Defaults to 16GB.\n        max_runner_disk_size_bytes: 16000000000\n        # executor.runner_pool.max_runner_memory_usage_bytes (int64): Maximum \n        # memory usage for a recycled runner; runners exceeding this threshold \n        # are not recycled. Defaults to 1/10 of total RAM allocated to the \n        # executor. (Only supported for Docker-based executors).\n        max_runner_memory_usage_bytes: 8000000000\n    # executor.shutdown_cleanup_duration (time.Duration): The minimum duration \n    # during the shutdown window to allocate for cleaning up containers. This \n    # is capped to the value of `max_shutdown_duration`.\n    shutdown_cleanup_duration: 15s\n    # executor.startup_warmup_max_wait_secs (int64): Maximum time to block \n    # startup while waiting for default image to be pulled. Default is no \n    # wait.\n    startup_warmup_max_wait_secs: 0\n    # executor.warmup_timeout_secs (int64): The default time (in seconds) to \n    # wait for an executor to warm up i.e. download the default docker image. \n    # Default is 120s\n    warmup_timeout_secs: 120\n    # executor.warmup_workflow_images (bool): Whether to warm up the Linux \n    # workflow images (firecracker only).\n    warmup_workflow_images: false\ngithub:\n    # github.access_token (string): The GitHub access token used to post \n    # GitHub commit statuses. ** Enterprise only **\n    access_token: ""\n    app:\n        # github.app.client_id (string): GitHub app OAuth client ID.\n        client_id: ""\n        # github.app.client_secret (string): GitHub app OAuth client secret.\n        client_secret: ""\n        # github.app.enabled (bool): Whether to enable the BuildBuddy GitHub \n        # app server.\n        enabled: false\n        # github.app.id (string): GitHub app ID.\n        id: ""\n        # github.app.private_key (string): GitHub app private key.\n        private_key: ""\n        # github.app.public_link (string): GitHub app installation URL.\n        public_link: ""\n        # github.app.webhook_secret (string): GitHub app webhook secret used \n        # to verify that webhook payload contents were sent by GitHub.\n        webhook_secret: ""\n    # github.client_id (string): The client ID of your GitHub Oauth App. ** \n    # Enterprise only **\n    client_id: ""\n    # github.client_secret (string): The client secret of your GitHub Oauth \n    # App. ** Enterprise only **\n    client_secret: ""\n    # github.jwt_key (string): The key to use when signing JWT tokens for \n    # github auth.\n    jwt_key: ""\n    # github.status_name_suffix (string): Suffix to be appended to all \n    # reported GitHub status names. Useful for differentiating BuildBuddy \n    # deployments. For example: \'(dev)\' ** Enterprise only **\n    status_name_suffix: ""\nmonitoring:\n    basic_auth:\n        # monitoring.basic_auth.password (string): Optional password for basic \n        # auth on the monitoring port.\n        password: ""\n        # monitoring.basic_auth.username (string): Optional username for basic \n        # auth on the monitoring port.\n        username: ""\n    # monitoring.ssl_port (int): If non-negative, the SSL port to listen for \n    # monitoring traffic on. `ssl` config must have `ssl_enabled: true` and be \n    # properly configured.\n    ssl_port: -1\nolap_database:\n    # olap_database.cluster_name (string): The cluster name of the database\n    cluster_name: \'{cluster}\'\n    # olap_database.enable_data_replication (bool): If true, data replication \n    # is enabled.\n    enable_data_replication: false\n    # olap_database.replica_name (string): The replica name of the table in \n    # zookeeper\n    replica_name: \'{replica}\'\n    # olap_database.zoo_path (string): The path to the table name in \n    # zookeeper, used to set up data replication\n    zoo_path: /clickhouse/{installation}/{cluster}/tables/{shard}/{database}/{table}\nremote_execution:\n    # remote_execution.enable_remote_exec (bool): If true, enable remote-exec. \n    # ** Enterprise only **\n    enable_remote_exec: true\n    # remote_execution.redis_target (string): A Redis target for storing \n    # remote execution state. Falls back to app.default_redis_target if \n    # unspecified. Required for remote execution. To ease migration, the redis \n    # target from the cache config will be used if neither this value nor \n    # app.default_redis_target are specified.\n    redis_target: ""\n    sharded_redis:\n        # remote_execution.sharded_redis.password (string): Redis password\n        password: ""\n        # remote_execution.sharded_redis.shards ([]string): Ordered list of \n        # Redis shard addresses.\n        shards: []\n        # remote_execution.sharded_redis.username (string): Redis username\n        username: ""\n    task_size_model:\n        # remote_execution.task_size_model.enabled (bool): Whether to enable \n        # model-based task size prediction.\n        enabled: false\n        # remote_execution.task_size_model.features_config_path (string): Path \n        # pointing to features.json config file.\n        features_config_path: ""\n        # remote_execution.task_size_model.serving_address (string): gRPC \n        # address pointing to TensorFlow Serving prediction service with task \n        # size models (cpu, mem).\n        serving_address: ""\n    # remote_execution.use_measured_task_sizes (bool): Whether to use measured \n    # usage stats to determine task sizes.\n    use_measured_task_sizes: false\nssl:\n    # ssl.cert_file (string): Path to a PEM encoded certificate file to use \n    # for TLS if not using ACME.\n    cert_file: ""\n    # ssl.client_ca_cert (string): PEM encoded certificate authority used to \n    # issue client certificates for mTLS auth.\n    client_ca_cert: ""\n    # ssl.client_ca_cert_file (string): Path to a PEM encoded certificate \n    # authority file used to issue client certificates for mTLS auth.\n    client_ca_cert_file: ""\n    # ssl.client_ca_key (string): PEM encoded certificate authority key used \n    # to issue client certificates for mTLS auth.\n    client_ca_key: ""\n    # ssl.client_ca_key_file (string): Path to a PEM encoded certificate \n    # authority key file used to issue client certificates for mTLS auth.\n    client_ca_key_file: ""\n    # ssl.client_cert_lifespan (time.Duration): The duration client \n    # certificates are valid for. Ex: \'730h\' for one month. If not set, \n    # defaults to 100 years.\n    client_cert_lifespan: 876000h0m0s\n    # ssl.default_host (string): Host name to use for ACME generated cert if \n    # TLS request does not contain SNI.\n    default_host: ""\n    # ssl.enable_ssl (bool): Whether or not to enable SSL/TLS on gRPC \n    # connections (gRPCS).\n    enable_ssl: false\n    # ssl.host_whitelist ([]string): Cloud-Only\n    host_whitelist: []\n    # ssl.key_file (string): Path to a PEM encoded key file to use for TLS if \n    # not using ACME.\n    key_file: ""\n    # ssl.self_signed (bool): If true, a self-signed cert will be generated \n    # for TLS termination.\n    self_signed: false\n    # ssl.upgrade_insecure (bool): True if http requests should be redirected \n    # to https. Assumes http traffic is served on port 80 and https traffic is \n    # served on port 443 (typically via an ingress / load balancer).\n    upgrade_insecure: false\n    # ssl.use_acme (bool): Whether or not to automatically configure SSL certs \n    # using ACME. If ACME is enabled, cert_file and key_file should not be \n    # set.\n    use_acme: false\nstorage:\n    # storage.tempdir (string): Root directory for temporary files. Defaults \n    # to the OS-specific temp dir.\n    tempdir: /tmp\n')))}b.isMDXComponent=!0;var m=["components"],y={id:"config-all-options",title:"All Options",sidebar_label:"All Options"},w=void 0,k={unversionedId:"config-all-options",id:"config-all-options",title:"All Options",description:"Provided below are working, documented YAML configs for each BuildBuddy binary",source:"@site/../docs/config-all-options.mdx",sourceDirName:".",slug:"/config-all-options",permalink:"/docs/config-all-options",draft:!1,editUrl:"https://github.com/buildbuddy-io/buildbuddy/edit/master/docs/../docs/config-all-options.mdx",tags:[],version:"current",lastUpdatedBy:"Son Luong Ngoc",lastUpdatedAt:1692345487,formattedLastUpdatedAt:"Aug 18, 2023",frontMatter:{id:"config-all-options",title:"All Options",sidebar_label:"All Options"},sidebar:"someSidebar",previous:{title:"Flags",permalink:"/docs/config-flags"}},v={},x=[{value:"BuildBuddy Server (FOSS)",id:"buildbuddy-server-foss",level:3},{value:"BuildBuddy Server (Enterprise)",id:"buildbuddy-server-enterprise",level:3},{value:"BuildBuddy Executor",id:"buildbuddy-executor",level:3}],I={toc:x},T="wrapper";function S(e){var t=e.components,n=(0,o.Z)(e,m);return(0,i.kt)(T,(0,a.Z)({},I,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("p",null,"Provided below are working, documented YAML configs for each BuildBuddy binary\ncontaining every option that that binary accepts, each set to the default value\nfor that option. Any option that can be specified in the YAML config can also be\npassed on the command line. For nested options, be sure to write out the full\nYAML path, with a ",(0,i.kt)("inlineCode",{parentName:"p"},".")," separating each part."),(0,i.kt)("p",null,"For example:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-yaml"},"storage:\n    disk:\n        root_directory: /tmp/buildbuddy\n")),(0,i.kt)("p",null,"becomes:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sh"},'buildbuddy -storage.disk.root_directory="/tmp/buildbuddy"\n')),(0,i.kt)("p",null,"For specifying lists of structures using flags on the command line, use the JSON\nrepresentation of the list you wish to concatenate to the end or the element you\nwish to append:"),(0,i.kt)("p",null,"For example, given the following schema:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-yaml"},'cache:\n    disk:\n        partitions: [] # type: []disk.Partition\n        # e.g.:\n        # - id: "" # type: string\n        #   max_size_bytes: 0 # type: int\n')),(0,i.kt)("p",null,"We see that ",(0,i.kt)("inlineCode",{parentName:"p"},"cache.disk.partitions")," is configured as a list of ",(0,i.kt)("inlineCode",{parentName:"p"},"disk.Partition"),". In YAML, we'd normally configure it like this:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-yaml"},'cache:\n    disk:\n        partitions:\n        - id: "1GB"\n          max_size_bytes: 1073741824\n        - id: "2GB"\n          max_size_bytes: 2147483648\n')),(0,i.kt)("p",null,"The flag equivalent of this example would be:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sh"},'buildbuddy -cache.disk.partitions=\'{"id": "1GB", "max_size_bytes": 1073741824}\' -cache.disk.partitions=\'{"id": "2GB", "max_size_bytes": 2147483648}\'\n')),(0,i.kt)("p",null,"or"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sh"},'buildbuddy -cache.disk.partitions=\'[{"id": "1GB", "max_size_bytes": 1073741824}, {"id": "2GB", "max_size_bytes": 2147483648}]\'\n')),(0,i.kt)("h3",{id:"buildbuddy-server-foss"},"BuildBuddy Server (FOSS)"),(0,i.kt)(c,{mdxType:"FreeServerConfig"}),(0,i.kt)("h3",{id:"buildbuddy-server-enterprise"},"BuildBuddy Server (Enterprise)"),(0,i.kt)(p,{mdxType:"EnterpriseServerConfig"}),(0,i.kt)("h3",{id:"buildbuddy-executor"},"BuildBuddy Executor"),(0,i.kt)(b,{mdxType:"ExecutorConfig"}))}S.isMDXComponent=!0}}]);